{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acaf664b-48a6-45c8-9efc-9116cfa794c2",
   "metadata": {},
   "source": [
    "<img align='left' src = '../images/linea.png' width=150 style='padding: 20px'> \n",
    "\n",
    "# DP02 duplicates analysis\n",
    "## Part 1 - Analysis of two tracts\n",
    "\n",
    "Analysis of duplicates found in the DP02 catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c023f9-fcb2-4373-971c-63245f766728",
   "metadata": {},
   "source": [
    "Contacts: Luigi Silva ([luigi.silva@linea.org.br](mailto:luigi.silva@linea.org.br)); Julia Gschwend ([julia@linea.org.br](mailto:julia@linea.org.br)).\n",
    "\n",
    "Last check: 03/10/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3f70e4-0841-402c-a6dc-92af3e79ae4b",
   "metadata": {},
   "source": [
    "#### Acknowledgments\n",
    "\n",
    "'_This notebook used computational resources from the Associação Laboratório Interinstitucional de e-Astronomia (LIneA) with financial support from the INCT of e-Universe (Process No. 465376/2014-2)._'\n",
    "\n",
    "'_This notebook uses libraries from the LSST Interdisciplinary Network for Collaboration and Computing (LINCC) Frameworks project, such as the hipscat, hipscat_import, and lsdb libraries. The LINCC Frameworks project is supported by Schmidt Sciences. It is also based on work supported by the National Science Foundation under Grant No. AST-2003196. Additionally, it receives support from the DIRAC Institute at the Department of Astronomy of the University of Washington. The DIRAC Institute is supported by gifts from the Charles and Lisa Simonyi Fund for Arts and Sciences and the Washington Research Foundation._'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bdc6a4-01e6-4e8c-8e7e-5fd09d9f7571",
   "metadata": {},
   "source": [
    "# Imports and Configs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f92f1d-df17-45d1-8a58-371bb6829d70",
   "metadata": {},
   "source": [
    "Let us import the packages that we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd99bae-972d-4cb4-ad03-0d092232115f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dask\n",
    "from dask import dataframe as dd\n",
    "from dask import delayed\n",
    "from dask.distributed import Client, performance_report\n",
    "from dask_jobqueue import SLURMCluster\n",
    "import tables_io\n",
    "import pandas as pd\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c723d089-c753-435d-86d0-db3eb4223913",
   "metadata": {},
   "source": [
    "Now, let us define the paths to save the logs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fa3152-7d29-4ba4-8597-6ba753c09aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = getpass.getuser()\n",
    "base_path = f'/lustre/t0/scratch/users/{user}/report_hipscat/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53e900b-806e-4d16-9cb4-541752abfc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = os.path.join(base_path, 'output')\n",
    "logs_dir = os.path.join(base_path, 'logs')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(logs_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd65540-36de-4bf2-9e8a-e79c2996182d",
   "metadata": {},
   "source": [
    "Then, let us define the parameters for the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a622d2-b688-44fe-afd9-66b4cbd391a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring the SLURMCluster.\n",
    "cluster = SLURMCluster(\n",
    "    interface=\"ib0\",    # Lustre interface\n",
    "    queue='cpu_small',  # Name of the queue\n",
    "    cores=30,           # Number of logical cores per node\n",
    "    processes=15,       # Number of dask processes per node\n",
    "    memory='20GB',     # Memory per node\n",
    "    walltime='06:00:00',  # Maximum execution time\n",
    "    job_extra_directives=[\n",
    "        '--propagate',\n",
    "        f'--output={output_dir}/dask_job_%j.out',  \n",
    "        f'--error={output_dir}/dask_job_%j.err'\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Scaling the cluster to use X nodes\n",
    "cluster.scale(jobs=6)\n",
    "\n",
    "# Defining the dask client\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706cc3ba-0967-421f-a077-e5d93dca9f67",
   "metadata": {},
   "source": [
    "# Analyzing two tracts of the DP02 object table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb52867-5588-4b2f-9333-c6363ed43002",
   "metadata": {},
   "source": [
    "First, let us define the paths to the parquets of the considered tracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22213830-4877-4857-afab-8672f82a27c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_tract4029 = f'/lustre/t1/cl/lsst/dp02/primary/catalogs/object/objectTable_tract_4029_DC2_2_2i_runs_DP0_2_v23_0_1_PREOPS-905_step3_18_20220220T153612Z.parq'\n",
    "path_tract4030 = f'/lustre/t1/cl/lsst/dp02/primary/catalogs/object/objectTable_tract_4030_DC2_2_2i_runs_DP0_2_v23_0_1_PREOPS-905_step3_18_20220220T153612Z.parq'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c9b61c-afa9-42fa-9332-27f9b55c7b56",
   "metadata": {},
   "source": [
    "Now, let us read the parquet files with dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed160da-e004-4b72-b45a-a7abe7f846ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_tract4029 = dd.read_parquet(path_tract4029)\n",
    "ddf_tract4030 = dd.read_parquet(path_tract4030)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c832f034-f39d-48cb-bedc-6ed24504b79c",
   "metadata": {},
   "source": [
    "Here, we use ```.compute()``` to generate pandas dataframes from the dask dataframes. The pandas dataframes must be small, otherwise the Jupyter memory will blow up. So, we select just some columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4dce29-cfcf-4efb-a131-92464e02ee29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selected_columns = ['coord_ra', 'coord_dec', 'u_cModelFlux', 'g_cModelFlux', 'r_cModelFlux', 'i_cModelFlux', \n",
    "#                    'z_cModelFlux', 'y_cModelFlux', 'u_cModelFluxErr', 'g_cModelFluxErr', 'r_cModelFluxErr', 'i_cModelFluxErr', \n",
    "#                    'z_cModelFluxErr', 'y_cModelFluxErr', 'detect_isPrimary']\n",
    "\n",
    "selected_columns = ['coord_ra', 'coord_dec', 'g_cModelFlux', 'r_cModelFlux', 'i_cModelFlux', \n",
    "                    'g_cModelFluxErr', 'r_cModelFluxErr', 'i_cModelFluxErr', 'detect_isPrimary']\n",
    "\n",
    "df_tract4029_small = ddf_tract4029[selected_columns].compute()\n",
    "df_tract4030_small = ddf_tract4030[selected_columns].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6260a405-88e6-4014-9d65-da10089685be",
   "metadata": {},
   "source": [
    "## Checking for duplicates in tract 4029, considering the R.A. and DEC coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d98b7c7-90d3-4658-af8d-1ad0e67859a0",
   "metadata": {},
   "source": [
    "Now, we will check for duplicates in tract 4029, considering the R.A. and DEC coordinates, and we sort the values based on the ```coord_ra``` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3124ea38-eaa0-426a-a426-99fe0df737fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tract4029_duplicates = df_tract4029_small[df_tract4029_small[['coord_ra', 'coord_dec']].duplicated(keep=False)].sort_values('coord_ra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1f71d7-fcff-452c-b2b0-9ed246761f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tract4029_duplicates.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a65fa8-bd2f-49c2-8991-01fd2064226e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tract4029_duplicates.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11761f2b-99f2-41e8-b5e5-e2ba17c14e40",
   "metadata": {},
   "source": [
    "Now, let us filter to see only the objects that have not-NaN values in the ```g_cModelFlux``` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142fcf7c-1012-4bab-b4c7-1ddac5253e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tract4029_duplicates_g_not_nan = df_tract4029_duplicates[df_tract4029_duplicates['g_cModelFlux'].notna()]\n",
    "df_tract4029_duplicates_g_not_nan.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07cbd8d-2244-47fa-9c2d-8c9d2a870aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tract4029_duplicates_g_not_nan.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ae618b-606a-4f6c-b64f-38da59824953",
   "metadata": {},
   "source": [
    "As we can see, there are objects that have different ```objectId``` but have the same R.A. and DEC coordinates. However, they don't have the same flux."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea0d46f-d3e8-49b2-ab86-158ade4f2237",
   "metadata": {},
   "source": [
    "## Checking for duplicates in tract 4030, considering the R.A. and DEC coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91b3954-e583-44eb-bba8-a38614e26cc3",
   "metadata": {},
   "source": [
    "Now, we will check for duplicates in tract 4030, considering the R.A. and DEC coordinates, and we sort the values based on the ```coord_ra``` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a008923-b9b9-4a1b-8325-1366a3aecdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tract4030_duplicates = df_tract4030_small[df_tract4030_small[['coord_ra', 'coord_dec']].duplicated(keep=False)].sort_values('coord_ra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e521b94-1791-4da8-b7a6-6ad713d880b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tract4030_duplicates.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d54c1e6-f27d-43e2-9619-637f11f7eccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tract4030_duplicates.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1896a269-a9f0-442d-b69e-18a13c7a6803",
   "metadata": {},
   "source": [
    "Now, let us filter to see only the objects that have not-NaN values in the ```g_cModelFlux``` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7679a9ac-07a8-4f95-93ab-547e6ad3e956",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tract4030_duplicates_g_not_nan = df_tract4030_duplicates[df_tract4030_duplicates['g_cModelFlux'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5a82eb-7570-4e31-bab7-fb57b81474e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tract4030_duplicates_g_not_nan.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99483511-cbf5-4ef3-93e1-0e47ea635f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tract4030_duplicates_g_not_nan.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92519098-be2d-4a5f-9703-8080b113b01f",
   "metadata": {},
   "source": [
    "Again, there are objects that have different ```objectId``` but have the same R.A. and DEC coordinates. However, they don't have the same flux."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04033e2-139b-4a11-9569-d26c26a4f7c5",
   "metadata": {},
   "source": [
    "## Checking for duplicates in both tracts concatenated, considering the R.A. and DEC coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d79a94e-0592-4355-a7db-61c90811306b",
   "metadata": {},
   "source": [
    "Now, we will check for duplicates in both tracts concatenated, considering the R.A. and DEC coordinates, and we sort the values based on the ```coord_ra``` column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fa0803-4666-4040-9b89-0e7d63d4ed47",
   "metadata": {},
   "source": [
    "First, let us concatenate the dataframes and save the R.A. and DEC. coordinates in a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba897d03-4c03-432f-8056-7582f7c5cb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat_4029_4030 = pd.concat([df_tract4029_small, df_tract4030_small])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c361481-9c63-4a93-81b1-3adea3786a13",
   "metadata": {},
   "source": [
    "Now, let us search for duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49326b68-2511-42f8-8138-6c2d3095a8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat_4029_4030[df_concat_4029_4030[['coord_ra', 'coord_dec']].duplicated(keep=False)].sort_values('coord_ra')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3e1198-b8f4-4e86-ba50-22033affb6e3",
   "metadata": {},
   "source": [
    "As we can see, the number of duplicates is exactly the sum of the number of duplicates of both tracts individually (4029 and 4030), that is, $40199\\text{ rows} + 37817 \\text{ rows} = 78016 \\text{ rows}$. So, it seems that the duplicates are coming from a individual tract, and not from the combination of both tracts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15f0051-1578-485d-b70d-90632ea62af2",
   "metadata": {},
   "source": [
    "## Checking for duplicates in both tracts concatenated, considering the objectId column (index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720506f1-38a8-4993-bb72-53830e0719bf",
   "metadata": {},
   "source": [
    "Now, we will check for duplicates in both tracts concatenated, considering just the ```objectId column```, and we sort the values based on the ```coord_ra``` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b796415-0c91-4e87-915f-64b650ce2e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat_4029_4030[df_concat_4029_4030.index.duplicated(keep=False)].sort_values('coord_ra')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8096b46-0de4-4a71-86c6-7bc8b460fe23",
   "metadata": {},
   "source": [
    "As we can see, there are no duplicated ```objectId```, although, as we saw before, there are objects that have the same R.A. and DEC, but with different ```objectId``` values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105c6d71-356b-430b-8362-6f8f8841a72e",
   "metadata": {},
   "source": [
    "# Closing the client and cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cc4f7b-bcd9-40f5-b6fc-2d9f71a24a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fechando o client\n",
    "client.close()\n",
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ondemand-kernel",
   "language": "python",
   "name": "ondemand-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
