{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3502450f-0ad0-4916-88ac-6dbd5e269604",
   "metadata": {},
   "source": [
    "<img align=\"left\" src = \"images/linea.png\" width=120 style=\"padding: 20px\"> \n",
    "<img align=\"left\" src = \"images/rubin.png\" width=140 style=\"padding: 30px\"> \n",
    "\n",
    "# [Under construction] PZ Compute - E2E Notebook\n",
    "### Photo-zs for LSST Object catalog\n",
    "\n",
    "<br><br>\n",
    "\n",
    "Notebook contributors: Julia Gschwend, Luigi Silva, Heloisa Mengisztki <br>\n",
    "Contact: [julia@linea.org.br](mailto:julia@linea.org.br) <br>\n",
    "Last verified run: **2024-Nov-08** <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17152e67-232f-4a54-8f6f-ee57bec5b0e1",
   "metadata": {},
   "source": [
    "## README - Disclaimer\n",
    "This notebook is an alternative front-end for the pipeline Photo-z Compute, originally developed for command line execution on LIneA's HPC environment. It is meant to be used by the \"photo-z experts\" in charge of the production tasks related to the Brazilian in-kind contribution to LSST. It should **not** be considered as a source of [documentation or user guide](https://github.com/linea-it/pz-compute/tree/main/doc/manpages). \n",
    "\n",
    "After each complete execution, this notebook must be exported and saved as HTML file to serve as an execution report for future provenance tracking. Additional process metadata and provenance info are available in the `provenance_info.yaml` file attached. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2bb938-26f6-497f-9950-c0b07047787e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Notebook contents \n",
    "\n",
    "1. Pre-processing: data preparation, photo-z training and validation \n",
    "2. Photo-z Compute \n",
    "3. Post-processing: analize results and performance  \n",
    "\n",
    "\n",
    "Each one of these steps was carefuly explored in separate notebooks. This notebook contains only the final decisions regarding sample selection and configuration choices.   \n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e24efbb-b5c0-4eec-b883-278f34c02f97",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Imports and auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e06a1c9-c5e1-4924-af90-94a6249b76c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tables_io\n",
    "import getpass\n",
    "import pyarrow\n",
    "import yaml\n",
    "import time\n",
    "import glob\n",
    "import h5py\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import qp \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import pyarrow.parquet as pq  \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from pathlib import Path\n",
    "from IPython.display import Markdown, display\n",
    "from IPython.core.magic import register_cell_magic\n",
    "from IPython import get_ipython\n",
    "\n",
    "pyarrow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935d10cd-e546-4dc5-b7c5-923f3c5e3ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rail modules\n",
    "from rail.core.data import TableHandle, QPHandle\n",
    "from rail.core.stage import RailStage\n",
    "from rail.core.utils import find_rail_file\n",
    "\n",
    "from rail.estimation.algos.naive_stack import NaiveStackSummarizer\n",
    "from rail.estimation.algos.point_est_hist import PointEstHistSummarizer\n",
    "from rail.evaluation.point_to_point_evaluator import PointToPointEvaluator\n",
    "from rail.evaluation.metrics.cdeloss import *\n",
    "\n",
    "from qp.metrics.pit import PIT\n",
    " \n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "DS = RailStage.data_store\n",
    "DS.__class__.allow_overwrite = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41c3e24-0487-40f3-928d-7657cd176446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ondemand_utils as utils\n",
    "from pzserver import PzServer\n",
    "\n",
    "utils.run_command(\"pwd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83228fac-4786-41f7-972e-de5177aaabfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a25f664-be93-4920-91c7-1523e3a5f333",
   "metadata": {},
   "source": [
    "in case ondemand_utils.utils does not import correctly, please run on your terminal: \n",
    "\n",
    ">```python \n",
    "> pip install -e $SCRATCH/pz-compute/ondemand/ondemand_utils/. \n",
    ">```\n",
    "\n",
    "or uncomment the following cell and after running it, restart your kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bd7ce7-a8ad-4d1e-a3ef-909e5509d452",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#! pip install -e $SCRATCH/pz-compute/ondemand/ondemand_utils/. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc795a95-3b73-44c2-8b1a-0b60753bb05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_cell_magic\n",
    "def skip_if(line, cell):\n",
    "    if eval(line):\n",
    "        return\n",
    "    get_ipython().run_cell(cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062130f7-7643-48e7-8659-74ba733233ee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Data Release: DP0.2 \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8fca8ea8-0374-4f01-b2c7-376c8c9a689a",
   "metadata": {},
   "source": [
    "TEXTO EM PT-BR = PLANEJAMENTO \n",
    "- mais texto em markdown para definir melhor todas as etapas - Julia "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874f638a-5d92-4ea2-9e08-8ab30ab2f04b",
   "metadata": {
    "tags": []
   },
   "source": [
    "--- \n",
    "\n",
    "# 1. Pre-processing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff0acd8-0396-4a00-8d89-1abcbb65fa7c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1 Create Skinny tables \n",
    "\n",
    "Skinny tables are a subset of the [LSST Object catalog](https://sdm-schemas.lsst.io/dp02.html#Object) that includes only the columns of interest for photo-z algorithms, with ready-to-use data, i.e.: fluxes converted into deredded magnitudes.  \n",
    "\n",
    "### Input data\n",
    "\n",
    "The very first input data of this end-to-end sequence is the original LSST Object catalog for DP0.2, stored in Lustre system at: \n",
    "\n",
    "`/lustre/t1/cl/lsst/dp02/primary/catalogs/object/` \n",
    "\n",
    "Filename pattern: `objectTable_tract_xxxx_DC2_2_2i_runs_DP0_2_v23_0_1_PREOPS-905_step3_x_2022xxxxTxxxxxxZ.parq`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f02225-aa45-4659-8150-833abd734c9e",
   "metadata": {},
   "source": [
    "File size summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea1e4fd-6612-40db-b7e5-f5fe93c82112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths for the catalog files\n",
    "catalog_path = '/lustre/t1/cl/lsst/dp02/primary/catalogs/object'\n",
    "catalog_files = '*.parq'\n",
    "catalog_files_paths = [f for f in glob.glob(os.path.join(catalog_path, catalog_files))]\n",
    "\n",
    "# If the IDs are in the index column, reset the index and add a column corresponding to the IDs.\n",
    "ids_are_in_the_index = True\n",
    "\n",
    "# Defining if you want to save the file size distribution, file size histogram and summarize pixels.\n",
    "save_input_catalog_info = False\n",
    "\n",
    "# If you choose True above, select the path to save the info.\n",
    "if save_input_catalog_info==True:\n",
    "    user = getpass.getuser()\n",
    "    path_to_save_input_catalog_info = f'/lustre/t0/scratch/users/{user}/pz-compute/ondemand/data'\n",
    "    os.makedirs(path_to_save_input_catalog_info, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e71895-5cde-46b8-b80a-04c444274629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the catalog and getting the columns\n",
    "catalog_ddf = dd.read_parquet(catalog_files_paths)\n",
    "if ids_are_in_the_index:\n",
    "    catalog_ddf = catalog_ddf.reset_index()\n",
    "catalog_columns = catalog_ddf.columns\n",
    "catalog_columns_list = catalog_columns.to_list()\n",
    "\n",
    "# Getting general information about the catalog files\n",
    "file_info_list = []\n",
    "for file_path in catalog_files_paths:\n",
    "    try:\n",
    "        # File size\n",
    "        file_size = os.stat(file_path).st_size  # Size in bytes\n",
    "        file_size_gb = file_size / (1024 ** 3)  # Converting to gigabytes\n",
    "        \n",
    "        # Counting the rows using parquet metadata\n",
    "        parquet_file = pq.ParquetFile(file_path)  # Loading parquet metadata\n",
    "        num_rows = parquet_file.metadata.num_rows  # Number of rows in the file\n",
    "        \n",
    "        # Adding information to the dictionary\n",
    "        file_info_list.append({\n",
    "            \"file\": file_path,\n",
    "            \"size_on_disk\": file_size,\n",
    "            \"gbs\": file_size_gb,\n",
    "            \"rows\": num_rows\n",
    "        })\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "# Creating a dataframe with the files info\n",
    "input_info_frame = pd.DataFrame(file_info_list)\n",
    "\n",
    "# Calculating statistics\n",
    "num_partitions = len(input_info_frame)\n",
    "min_size_on_disk = input_info_frame[\"gbs\"].min() if not input_info_frame.empty else 0\n",
    "max_size_on_disk = input_info_frame[\"gbs\"].max() if not input_info_frame.empty else 0\n",
    "total_size_on_disk = input_info_frame[\"gbs\"].sum()\n",
    "\n",
    "# Getting the rows count corresponding to the min and max size files\n",
    "min_size_file = input_info_frame[input_info_frame[\"gbs\"] == min_size_on_disk]\n",
    "max_size_file = input_info_frame[input_info_frame[\"gbs\"] == max_size_on_disk]\n",
    "min_rows = min_size_file[\"rows\"].iloc[0] if not min_size_file.empty else 0\n",
    "max_rows = max_size_file[\"rows\"].iloc[0] if not max_size_file.empty else 0\n",
    "\n",
    "total_rows = input_info_frame[\"rows\"].sum() if not input_info_frame.empty else 0\n",
    "avg_file_size = total_size_on_disk / num_partitions if num_partitions > 0 else 0\n",
    "avg_rows_per_file = total_rows / num_partitions if num_partitions > 0 else 0\n",
    "\n",
    "# Preparing the table in Markdown format\n",
    "markdown_table = f\"\"\"\n",
    "| Metric               | Value                       |\n",
    "|----------------------|-----------------------------|\n",
    "| Number of files      | {len(catalog_files_paths)}  |\n",
    "| Number of columns    | {len(catalog_columns_list)} |\n",
    "| Min file size        | {min_size_on_disk:.2f} GB; {min_rows} rows |\n",
    "| Max file size        | {max_size_on_disk:.2f} GB; {max_rows} rows |\n",
    "| Average file size    | {avg_file_size:.2f} GB; {avg_rows_per_file:.0f} rows |\n",
    "| Total size on disk   | {total_size_on_disk:.2f} GB; {total_rows} rows |\n",
    "\"\"\"\n",
    "\n",
    "# Display the Markdown table in a cell\n",
    "display(Markdown(markdown_table))\n",
    "\n",
    "# Saving the information if required\n",
    "if save_input_catalog_info:\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(path_to_save_input_catalog_info, exist_ok=True)\n",
    "    \n",
    "    # Save the DataFrame as CSV\n",
    "    input_info_frame_path = os.path.join(path_to_save_input_catalog_info, \"input_info_frame.csv\")\n",
    "    input_info_frame.to_csv(input_info_frame_path, index=False)\n",
    "    \n",
    "    # Save the Markdown table as a text file\n",
    "    markdown_table_path = os.path.join(path_to_save_input_catalog_info, \"input_info_summary.txt\")\n",
    "    with open(markdown_table_path, \"w\") as f:\n",
    "        f.write(markdown_table)\n",
    "    \n",
    "    # Save the provenance information\n",
    "    provenance_path = os.path.join(path_to_save_input_catalog_info, \"input_info_provenance.txt\")\n",
    "    with open(provenance_path, \"w\") as f:\n",
    "        f.write(f\"catalog_path: {catalog_path}\\n\")\n",
    "        f.write(f\"catalog_files: {catalog_files}\\n\")\n",
    "    \n",
    "    print(f\"Information saved to {path_to_save_input_catalog_info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980bbd44-daba-4c00-b933-58aa07f1f38a",
   "metadata": {},
   "source": [
    "File sizes distribution and file sizes histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9de3cab-cd18-47db-a1cb-388a9967f1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_and_save_histogram_info(info_frame, bins, labels, type_of_files, logs_dir, save=False, show=False):\n",
    "    \"\"\"\n",
    "    Computes, optionally saves, and optionally shows histogram information (counts and percentages) for given bins and labels.\n",
    "\n",
    "    Args:\n",
    "        info_frame (DataFrame): DataFrame containing the file size information.\n",
    "        bins (list): Bin edges for the histogram.\n",
    "        labels (list): Labels corresponding to the bins.\n",
    "        type_of_files (str): Identifier for the type of files (used in filenames).\n",
    "        logs_dir (str): Directory to save the output files.\n",
    "        save (bool): Whether to save the histogram information.\n",
    "        show (bool): Whether to display the histogram information as a Markdown table.\n",
    "    \"\"\"\n",
    "    # Compute histogram counts and percentages\n",
    "    hist = np.histogram(info_frame[\"gbs\"], bins=bins)[0]\n",
    "    pcts = hist / len(info_frame) if len(info_frame) > 0 else [0] * len(labels)\n",
    "\n",
    "    # Save histogram information\n",
    "    if save and logs_dir:\n",
    "        os.makedirs(logs_dir, exist_ok=True)\n",
    "        output_path = os.path.join(logs_dir, f\"{type_of_files}_file_size_distribution.txt\")\n",
    "        with open(output_path, \"w\") as file:\n",
    "            file.write(f\"Bins: {bins} GB\\n\")\n",
    "            file.write(f\"Labels: {labels} \\n \\n\")\n",
    "            for i, label in enumerate(labels):\n",
    "                file.write(f\"{label} \\t: {hist[i]} \\t({pcts[i]*100:.1f} %)\\n\")\n",
    "        print(f\"Histogram information saved to {output_path}\")\n",
    "        \n",
    "    # Show histogram information as a Markdown table\n",
    "    if show:\n",
    "        # Prepare the Markdown table\n",
    "        markdown_table = \"| Label        | Count | Percentage (%) |\\n\"\n",
    "        markdown_table += \"|--------------|-------|----------------|\\n\"\n",
    "        for i, label in enumerate(labels):\n",
    "            markdown_table += f\"| {label:<12} | {hist[i]:<5} | {pcts[i]*100:.1f}       |\\n\"\n",
    "\n",
    "        # Display bins and labels in text, then the Markdown table\n",
    "        bins_labels_md = f\"**Bins**: {bins}  GB \\n\\n**Labels**: {labels}  \\n\\n\"\n",
    "        display(Markdown(bins_labels_md + markdown_table))\n",
    "\n",
    "\n",
    "def plot_and_save_histogram(info_frame, bins, type_of_files, logs_dir, save=False, show=False):\n",
    "    \"\"\"\n",
    "    Plots and optionally saves the histogram of file sizes and displays the bins as Markdown.\n",
    "\n",
    "    Args:\n",
    "        info_frame (DataFrame): DataFrame containing the file size information.\n",
    "        bins (list or None): Bin edges for the histogram. If None, bins will be generated automatically.\n",
    "        type_of_files (str): Identifier for the type of files (used in filenames).\n",
    "        logs_dir (str): Directory to save the output files.\n",
    "        save (bool): Whether to save the histogram plot.\n",
    "        show (bool): Whether to display the plot and bins information.\n",
    "    \"\"\"\n",
    "    # Plot the histogram\n",
    "    n, bins_generated, _ = plt.hist(info_frame[\"gbs\"], bins=bins, edgecolor='black')\n",
    "    plt.xlabel(\"File size (GB)\")\n",
    "    plt.ylabel(\"Number of files\")\n",
    "    \n",
    "    bins_used = bins_generated if bins is None else bins\n",
    "    bins_used_rounded = [round(float(bin_edge), 3) for bin_edge in bins_used]\n",
    "    \n",
    "    # Save the plot\n",
    "    if save and logs_dir:\n",
    "        os.makedirs(logs_dir, exist_ok=True)\n",
    "        output_path_plot = os.path.join(logs_dir, f\"{type_of_files}_file_size_histogram.png\")\n",
    "        plt.savefig(output_path_plot)\n",
    "        print(f\"Histogram plot saved to {output_path_plot}\")\n",
    "        \n",
    "        output_path_bins = os.path.join(logs_dir, f\"{type_of_files}_file_size_histogram.txt\")\n",
    "        with open(output_path_bins, \"w\") as file:\n",
    "            file.write(f\"Bins: {bins_used} GB\\n\")\n",
    "\n",
    "    # Show the plot\n",
    "    if show:\n",
    "        # Display bins in Markdown format\n",
    "        bins_md = f\"**Bins**: {bins_used_rounded}  GB\\n\"\n",
    "        display(Markdown(bins_md))\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa2b4de-d112-4c89-9f80-92be969c92fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "type_of_files = 'input'\n",
    "\n",
    "bins_lincc_categories = [0, 0.5, 1, 2, 100]\n",
    "labels_lincc_categories = [\"small-ish\", \"sweet-spot\", \"big-ish\", \"too-big\"]\n",
    "\n",
    "bins_plotted_histogram = None\n",
    "\n",
    "logs_dir = path_to_save_input_catalog_info if save_input_catalog_info else None\n",
    "\n",
    "compute_and_save_histogram_info(\n",
    "    info_frame=input_info_frame, \n",
    "    bins=bins_lincc_categories, \n",
    "    labels=labels_lincc_categories, \n",
    "    type_of_files=type_of_files, \n",
    "    logs_dir=logs_dir, \n",
    "    save=save_input_catalog_info, \n",
    "    show=True\n",
    ")\n",
    "\n",
    "print(\"\\n \\n\")\n",
    "\n",
    "plot_and_save_histogram(\n",
    "    info_frame=input_info_frame, \n",
    "    bins=bins_plotted_histogram,  \n",
    "    type_of_files=type_of_files, \n",
    "    logs_dir=logs_dir, \n",
    "    save=save_input_catalog_info, \n",
    "    show=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65287f8f-8229-4757-b3b2-9c9802453d10",
   "metadata": {},
   "source": [
    "### Column selection  \n",
    "\n",
    "Columns included in the skinny tables: \n",
    "\n",
    "| column name | data type |  description |\n",
    "| ---         | ---       |  ---         |\n",
    "| objectId\t  | int  \t  | Unique identifier | \n",
    "| coord_ra\t  | float64\t  | Fiducial ICRS Right Ascension of centroid (degrees)|\n",
    "| coord_dec   |\tfloat64\t  | Fiducial ICRS Declination of centroid (degrees)| \n",
    "| detect_isPrimary\t| boolean\t| True if source has no children and is in the inner region of a coadd patch and is in the inner region of a coadd tract and is not a sky source | \n",
    "| mag_{u, g, r, i, z, y} | float64 | {u, g, r, i, z, y}-band magnitude converted from final cmodel fit flux measurements | \n",
    "| magerr_{u, g, r, i, z, y} | float64 | {u, g, r, i, z, y}-band magnitude errors converted from final cmodel fit flux error measurements | \n",
    "\n",
    "### Object selection  \n",
    "\n",
    "Data cleaning to reduce the number of rows in the catalog is recommended for tests or when using the pipeline to create value-added catalogs for science cases. Survey conditions maps can be used to remove intire regions at once, based on a given quality threshold.   \n",
    "\n",
    "\n",
    "**WARNING: Data cleaning must not be applied on the production runs to generate the official data products to be delivered as part of the in-kind contribution. Every detected object from the Object catalog must receive a photo-z estimate regardless of its nature or photometry quality.**      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513f39b1-c57b-400c-b088-cb94bf8b63df",
   "metadata": {},
   "source": [
    "### Configuration parameters \n",
    "\n",
    "Parameters defined inside `$SCRATCH/bin/rail-slurm-preprocess.batch`: \n",
    "\n",
    "```python \n",
    "SRUN = 'srun'\n",
    "SRUN_ARGS = [SRUN, '-n1', '-N1']\n",
    "LOG = 'log'\n",
    "PROG = 'rail-slurm-preprocess'\n",
    "PREPROCESS = 'rail-preprocess-parquet'\n",
    "PREPROCESS_ARGS = ['--rows=130000', '--apply-dered=sfd', '--apply-detect-flag=True', \n",
    "                   '--round-mags=4', '--output-template={fname}-part{idx}.parquet']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9755e60-2667-406e-afa5-ed142e7a260a",
   "metadata": {},
   "source": [
    "### Execute `rail-preprocess-parquet` on Apollo\n",
    "\n",
    "WARNING: Current working directory must contain a directory named as `log`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ea8b0f-1187-4f4c-a108-5dde5c0c385c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fazer funcionar a execução em linha de comando - Helo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4e8e22-0adf-49db-a071-be6b902aadac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! sbatch -N 2 -n 20 rail-slurm-preprocess.batch  input/ output/ input/objectTable_tract_*.parq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a077c3bc-b878-4ecb-84bc-f47abf3a0a8c",
   "metadata": {},
   "source": [
    "monitor slurm queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c67614c-9c87-443f-a71a-9deea57a3270",
   "metadata": {},
   "outputs": [],
   "source": [
    "! squeue"
   ]
  },
  {
   "cell_type": "raw",
   "id": "42a392ad-c59e-4838-a829-97178a32ddc2",
   "metadata": {},
   "source": [
    "TO DO: debug\n",
    "\n",
    "- mag_err com notação cient;ífica apenas em algumas bandas quando usa o round mag --> trocar para float 32 - Julia \n",
    "- verificar impacto do deredenning (em outro notebook)  - Julia \n",
    "- drop coluna detect_isPrimary quando for usada para o corte e manter quando não usar - Julia  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "38072a87-0fb0-41bc-b5b4-0782fdbd1b7e",
   "metadata": {},
   "source": [
    "travar aqui enquanto a skinny table fica pronta (vide \"job wait\" no tap service síncrono do LSST nos notebooks de tutorial) - Helo "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c517038f-1231-4335-b581-27ccf48ba7a9",
   "metadata": {},
   "source": [
    "### Output data \n",
    "\n",
    "#### Basic QA of skinny tables "
   ]
  },
  {
   "cell_type": "raw",
   "id": "f43378a9-1179-4f88-b924-ad0078620995",
   "metadata": {},
   "source": [
    "Histograma tamanhos arquivos tabela skinny (comparar lado a lado) - Luigi  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d979d4-633f-4f66-9d49-16a15156afd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "2a221adf-75b2-48c4-ba45-a8a5afcd3ed3",
   "metadata": {},
   "source": [
    "QA científico - selecionar plots mais importantes - Luigi\n",
    "começar por: \n",
    "- dist. espacial \n",
    "- N(mag), banda i \n",
    "- mag x err, só banda i "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17be7b97-3bd9-401f-912b-2d9a892b10aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1.2 Create Training and Test Sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8d4764-141f-4703-9fb8-ed02fd4b1f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "will_train = utils.load_config('process_info.yaml')['will_train']\n",
    "will_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8663fdd8-169c-4b83-a628-2a856a366fa4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.2.1 Creating a training and testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bb4f20-9423-4c16-9bb8-3b4e87d73e72",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Representative spectroscopic sample \n",
    "\n",
    "A true-z sample randomly selected from the DC2 simulation to mimic a representative spectroscopic sample regarding the color-magnitude-redshift space. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "6fa06133-7945-4192-9072-c9b4a4523e34",
   "metadata": {},
   "source": [
    "executar random selecion - Luigi "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86638fb-6bab-4ecd-b5d3-11a46a65b83d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Download from PZ-Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20de284e-fe9d-42b4-a2c8-fe04cdd4ead0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip_if not will_train\n",
    "\n",
    "token = 'a22d527544fb1215105a40cea9d36279ed8220ab'\n",
    "# with open('.token.txt', 'r') as file:\n",
    "#     token = file.read()\n",
    "pz_server = PzServer(token=token, host=\"pz-dev\") # \"pz-dev\" is the temporary host for test phase  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06c5a97-e234-434b-af9b-81775e3e077a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip_if not will_train\n",
    "\n",
    "pz_server.download_product('72_pzcompute_results_for_qa_validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36100ef1-c8f9-4cfe-bba3-91eaeb7c1162",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip_if not will_train\n",
    "\n",
    "! unzip {cwd}/72_pzcompute_results_for_qa_validation_*.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bd21fb-707e-43ed-a8f7-3158d19b1708",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip_if not will_train\n",
    "\n",
    "! unzip {cwd}/validation_set.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192c56c8-734a-4ef4-b10d-f8f0bc670578",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Creating a training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56154b8b-7182-4bf1-ab26-c311b7e5ff5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip_if not will_train\n",
    "\n",
    "file_path = f'{cwd}/validation_set.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7b0bed-be20-450d-8251-00616b49dcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip_if not will_train\n",
    "\n",
    "full_file = find_rail_file(file_path)\n",
    "full_data = DS.read_file('full_set_tests', TableHandle, full_file)\n",
    "\n",
    "truth = tables_io.convertObj(full_data(), tables_io.types.PD_DATAFRAME)\n",
    "truth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbec976b-d376-4dcf-834c-a2e47191be66",
   "metadata": {},
   "source": [
    "Split the sample into two random subsets, with 70% of the galaxies designated for training and 30% for tests by adding an extra column `test`: \n",
    "* `test=0`: galaxies included in the **training** procedure\n",
    "* `test=1`: galaxies included in the **test** procedure, mandatorily excluded from the training procedure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754d2fc2-012f-460a-961d-43189cf7c08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip_if not will_train\n",
    "\n",
    "train_file_hdf5 = 'training_set.hdf5'\n",
    "test_file_hdf5 = 'validation_set.hdf5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4192dc12-27bf-4741-8e60-e570440179a4",
   "metadata": {},
   "source": [
    "#### Basic QA of the representative training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bc4e17-75bc-417c-b5b8-e401a0272449",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip_if not will_train\n",
    "\n",
    "## todo implement here basic qa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32d5ec3-fa75-4700-aadf-8263041ee52c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Realistic spectroscopic sample (TBD)\n",
    "\n",
    "A true-z sample arbitrarily selected from the DC2 simulation to mimic realistic spectroscopic sample regarding the color-magnitude-redshift space, based on current spectroscopic data available from the literature . \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c61c05-8c3a-4974-9763-a013b1b20e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip_if not will_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf98631c-3d87-44c1-ba84-b90cab8bed44",
   "metadata": {},
   "source": [
    "#### Basic QA of the realistic training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b010715-b669-4fea-b41c-5386d5f29d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip_if not will_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f109f543-d0b4-4734-a21b-696d7757818b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1.2.2  Train the photo-z algorithm  \n",
    "\n",
    "Train the photo-z algorithm with RAIL (`rail_inform`). Available options: BPZ, FlexZBoost, GPz, LePHARE,and  TPZ.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336d7cee-c5e2-41bc-aad5-635e22a96564",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip_if not will_train\n",
    "\n",
    "pz_train_configs = utils.load_config(\"pz-train.yaml\")\n",
    "pz_train_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4af7418-29a2-47f3-94d9-6f37dcdd1f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip_if not will_train\n",
    "\n",
    "train_configs = utils.load_config(pz_train_configs['param_file'])\n",
    "train_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35f0380-8a5e-4666-8a0a-b59e1ed26c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip_if not will_train\n",
    "# utils.run_pz_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31567255-7578-4c5f-8bf3-b5b916cf3e25",
   "metadata": {},
   "source": [
    "### SUBMIT TRAINING JOB\n",
    "\n",
    "Once these files are with all the configs you want to, run on of the following commands in the terminal:\n",
    "\n",
    ">```python \n",
    ">pz-train-dev or pz-train\n",
    ">```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef17fed-e81f-47ef-a5f9-8efb2cd45205",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip_if not will_train\n",
    "\n",
    "pz_train_job_id = utils.get_last_job_id()\n",
    "utils.monitor_job(pz_train_job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85663883-845a-4432-821b-c8662c0bd8c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.2.3  Photo-z Validation    \n",
    "\n",
    "### PZ estimates for the Test Set\n",
    "\n",
    "For details over the fields for the yamls look into: https://github.com/linea-it/pz-compute/tree/main/doc/manpages\n",
    "\n",
    "Run `rail_estimate` module to produce the photo-z estimates (PDFs) for the Test Set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dea884e-310c-4d99-af51-1d3d36b4254d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip_if not will_train\n",
    "\n",
    "utils.load_config('process_info.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206072c8-d760-4149-b857-1e1fc90f4980",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip_if not will_train\n",
    "\n",
    "!mkdir input_test\n",
    "!mkdir output_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb77c5d7-6d0b-4c7c-8188-efe961b3a205",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip_if not will_train\n",
    "\n",
    "!mv {test_file_hdf5} input_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6376e6b2-8d47-4694-a825-9fcd3b9e9230",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip_if not will_train\n",
    "\n",
    "pz_compute_configs = utils.load_config('pz-compute.yaml')\n",
    "pz_compute_configs['inputdir'] = 'input_test'\n",
    "pz_compute_configs['outputdir'] = 'output_test'\n",
    "pz_compute_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f16e776-cdde-4c6c-8ada-e37ddb7c8b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip_if not will_train\n",
    "with open('pz-compute.yaml', 'w') as outfile:\n",
    "    yaml.dump(pz_compute_configs, outfile)\n",
    "\n",
    "utils.load_config('pz-compute.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1511f752-68ee-476c-aa93-ed5035e1b458",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip_if not will_train\n",
    "\n",
    "utils.load_config(pz_compute_configs['param_file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d65e292-ab10-4a63-b9f6-a1487546afb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip_if not will_train\n",
    "\n",
    "# utils.run_pz_compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8198cfe-acae-4405-9850-55eb90c69465",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Run test pz-compute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee01242-0f17-4b5e-9539-834a4646e9d5",
   "metadata": {},
   "source": [
    "Once these files are with all the configs you want to, run on of the following commands in the terminal:\n",
    "\n",
    ">```python \n",
    ">pz-compute-dev or pz-compute\n",
    ">```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb625a4-e136-495b-8790-671944738999",
   "metadata": {},
   "source": [
    "### Monitor run test pz-compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08fdb45-c997-4fd1-bb62-e5411b53bb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip_if not will_train\n",
    "\n",
    "pz_compute_job_id = utils.get_last_job_id()\n",
    "utils.monitor_job(pz_compute_job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e720b20c-6a88-49d2-8e3c-4bfb2e5077a9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### PZ validation results\n",
    "\n",
    "for a complete explanation over the metrics, please check out this notebook: https://github.com/linea-it/pz-compute/blob/main/doc/output-validation.ipynb\n",
    "\n",
    "#### Metrics and plots \n",
    "\n",
    "Run `rail_evaluate` module to compute PDF metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da29f0ae-19ef-406c-b7bd-606024a514d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip_if not will_train\n",
    "\n",
    "test_set_output_path = f'{cwd}/output_test/{test_file_hdf5}'\n",
    "pdfs_file_output = find_rail_file(test_set_output_path)\n",
    "table = tables_io.read(pdfs_file_output, fmt='hdf5')\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111de538-dbe7-4ae5-8382-9290714c7921",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Adding Zmode to the output\n",
    "\n",
    "Adding the mode of the pdf generated for each object in the file, each zmode is equivalent to the photoz calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f5c3ab-3a6c-4a17-becb-20e92cdd09b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip_if not will_train\n",
    "\n",
    "utils.add_zmodes(table, pdfs_file_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba44eaa0-be23-4d2e-9dd8-7c1991c107f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip_if not will_train\n",
    "\n",
    "output_pdfs = DS.read_file(pdfs_file_output, QPHandle, pdfs_file_output)\n",
    "output_pdfs().build_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6021c248-1a0b-4d37-b917-e268902d60dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Reading the Truth table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9fe41a-ec71-4d5d-a581-8131c073b4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip_if not will_train\n",
    "\n",
    "ztrue_file = find_rail_file(f'{cwd}/input_test/{test_file_hdf5}')\n",
    "ztrue_data = DS.read_file('ztrue_data', TableHandle, ztrue_file)\n",
    "\n",
    "truth = tables_io.convertObj(ztrue_data(), tables_io.types.PD_DATAFRAME)\n",
    "truth.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad791782-eedc-444f-91a1-5e2fe91c8407",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip_if not will_train\n",
    "x_vals = output_pdfs().metadata()['xvals'] #the photoz bins\n",
    "y_vals = output_pdfs().build_tables()['data']['yvals'] #the pdfs\n",
    "\n",
    "print(f\"ztrue with {len(truth)} objects\")\n",
    "print(f\"pdfs output com {len(y_vals)} objetos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e38c459-73a3-41a9-a6ce-35fdf9028658",
   "metadata": {},
   "source": [
    "#### Point to point metrics - Sumary statistics\n",
    "\n",
    "1. point_stats_iqr: 'Interquatile range from 0.25 to 0.75', i.e., the middle 50% of the distribution of point_stats_ez, robust to outliers\n",
    "2. point_bias: Median of point_stats_ez\n",
    "3. point_outlier_rate: Calculates the catastrophic outlier rate, defined in the Science Book as the number of galaxies with ez larger than max(0.06,3sigma). This keeps the fraction reasonable when sigma is very small.\n",
    "4. point_stats_sigma_mad: Sigma of the median absolute deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990cb3ad-596e-4f98-96fa-ff5348e63c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip_if not will_train\n",
    "\n",
    "stage_dict = dict(\n",
    "    metrics=['point_stats_ez', 'point_stats_iqr', 'point_bias', 'point_outlier_rate', 'point_stats_sigma_mad'],\n",
    "    _random_state=None,\n",
    "    hdf5_groupname='photometry',\n",
    "    point_estimate_key='zmode',\n",
    "    chunk_size=10000,\n",
    "    metric_config={\n",
    "        'point_stats_iqr':{'tdigest_compression': 100},\n",
    "    }\n",
    ")\n",
    "ptp_stage = PointToPointEvaluator.make_stage(name='point_to_point', **stage_dict)\n",
    "ptp_results = ptp_stage.evaluate(output_pdfs, ztrue_data)\n",
    "ptp_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c4b50f-154b-4cd8-9c98-653e90b70757",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip_if not will_train\n",
    "\n",
    "results_df = tables_io.convertObj(ptp_results['summary'](), tables_io.types.PD_DATAFRAME)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37958f9a-2988-435c-bcee-55c1c241b944",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Dist to point metrics - CDF based Metrics\n",
    "\n",
    "1. cdeloss: [Conditional Density Estimation](https://vitaliset.github.io/conditional-density-estimation/)\n",
    "2. pit: [Probability Integral Transform](https://en.wikipedia.org/wiki/Probability_integral_transform)\n",
    "3. cvm: [Cramer-von Mises](https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93von_Mises_criterion)\n",
    "4. ks: [Kolmogorov-Smirnov](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test)\n",
    "5. ad: [Anderson-Darling](https://en.wikipedia.org/wiki/Anderson%E2%80%93Darling_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c92219-d95c-4dc4-9d99-61cdfc9d30d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip_if not will_train\n",
    "\n",
    "cdelossobj = CDELoss(output_pdfs.data, x_vals.ravel(), ztrue_data()['redshift'])\n",
    "cde_stat_and_pval = cdelossobj.evaluate()\n",
    "\n",
    "pitobj = PIT(output_pdfs(), ztrue_data()['redshift'])\n",
    "pit_out_rate = pitobj.evaluate_PIT_outlier_rate()\n",
    "ks_stat_and_pval = pitobj.evaluate_PIT_KS()\n",
    "cvm_stat_and_pval = pitobj.evaluate_PIT_CvM()\n",
    "ad_stat_crit_sig = pitobj.evaluate_PIT_anderson_ksamp()\n",
    "ad_stat_crit_sig_cut = pitobj.evaluate_PIT_anderson_ksamp(pit_min=0.01, pit_max=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5f08cc-33e2-48ca-8f2a-652506551d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip_if not will_train\n",
    "\n",
    "STD_DEV = results_df['point_stats_iqr'][0]\n",
    "BIAS = results_df['point_outlier_rate'][0]\n",
    "OUTRATE = pit_out_rate\n",
    "PIT = cde_stat_and_pval.statistic\n",
    "CDE_LOSS = ad_stat_crit_sig.statistic\n",
    "AD = ad_stat_crit_sig.statistic\n",
    "CVM = cvm_stat_and_pval.statistic\n",
    "KS = ks_stat_and_pval.statistic\n",
    "\n",
    "CDE_LOSS_P = cde_stat_and_pval.p_value\n",
    "AD_P = ad_stat_crit_sig.pvalue\n",
    "CVM_P = cvm_stat_and_pval.pvalue\n",
    "KS_P = ks_stat_and_pval.pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e7499c-672b-4fd4-9804-ac6e82de4a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip_if not will_train\n",
    "\n",
    "markdown_table = f\"\"\"\n",
    "| metric   | classification | values    | limits                   | reference |\n",
    "| :--------| :-------------:| :-------: | :----------------------: | :-------: |\n",
    "| STD DEV  | POINT Metric   | {STD_DEV:.4f} | < 0.05(1 + zphot)       |  [Schmidt et al., 2020](http://doi.org/10.1093/mnras/staa2799)| \n",
    "| BIAS     | POINT Metric   | {BIAS:.4f}    | < 0.003                 | [Schmidt et al., 2020](http://doi.org/10.1093/mnras/staa2799) | \n",
    "| OUTRATE  | POINT Metric   | {OUTRATE:.4f} | < 10%                   | [Schmidt et al., 2020](http://doi.org/10.1093/mnras/staa2799) | \n",
    "| CDE loss | CDE metric     | {PIT:.4f}     | lower the better        | [Izbicki & Lee, 2017](https://arxiv.org/abs/1704.08095)       |\n",
    "| PIT      | PIT Metric     | {CDE_LOSS:.4f} p-value {CDE_LOSS_P:.4f} | aprox 1                 | [Polsterer et al., 2016](https://arxiv.org/abs/1608.08016)    |\n",
    "| AD       | PIT Metrics    | {AD:.4f}    p-value {AD_P:.4f}  | lower for a uniform PIT | [Schmidt et al., 2020](http://doi.org/10.1093/mnras/staa2799) |\n",
    "| CVM      | PIT Metrics    | {CVM:.4f}   p-value {CVM_P:.4f}  | lower for a uniform PIT | [Schmidt et al., 2020](http://doi.org/10.1093/mnras/staa2799) |\n",
    "| KS       | PIT Metrics    | {KS:.4f}    p-value {KS_P:.4f}  | lower for a uniform PIT | [Schmidt et al., 2020](http://doi.org/10.1093/mnras/staa2799) |\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(markdown_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a72111-d9be-47a0-9659-89adf2f498d2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Redshift stacked distribution of the pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36216367-7844-41c7-a1d4-03cd43ddb6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip_if not will_train\n",
    "\n",
    "point_estimate_test = PointEstHistSummarizer.make_stage(name=\"point_estimate_test\")\n",
    "naive_stack_test = NaiveStackSummarizer.make_stage(name=\"naive_stack_test\")\n",
    "\n",
    "point_estimate_ens = point_estimate_test.summarize(output_pdfs)\n",
    "naive_stack_ens = naive_stack_test.summarize(output_pdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11b44c4-9857-4ff5-acd8-af726ab348eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip_if not will_train\n",
    "\n",
    "naive_stack_ens.data.plot_native(xlim=(0, 3)) #pdfs\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2d5221-ad14-42d8-a356-2d4a1243b7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip_if not will_train\n",
    "\n",
    "point_estimate_ens.data.plot_native(xlim=(0, 3)) #zmode\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546c5dec-eb5d-4476-9040-e4506f0f8436",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Zphot vs. Ztrue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb89ce5-4fdf-4016-ac37-859eb3c846b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%skip_if not will_train\n",
    "\n",
    "utils.photoz_specz_plot(ztrue_data, output_pdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538a66d9-a58b-4208-82c4-e18da9c03846",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip_if not will_train\n",
    "\n",
    "utils.plot_pit_qq(output_pdfs.data.objdata()['yvals'], x_vals.ravel(), ztrue_data()['redshift'], \n",
    "                  title=\"PIT-QQ plot\", code=\"FlexZBoost\", pit_out_rate=pit_out_rate, savefig=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e38f99-96e4-454b-92fd-907a58a8098b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b951ed7-b1bc-4547-b999-13a3cfaece77",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip_if not will_train\n",
    "\n",
    "utils.ks_plot(pitobj)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217b0e87-bc94-47fb-a7cb-43cb94c1a099",
   "metadata": {},
   "source": [
    "#### PZ Validation conclusions \n",
    "\n",
    "Quality assessment, comparison with science requirements. \n",
    "\n",
    "Write here your conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688d7554-eac4-4bcb-b3e0-93d932a8ae98",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# 2. Photo-z Compute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83daef93-a3a3-4478-9837-72eefac839c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.load_config('process_info.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486d38a4-847a-4b37-ac4a-c68d5a0785a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.1 Check Pipeline and Algorithm Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d282952-935d-49e3-a307-3e73b60859c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pz_compute_configs = utils.load_config('pz-compute.yaml')\n",
    "pz_compute_configs\n",
    "\n",
    "if pz_compute_configs['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b48a92-a22b-4f19-b207-db5e9a038062",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pz-compute.yaml', 'w') as outfile:\n",
    "    yaml.dump(pz_compute_configs, outfile)\n",
    "\n",
    "utils.load_config('pz-compute.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093f3200-7eb8-4111-bb69-3d743cb8779f",
   "metadata": {},
   "source": [
    "in case you want to specify the apollo machines, modify the sbatch_args param, for example:  \n",
    "\n",
    "```yaml \n",
    "sbatch_args: -N3 -n 216 -w apl08,apl09,apl16\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97eae09-06b4-413d-8850-32a1ee087c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.load_config(pz_compute_configs['param_file'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce89fa37-0921-4f06-8792-87b879440310",
   "metadata": {},
   "source": [
    "## 2.2 Submit pipeline to Apollo cluster \n",
    "\n",
    "Once these files are with all the configs you want to, run on of the following commands in the terminal:\n",
    "\n",
    ">```python \n",
    ">pz-compute-dev or pz-compute\n",
    ">```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8595cb-1a1c-4376-b5c9-c7bd0d3c8f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.run_pz_compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b47da81-95c4-4950-8b7f-fe44de524b6f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.3 Real-time monitoring  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b95ab5-6849-4cad-b09b-f93378561959",
   "metadata": {},
   "outputs": [],
   "source": [
    "! squeue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e31846-704a-4c6d-b8b0-bf67afdc222a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pz_compute_job_id = utils.get_last_job_id()\n",
    "pz_compute_job_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8347393-2824-42b1-95d3-591138aab726",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.monitor_job(pz_compute_job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369fd28c-0899-48a6-ad21-28b19b697445",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fedfdac-c40c-4240-a4a6-ec43cec62621",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_info = utils.load_config('process_info.yaml' )\n",
    "process_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89263a61-5f42-4de8-8bf9-7bd78e4478d6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 3.1 Performance evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80294cc4-315d-4e16-bd81-1a55f968ee51",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.run_post_process_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40cb9dd-dac1-423d-8570-0b44e056d8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_info = utils.load_config('process_info.yaml' )\n",
    "process_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ed5d69-39cb-47bd-98f1-5b683fe595d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = mpimg.imread('processes_time_profiler.png')\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20448fcd-eb7f-4052-a790-7440c360282e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## PZ Estimates - QA of final results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a36238b-541d-4907-b89b-f453712f2c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = mpimg.imread('stack_nz.png')\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b932a1c0-92a7-4a8a-9cdc-748ecb881298",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Export to HTML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d96bb82-8c7a-4afc-ad86-363016172262",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(f'jupyter nbconvert --to html {cwd.split(\"/\")[-1]}.ipynb')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pz_compute_dev",
   "language": "python",
   "name": "pz_compute_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
