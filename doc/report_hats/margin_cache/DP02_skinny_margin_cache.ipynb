{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cea31ded-6bc2-4a0b-8dad-3a93eeab11e9",
   "metadata": {},
   "source": [
    "<img align='left' src = '../../images/linea.png' width=150 style='padding: 20px'> \n",
    "\n",
    "# Report HATS\n",
    "## Generating the margin cache for DP0.2 (Object) Skinny catalog\n",
    "\n",
    "Performance report on LINCC libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1c3331-81a3-4a97-9640-d0b8f3a77f1f",
   "metadata": {},
   "source": [
    "Contacts: Luigi Silva ([luigi.silva@linea.org.br](mailto:luigi.silva@linea.org.br)); Julia Gschwend ([julia@linea.org.br](mailto:julia@linea.org.br)).\n",
    "\n",
    "Last check: 08/11/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8168f1f-b9a4-41de-ba6b-c52b27044dea",
   "metadata": {},
   "source": [
    "#### Acknowledgments\n",
    "\n",
    "'_This notebook used computational resources from the Associação Laboratório Interinstitucional de e-Astronomia (LIneA) with financial support from the INCT of e-Universe (Process No. 465376/2014-2)._'\n",
    "\n",
    "'_This notebook uses libraries from the LSST Interdisciplinary Network for Collaboration and Computing (LINCC) Frameworks project, such as the hats, hats_import, and lsdb libraries. The LINCC Frameworks project is supported by Schmidt Sciences. It is also based on work supported by the National Science Foundation under Grant No. AST-2003196. Additionally, it receives support from the DIRAC Institute at the Department of Astronomy of the University of Washington. The DIRAC Institute is supported by gifts from the Charles and Lisa Simonyi Fund for Arts and Sciences and the Washington Research Foundation._'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a5886a-379d-4abf-8aa1-769558f07fe3",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dd09ef-d61b-4c88-8c76-144578ad3f05",
   "metadata": {},
   "source": [
    "Let us import the packages that we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87f1f35-9c72-4489-9f8b-e71209067a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### GENERAL ##########################\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "import time\n",
    "import math\n",
    "import getpass\n",
    "import warnings\n",
    "import tables_io\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import healpy as hp\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "############################ DASK ############################\n",
    "import dask\n",
    "from dask import dataframe as dd\n",
    "from dask import delayed\n",
    "from dask.distributed import Client, performance_report, wait\n",
    "import dask_jobqueue\n",
    "from dask_jobqueue import SLURMCluster\n",
    "########################## HATS ###########################\n",
    "import hats\n",
    "from hats.inspection.visualize_catalog import plot_pixels\n",
    "from hats.pixel_math import HealpixPixel\n",
    "########################## HATS IMPORT ###########################\n",
    "import hats_import\n",
    "from hats_import.catalog.file_readers import ParquetReader, FitsReader\n",
    "from hats_import.margin_cache.margin_cache_arguments import MarginCacheArguments\n",
    "from hats_import.pipeline import ImportArguments, pipeline_with_client\n",
    "############################ LSDB ############################\n",
    "import lsdb\n",
    "from lsdb.core.search import BoxSearch\n",
    "######################## VISUALIZATION #######################\n",
    "### BOKEH\n",
    "import bokeh\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.models import ColorBar, LinearColorMapper\n",
    "from bokeh.palettes import Viridis256\n",
    "\n",
    "### HOLOVIEWS\n",
    "import holoviews as hv\n",
    "from holoviews import opts\n",
    "from holoviews.operation.datashader import rasterize, dynspread\n",
    "\n",
    "### GEOVIEWS\n",
    "import geoviews as gv\n",
    "import geoviews.feature as gf\n",
    "from cartopy import crs\n",
    "\n",
    "### DATASHADER\n",
    "import datashader as ds\n",
    "\n",
    "### MATPLOTLIB\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Polygon\n",
    "########################## ASTRONOMY #########################\n",
    "from astropy.io import fits\n",
    "from astropy import units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.units.quantity import Quantity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d29eb2-5029-41fe-be9f-0b7913388cd2",
   "metadata": {},
   "source": [
    "Defining the plots to be inline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bca724-d9ac-4422-b5c3-b08737409153",
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.extension('bokeh')\n",
    "gv.extension('bokeh')\n",
    "output_notebook()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461fc517-36d0-4a3d-964d-02c211d7b5f9",
   "metadata": {},
   "source": [
    "Printing the versions of the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5410f242-4f62-49fb-916e-5738415260be",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Getting hats version manually, because it has no __version__ attribute.\n",
    "result = subprocess.run(\n",
    "    [\"conda\", \"run\", \"-p\", \"/lustre/t0/scratch/users/luigi.silva/hats_env_081124\", \"conda\", \"list\", \"hats\"],\n",
    "    stdout=subprocess.PIPE, text=True\n",
    ")\n",
    "for line in result.stdout.splitlines():\n",
    "    if line.startswith(\"hats \"):\n",
    "        hats_version = line.split()[1]\n",
    "        break\n",
    "\n",
    "### Printing the versions.\n",
    "print(f'python version: {sys.version}')\n",
    "print(f'numpy version: {np.__version__}')\n",
    "print(f'dask version: {dask.__version__}')\n",
    "print(f'dask_jobqueue version: {dask_jobqueue.__version__}')\n",
    "print(f'hats version: {hats_version}')\n",
    "print(f'hats_import version: {hats_import.__version__}')\n",
    "print(f'lsdb version: {lsdb.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f50c08-a2de-46f8-9e47-36092ec6826a",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758e7cbb-6c18-4dc8-8a5d-93aec4dc28ee",
   "metadata": {},
   "source": [
    "## Running configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f4dc37-137e-4c96-8b2a-ff2e9b54f5e3",
   "metadata": {},
   "source": [
    "Set the configurations for this running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f7b543-8e53-404d-bd04-d6015b8b0dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO YOU WANT TO RUN THE MARGIN CACHE PIPELINE? \n",
    "run_the_pipeline = False\n",
    "\n",
    "# DO YOU WANT TO SAVE ALL THE DASK JOBS OUTPUTS AND ERRORS OF DASK SLURMCluster?\n",
    "save_the_dask_jobs_info = True\n",
    "\n",
    "# DO YOU WANT TO SAVE ALL THE GENERAL INFORMATIONS OF THIS RUNNING (MAIN LIB VERSIONS, INPUT FILES SIZES, JOBS SCONTROL INFO, OUTPUT FILES SIZES)?\n",
    "save_the_info = True\n",
    "\n",
    "# DO YOU WANT TO SHOW THE INFO INLINE?\n",
    "show_info_inline = True\n",
    "\n",
    "# DO YOU WANT TO CLOSE THE CLIENT AND THE CLUSTER AT THE END?\n",
    "close_the_cluster = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab03e40-4ac8-4b33-b623-8b5187534b55",
   "metadata": {},
   "source": [
    "## Catalogs paths configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01e45c6-fbb3-40b0-80c1-71e33edd4ff8",
   "metadata": {},
   "source": [
    "Defining the path to the INPUT HATS catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00153c4a-3868-476b-8869-c2a2ccf6dc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "hats_input_catalog = '/lustre/t1/cl/lsst/dp02/secondary/catalogs/skinny_hats'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ded222-d1d6-4fc4-8c40-4a375b9f2b13",
   "metadata": {},
   "source": [
    "Defining the name of the RA and DEC columns in the input catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67590daa-3b79-4aa8-9927-14ecf1105fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hats_input_catalog_ra = 'coord_ra'\n",
    "hats_input_catalog_dec = 'coord_dec'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfb8be9-820c-4fe2-9198-1734066ba87d",
   "metadata": {},
   "source": [
    "If you choose not to run the pipeline, give the path to an existing margin cache for the above HATS catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2009de1a-057d-424d-8ea0-c87fbfda1245",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATALOG_MARGIN_CACHE_DIR='/lustre/t1/cl/lsst/pz_project/dp02_skinny_margin_cache'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2ce3ad-2a69-49b7-8396-7012d53de748",
   "metadata": {},
   "source": [
    "Defining the OUTPUT catalog path and name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e695e1d-589a-4c0b-9cff-787390b800cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_the_pipeline==True:\n",
    "    hats_margin_cache_path = '/lustre/t1/cl/lsst/pz_project'\n",
    "    hats_margin_cache_name = f'dp02_skinny_margin_cache'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385ebb81-5efe-493b-849d-dd07855836dc",
   "metadata": {},
   "source": [
    "Defining the USER base path, for saving logs, graphs and other informations about the running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c634f332-4bae-4ecd-b6d6-cbfcb30eee77",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_the_dask_jobs_info or save_the_info:\n",
    "    user = getpass.getuser()\n",
    "    user_base_path = f'/lustre/t0/scratch/users/{user}/report_hats/DP02-skinny-margin-cache'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247be1ee-52cc-434a-9552-f3b096da2dca",
   "metadata": {},
   "source": [
    "### Creating directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21e4616-6171-43e7-a4eb-0d286c26ebe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_the_dask_jobs_info or save_the_info:\n",
    "    os.makedirs(user_base_path, exist_ok=True)\n",
    "\n",
    "    current_date = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "\n",
    "    run_path = os.path.join(user_base_path, f'run_hats_{current_date}')\n",
    "    os.makedirs(run_path, exist_ok=True)\n",
    "\n",
    "    logs_dir = os.path.join(run_path, f'logs')\n",
    "    os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "    dask_logs_dir = os.path.join(logs_dir, f'dask_logs')\n",
    "    os.makedirs(dask_logs_dir , exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce962447-e1f8-403a-a2e2-6270763bdd26",
   "metadata": {},
   "source": [
    "## Cluster configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c22c80b-972c-4c7f-842c-304bc0047f7d",
   "metadata": {},
   "source": [
    "Do you want to customize extra dask parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb07646-367c-4ae7-905f-e334ab1bfe16",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_dask_configs=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a2bc13-c23d-4d40-97ec-b4956d02f2ac",
   "metadata": {},
   "source": [
    "If you choose ```True```, see the explanation of the parameters and customize them below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bbae9f-7c5a-4d2f-8212-555c321e33d8",
   "metadata": {},
   "source": [
    "**Explanation of Parameters**\n",
    "\n",
    "* ```distributed.worker.memory.target```: sets the memory limit before Dask attempts to release memory from completed tasks. At the specified percentage, Dask will start memory collection earlier, reducing the risk of excessive accumulation.\n",
    "\n",
    "* ```distributed.worker.memory.spill```: defines the point at which Dask starts spilling data to disk (swap) instead of keeping it in RAM. This helps free up memory for new tasks.\n",
    "\n",
    "* ```distributed.worker.memory.pause```: when memory usage reaches the specified percentage, Dask will temporarily pause the worker to prevent excessive resource use.\n",
    "\n",
    "* ```distributed.worker.memory.terminate```: if memory usage reaches the specified percentage, the worker will be restarted, which prevents crashes and helps keep usage under control.\n",
    "\n",
    "* ```distributed.worker.memory.recent-to-old```: determines the fraction of recently accessed data Dask considers as “old” and, therefore, eligible for spilling to disk. A lower percentage (e.g., 0.2 for 20%) means only the most recent data is retained in RAM, while older data is more likely to be released, helping to manage cache memory efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3887274e-3f89-4027-a79f-1a716ca27fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if extra_dask_configs==True:\n",
    "    # Additional Dask configurations\n",
    "    dask_config = {\n",
    "        \"distributed.worker.memory.target\": 0.75,         # 75% before starting memory collection\n",
    "        \"distributed.worker.memory.spill\": 0.85,          # 85% before starting to use disk\n",
    "        \"distributed.worker.memory.pause\": 0.92,          # Pause the worker at 92%\n",
    "        \"distributed.worker.memory.terminate\": 0.98,      # Restart the worker at 98%\n",
    "        \"distributed.worker.memory.recent-to-old\": 0.2    # Keep 20% of recent data in memory\n",
    "    }\n",
    "\n",
    "    # Applying the Dask configurations\n",
    "    dask.config.set(dask_config)\n",
    "else:\n",
    "    print(\"Running DASK with the standard memory configuration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f35abe-7545-4c8e-aed2-d82e433b39f5",
   "metadata": {},
   "source": [
    "Defining the configurations for the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a4e509-ad16-45ff-b5ba-a45dde5e40f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "interface=\"ib0\"\n",
    "queue='cpu_small'\n",
    "cores=48         \n",
    "processes=2       \n",
    "memory='114GB'   \n",
    "walltime='04:00:00'\n",
    "\n",
    "if save_the_dask_jobs_info:\n",
    "    job_extra_directives=[\n",
    "        '--propagate',\n",
    "        f'--output={dask_logs_dir}/dask_job_%j_{current_date}.out',  \n",
    "        f'--error={dask_logs_dir}/dask_job_%j_{current_date}.err'\n",
    "    ]\n",
    "else:\n",
    "    job_extra_directives=[\n",
    "        '--propagate',\n",
    "        f'--output=/dev/null',  \n",
    "        f'--error=/dev/null'\n",
    "    ]\n",
    "\n",
    "number_of_nodes=20 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3358b7-f860-4e89-b083-46f12179d27c",
   "metadata": {},
   "source": [
    "Starting the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5d69ae-f2b1-4368-8d9f-7652c93e8f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_date = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "\n",
    "# Configuring the SLURMCluster.\n",
    "cluster = SLURMCluster(\n",
    "    interface=interface,         # Lustre interface\n",
    "    queue=queue,                 # Name of the queue\n",
    "    cores=cores,                 # Number of logical cores per node\n",
    "    processes=processes,         # Number of dask processes per node\n",
    "    memory=memory,               # Memory per node\n",
    "    walltime=walltime,           # Maximum execution time\n",
    "    job_extra_directives=job_extra_directives,\n",
    ")\n",
    "\n",
    "# Scaling the cluster to use X nodes\n",
    "cluster.scale(jobs=number_of_nodes)\n",
    "\n",
    "# Defining the dask client\n",
    "client = Client(cluster)\n",
    "\n",
    "# Wait for the workers to initialize\n",
    "cluster.wait_for_workers(n_workers=number_of_nodes*processes)\n",
    "client.run(lambda: gc.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ff08ee-ddbe-4cab-a1d4-dcc8a153a47f",
   "metadata": {},
   "source": [
    "Showing informations about the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1e7b37-22b5-4e30-9446-d761d7110a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_info = client.cluster\n",
    "cluster_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e84da8-62e5-4da0-aaed-8dba94498da4",
   "metadata": {},
   "source": [
    "Saving the requested resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38764081-349e-4930-8f29-dc504a7f977c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_the_info == True:  \n",
    "\n",
    "    # Specific settings that you want to separate for the memory section\n",
    "    memory_params = {\n",
    "        \"distributed.worker.memory.target\": None,\n",
    "        \"distributed.worker.memory.spill\": None,\n",
    "        \"distributed.worker.memory.pause\": None,\n",
    "        \"distributed.worker.memory.terminate\": None,\n",
    "        \"distributed.worker.memory.recent-to-old\": \"None\",\n",
    "        \"distributed.worker.memory.recent-to-old-time\": \"None\"\n",
    "    }\n",
    "\n",
    "    # Example of requested resource settings\n",
    "    requested_resources = {\n",
    "        \"interface\": f\"{interface}\",\n",
    "        \"queue\": f\"{queue}\",\n",
    "        \"cores\": cores,\n",
    "        \"processes\": processes,\n",
    "        \"memory\": f\"{memory}\",\n",
    "        \"walltime\": f\"{walltime}\",\n",
    "        \"job_extra_directives\": job_extra_directives,\n",
    "        \"number_of_nodes\": number_of_nodes\n",
    "    }\n",
    "\n",
    "    # Getting Dask configurations\n",
    "    dask_config = dask.config.config\n",
    "\n",
    "    # Overwrite the memory parameters if they are set in the Dask configuration\n",
    "    for param in memory_params.keys():\n",
    "        sections = param.split('.')\n",
    "        config = dask_config\n",
    "        for section in sections:\n",
    "            config = config.get(section, None)\n",
    "            if config is None:\n",
    "                break\n",
    "        if config is not None:\n",
    "            memory_params[param] = config\n",
    "\n",
    "    # Preparing sections\n",
    "    output = []\n",
    "\n",
    "    # Requested resources section\n",
    "    output.append(\"# Requested resources\")\n",
    "    for key, value in requested_resources.items():\n",
    "        output.append(f\"{key}={value}\")\n",
    "\n",
    "    # Memory configuration section\n",
    "    output.append(\"\\n# Dask memory configuration:\")\n",
    "    for key, value in memory_params.items():\n",
    "        output.append(f'\"{key}\": {value}')\n",
    "\n",
    "    # Section with all Dask configurations\n",
    "    output.append(\"\\n# Dask all configurations:\")\n",
    "    for section, config in dask_config.items():\n",
    "        if isinstance(config, dict):\n",
    "            output.append(f\"[{section}]\")\n",
    "            for key, value in config.items():\n",
    "                output.append(f\"{key}: {value}\")\n",
    "        else:\n",
    "            output.append(f\"{section}: {config}\")\n",
    "\n",
    "    # Saving to a file or displaying the result\n",
    "    with open(f'{logs_dir}/requested_resources_info.txt', 'w') as f:\n",
    "        f.write(\"\\n\".join(output))\n",
    "\n",
    "    print(\"Informations saved in requested_resources_info.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b6ac1d-7e92-4c66-b63c-17213017f148",
   "metadata": {},
   "source": [
    "# Reading the input catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589e7315-7346-46be-b018-4cdb7139cbc2",
   "metadata": {},
   "source": [
    "Loading the input catalog with hats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d0b5dd-b0ae-4cff-bb1d-89c7c72828d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_hats_catalog_from_disk_lsdb = lsdb.read_hats(hats_input_catalog)\n",
    "loaded_hats_catalog_from_disk_hats = hats.read_hats(hats_input_catalog)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c8300f-f3c2-4172-8ae6-bf9bc0484e19",
   "metadata": {},
   "source": [
    "Making the pixels plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55df398-e8be-4766-b471-7d9f59dd2300",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_the_info==True:\n",
    "    current_date = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "    plot_pixels(loaded_hats_catalog_from_disk_hats)\n",
    "    plt.savefig(f\"{logs_dir}/input_pixels_plot_{current_date}.png\")\n",
    "else:\n",
    "    plot_pixels(loaded_hats_catalog_from_disk_hats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d749736-cb53-4ab8-a6fd-6fd4c8cbc972",
   "metadata": {},
   "source": [
    "Computing the number of rows in the HATS catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d53a420-ced2-44d7-956e-75834b749a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "hats_catalog_total_columns = loaded_hats_catalog_from_disk_lsdb.columns.to_list()\n",
    "hats_catalog_total_rows = loaded_hats_catalog_from_disk_hats.catalog_info.total_rows\n",
    "\n",
    "if show_info_inline == True:\n",
    "    print(f\"HATS catalog path: {hats_input_catalog} \\n\")\n",
    "    print(f\"Total number of rows: {hats_catalog_total_rows}\\n\")\n",
    "    print(f\"Total number of columns: {len(hats_catalog_total_columns)}\\n\\n\")\n",
    "\n",
    "if save_the_info == True:\n",
    "    with open(f'{logs_dir}/input_total_len_of_catalog_{current_date}.txt', 'a') as f:\n",
    "        f.write(f\"HATS catalog path: {hats_input_catalog}\\n\")\n",
    "        f.write(f\"Total number of rows: {hats_catalog_total_rows}\\n\")\n",
    "        f.write(f\"Total number of columns: {len(hats_catalog_total_columns)}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c5d92f-006e-456f-8caf-6b945bf79fb8",
   "metadata": {},
   "source": [
    "### Summarize pixels and sizes of input catalog\n",
    "* \"healpix orders: distinct healpix orders represented in the partitions\n",
    "\n",
    "* num partitions: total number of partition files\n",
    "\n",
    "Size on disk data - using the file sizes fetched above, check the balance of your data. If your rows are fixed-width (e.g. no nested arrays, and few NaNs), the ratio here should be similar to the ratio above. If they’re very different, and you experience problems when parallelizing operations on your data, you may consider re-structuring the data representation.\n",
    "\n",
    "* min size_on_disk: smallest file (in GB)\n",
    "\n",
    "* max size_on_disk: largest file size (in GB)\n",
    "\n",
    "* size_on_disk ratio: max/min\n",
    "\n",
    "total size_on_disk: sum of all parquet catalog files (actual catalog size may vary due to other metadata files)\"\n",
    "\n",
    "Source: https://hats.readthedocs.io/en/stable/notebooks/catalog_size_inspection.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0f439f-203f-4314-96fc-1f13ecc65e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_dir = hats_input_catalog\n",
    "\n",
    "catalog = loaded_hats_catalog_from_disk_hats\n",
    "\n",
    "info_frame = catalog.partition_info.as_dataframe()\n",
    "\n",
    "for index, partition in info_frame.iterrows():\n",
    "    file_name = result = hats.io.paths.pixel_catalog_file(\n",
    "        catalog_dir, HealpixPixel(partition[\"Norder\"], partition[\"Npix\"])\n",
    "    )\n",
    "    info_frame.loc[index, \"size_on_disk\"] = os.path.getsize(file_name)\n",
    "\n",
    "info_frame = info_frame.astype(int)\n",
    "info_frame[\"gbs\"] = info_frame[\"size_on_disk\"] / (1024 * 1024 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed49adc-7f3f-4c2f-99cb-bf89574b971c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_the_info==True:\n",
    "    current_date = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "    with open(f\"{logs_dir}/input_summarize_pixels_{current_date}.txt\", \"w\") as file:\n",
    "        file.write(f'healpix orders: {info_frame[\"Norder\"].unique()}\\n')\n",
    "        file.write(f'num partitions: {len(info_frame[\"Npix\"])}\\n')\n",
    "        file.write(\"------\\n\")\n",
    "        file.write(f'min size_on_disk: {info_frame[\"gbs\"].min():.8f}\\n')\n",
    "        file.write(f'max size_on_disk: {info_frame[\"gbs\"].max():.8f}\\n')\n",
    "        file.write(f'size_on_disk ratio: {info_frame[\"gbs\"].max()/info_frame[\"gbs\"].min():.8f}\\n')\n",
    "        file.write(f'total size_on_disk: {info_frame[\"gbs\"].sum():.8f}\\n')\n",
    "if show_info_inline==True:\n",
    "    print(f'healpix orders: {info_frame[\"Norder\"].unique()}')\n",
    "    print(f'num partitions: {len(info_frame[\"Npix\"])}')\n",
    "    print(\"------\")\n",
    "    print(f'min size_on_disk: {info_frame[\"gbs\"].min():.7f}')\n",
    "    print(f'max size_on_disk: {info_frame[\"gbs\"].max():.7f}')\n",
    "    print(f'size_on_disk ratio: {info_frame[\"gbs\"].max()/info_frame[\"gbs\"].min():.7f}')\n",
    "    print(f'total size_on_disk: {info_frame[\"gbs\"].sum():.7f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f177cd23-5b75-4857-a08a-db1c090f638a",
   "metadata": {},
   "source": [
    "### File size distribution of input catalog\n",
    "\"Below we look at histograms of file sizes.\n",
    "\n",
    "In our initial testing, we find that there’s a “sweet spot” file size of 100MB-1GB. Files that are smaller create more overhead for individual reads. Files that are much larger may create slow-downs when cross-matching between catalogs. Files that are much larger can create out-of-memory issues for dask when loading from disk.\n",
    "\n",
    "The majority of your files should be in the “sweet spot”, and no files in the “too-big” category.\"\n",
    "\n",
    "Source: https://hats.readthedocs.io/en/stable/notebooks/catalog_size_inspection.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63318f3b-10f2-48bf-afbf-84d38f0fa73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_the_info==True:\n",
    "    current_date = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "    plt.hist(info_frame[\"gbs\"], edgecolor='black')\n",
    "    plt.xlabel(\"File size (GB)\")\n",
    "    plt.ylabel(\"Number of files\")\n",
    "    plt.savefig(f\"{logs_dir}/input_file_size_histogram_{current_date}.png\")  \n",
    "    plt.close()  \n",
    "\n",
    "    bins = [0, 0.5, 1, 2, 100]\n",
    "    labels = [\"small-ish\", \"sweet-spot\", \"big-ish\", \"too-big\"]\n",
    "    hist = np.histogram(info_frame[\"gbs\"], bins=bins)[0]\n",
    "    pcts = hist / len(info_frame)\n",
    "\n",
    "    with open(f\"{logs_dir}/input_file_size_distribution_{current_date}.txt\", \"w\") as file:\n",
    "        for i in range(len(labels)):\n",
    "            file.write(f\"{labels[i]} \\t: {hist[i]} \\t({pcts[i]*100:.1f} %)\\n\")\n",
    "            \n",
    "if show_info_inline==True:\n",
    "    plt.hist(info_frame[\"gbs\"], edgecolor='black')\n",
    "    plt.xlabel(\"File size (GB)\")\n",
    "    plt.ylabel(\"Number of files\")\n",
    "    plt.show()\n",
    "\n",
    "    bins = [0, 0.5, 1, 2, 100]\n",
    "    labels = [\"small-ish\", \"sweet-spot\", \"big-ish\", \"too-big\"]\n",
    "    hist = np.histogram(info_frame[\"gbs\"], bins=bins)[0]\n",
    "    pcts = hist / len(info_frame)\n",
    "    for i in range(0, len(labels)):\n",
    "        print(f\"{labels[i]} \\t: {hist[i]} \\t({pcts[i]*100:.1f} %)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e93bcd-49d1-4046-8e49-5e380cc024a6",
   "metadata": {},
   "source": [
    "# Saving libraries and jobs informations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f2a3df-6449-4ff8-99fa-a737a5484923",
   "metadata": {},
   "source": [
    "Saving the libraries versions information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5acac0c-622f-433d-b75a-7fdfada2eb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_the_info==True:\n",
    "    current_date = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "    with open(f'{logs_dir}/main_lib_versions_{current_date}.txt', 'w') as f:\n",
    "        f.write(f'python version: {sys.version} \\n')\n",
    "        f.write(f'numpy version: {np.__version__} \\n')\n",
    "        f.write(f'dask version: {dask.__version__} \\n')\n",
    "        f.write(f'dask_jobqueue version: {dask_jobqueue.__version__} \\n')\n",
    "        f.write(f'hats version: {hats_version} \\n')\n",
    "        f.write(f'hats_import version: {hats_import.__version__} \\n')\n",
    "        f.write(f'lsdb version: {lsdb.__version__} \\n')\n",
    "    print(f'File saved as: {logs_dir}/main_lib_versions_{current_date}.txt \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6645aff1-b25d-4cd2-a1a5-c3f0915f10d8",
   "metadata": {},
   "source": [
    "Defining functions to get informations about the jobs running in the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1849336-3caf-4a5a-96b6-f8488646a6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to collect information about a job using the scontrol show job command\n",
    "def get_scontrol_job_info(job_id):\n",
    "    # Remove any interval or `%` from job_id\n",
    "    clean_job_id = re.sub(r'\\[.*?\\]', '', job_id)\n",
    "    \n",
    "    # Execute scontrol show job\n",
    "    result = subprocess.run(['scontrol', 'show', 'job', clean_job_id], stdout=subprocess.PIPE)\n",
    "    job_info = result.stdout.decode('utf-8')\n",
    "    \n",
    "    job_dict = {}\n",
    "    \n",
    "    # Process the info line by line\n",
    "    for line in job_info.splitlines():\n",
    "        items = line.split()\n",
    "        for item in items:\n",
    "            if \"=\" in item:\n",
    "                key, value = item.split(\"=\", 1)\n",
    "                job_dict[key] = value\n",
    "    \n",
    "    return job_dict\n",
    "\n",
    "# Function to collect information about all jobs of the user\n",
    "def get_all_jobs_info_MINE():\n",
    "    # Gets the username using os.getenv('USER')\n",
    "    user = os.getenv('USER')\n",
    "    \n",
    "    # Captures the list of running jobs for the user\n",
    "    result = subprocess.run(['squeue', '-u', user, '-h', '-o', '%i'], stdout=subprocess.PIPE)\n",
    "    job_ids = result.stdout.decode('utf-8').splitlines()\n",
    "\n",
    "    # Collects information for each job\n",
    "    jobs_info = []\n",
    "    for job_id in job_ids:\n",
    "        # Removes intervals or % from job_id before passing it to scontrol\n",
    "        clean_job_id = re.sub(r'\\[.*?\\]', '', job_id)\n",
    "        try:\n",
    "            job_info = get_scontrol_job_info(clean_job_id)\n",
    "            jobs_info.append(job_info)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing job {job_id}: {e}\")\n",
    "    \n",
    "    # Converts the list of dictionaries into a Pandas DataFrame\n",
    "    df = pd.DataFrame(jobs_info)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Function to collect information about all jobs that do not belong to the current user\n",
    "def get_all_jobs_info_NOT_MINE():\n",
    "    current_user = os.getenv('USER')\n",
    "    \n",
    "    # Captures the list of running jobs\n",
    "    result = subprocess.run(['squeue', '-h', '-o', '%i %u'], stdout=subprocess.PIPE)\n",
    "    job_lines = result.stdout.decode('utf-8').splitlines()\n",
    "    \n",
    "    # Filters jobs from other users\n",
    "    jobs_info = []\n",
    "    for line in job_lines:\n",
    "        job_id, user = line.split()\n",
    "        \n",
    "        # Ignores jobs belonging to the current user\n",
    "        if user != current_user:\n",
    "            # Removes intervals or % from job_id before passing it to scontrol\n",
    "            clean_job_id = re.sub(r'\\[.*?\\]', '', job_id)\n",
    "            try:\n",
    "                job_info = get_scontrol_job_info(clean_job_id)\n",
    "                jobs_info.append(job_info)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing job {job_id}: {e}\")\n",
    "    \n",
    "    # Converts to DataFrame\n",
    "    df = pd.DataFrame(jobs_info)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9c84db-9700-400a-aa0b-c010ea373573",
   "metadata": {},
   "source": [
    "Getting my jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593606a4-e1bf-42fe-9896-0ad3ebdd0772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collects information of all jobs and saves it in the DataFrame\n",
    "df_jobs_MINE = get_all_jobs_info_MINE()\n",
    "\n",
    "if show_info_inline==True:\n",
    "    print(df_jobs_MINE[['JobId','NodeList','NumNodes','NumCPUs','NumTasks','CPUs/Task','TRES']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e85729-f42f-48d7-bce6-0ce997101117",
   "metadata": {},
   "source": [
    "Getting other people jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34159ad-c991-4086-b087-ca3f69606c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collects information of all jobs and saves it in the DataFrame\n",
    "df_jobs_NOT_MINE = get_all_jobs_info_NOT_MINE()\n",
    "\n",
    "if len(df_jobs_NOT_MINE)!=0:\n",
    "    if show_info_inline==True:\n",
    "        print(df_jobs_NOT_MINE[['JobId','NodeList','NumNodes','NumCPUs','NumTasks','CPUs/Task','TRES']])\n",
    "else:\n",
    "    df_jobs_NOT_MINE_EMPTY_MSG = pd.DataFrame({\"EMPTY\": [\"There are no other jobs running in the cluster.\"]})\n",
    "    print(\"There are no other jobs running in the cluster.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e383e2e-b6a1-46e8-9b00-f48e12ebcc14",
   "metadata": {},
   "source": [
    "Saving the data of the jobs in a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2968af24-9c87-4678-b274-37abad45ab2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_the_info==True:\n",
    "    current_date = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "    \n",
    "    file_name_MINE = f'{logs_dir}/jobs_info_MINE_{current_date}.csv'\n",
    "    file_name_NOT_MINE = f'{logs_dir}/jobs_info_NOT_MINE_{current_date}.csv'\n",
    "    \n",
    "    df_jobs_MINE.to_csv(file_name_MINE, index=False)\n",
    "    if len(df_jobs_NOT_MINE)!=0:\n",
    "        df_jobs_NOT_MINE.to_csv(file_name_NOT_MINE, index=False)\n",
    "    else:\n",
    "        df_jobs_NOT_MINE_EMPTY_MSG.to_csv(file_name_NOT_MINE, index=False)\n",
    "        \n",
    "    print(f'Files saved as: \\n')\n",
    "    print(f'{file_name_MINE} \\n')\n",
    "    print(f'{file_name_NOT_MINE} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08688d78-9ee4-4780-b2cc-a0596d046ab4",
   "metadata": {},
   "source": [
    "# Generating the margin cache for the HATS catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a0b509-0797-410e-959e-c692396995e6",
   "metadata": {},
   "source": [
    "Generating the margin cache for the HATS catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bd6ec4-c183-42c2-b15e-a086d2e62f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_the_pipeline==True:\n",
    "    ################################## INPUT CONFIGS #################################\n",
    "    ### Directory of the input catalog.\n",
    "    CATALOG_HATS_DIR = Path(hats_input_catalog)\n",
    "    MARGIN_CACHE_THRESHOLD = 1.0 #arcsec\n",
    "    ###########################################################################################\n",
    "\n",
    "    ################################# CONFIGURAÇÕES DE OUTPUT #################################\n",
    "    ### Name of the margin cache to be saved.\n",
    "    CATALOG_MARGIN_CACHE_NAME = hats_margin_cache_name\n",
    "    \n",
    "    ### Output directory for the margin cache and logs.\n",
    "    HATS_DIR = Path(hats_margin_cache_path)\n",
    "    LOGS_DIR = Path(logs_dir)\n",
    "    \n",
    "    CATALOG_MARGIN_CACHE_DIR = HATS_DIR / CATALOG_MARGIN_CACHE_NAME\n",
    "\n",
    "    ### Path to dask performance report.\n",
    "    PERFORMANCE_REPORT_NAME = f'dask_performance_report_{current_date}.html'\n",
    "    PERFORMANCE_DIR = LOGS_DIR / PERFORMANCE_REPORT_NAME\n",
    "    ###########################################################################################\n",
    "\n",
    "    ############################### EXECUTANDO O PIPELINE ######################################\n",
    "    with performance_report(filename=PERFORMANCE_DIR):   \n",
    "        ### Getting informations from the catalog.\n",
    "        catalog = hats.read_hats(CATALOG_HATS_DIR)\n",
    "        info_frame = catalog.partition_info.as_dataframe()\n",
    "        info_frame = info_frame.astype(int)\n",
    "        \n",
    "        ### Computing the margin cache, if it is possible.\n",
    "        number_of_pixels = len(info_frame[\"Npix\"])\n",
    "        if number_of_pixels <= 1:\n",
    "            warnings.warn(f\"Number of pixels is equal to {number_of_pixels}. Impossible to compute margin cache.\")\n",
    "        else:\n",
    "            margin_cache_args = MarginCacheArguments(\n",
    "                input_catalog_path=CATALOG_HATS_DIR,\n",
    "                output_path=HATS_DIR,\n",
    "                margin_threshold=MARGIN_CACHE_THRESHOLD,  # arcsec\n",
    "                output_artifact_name=CATALOG_MARGIN_CACHE_NAME,\n",
    "            )\n",
    "            pipeline_with_client(margin_cache_args, client)\n",
    "###########################################################################################\n",
    "else:\n",
    "    print('You selected not to run the pipeline.') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19547d2-0269-41ea-9a76-217087f8c736",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Analysing the outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1452ac5-0a31-46c6-ac6a-eaaad3b96e9c",
   "metadata": {},
   "source": [
    "## Loading the catalog with the margin cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba216cc-f14e-457b-8669-1ac58d6fbc7e",
   "metadata": {},
   "source": [
    "Define if you want to load the full catalog or just a region in the sky."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a767428-781f-432e-b732-da557691c36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_full_catalog_with_margin_cache = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ca1e64-9852-4571-adc8-cc72bf963141",
   "metadata": {},
   "source": [
    "If you choose ```False```, select the region of the sky."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff75f01-56f6-462e-9982-0700da1959e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ra=(48, 76)\n",
    "#dec=(-45, -26)\n",
    "#box = BoxSearch(ra=ra, dec=dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82788de-307f-4da7-abe0-9c7aaf913c2c",
   "metadata": {},
   "source": [
    "Loading the catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882fcfea-01ee-4a7b-a3aa-04340bdbfee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_full_catalog_with_margin_cache==True:\n",
    "    loading_message = 'You choose to load the full catalog and margin cache.'\n",
    "    catalog_with_margin = lsdb.read_hats(\n",
    "        hats_input_catalog, margin_cache=CATALOG_MARGIN_CACHE_DIR, columns=[hats_input_catalog_ra, hats_input_catalog_dec]\n",
    "    )\n",
    "else:\n",
    "    loading_message = f'You choose to load a limited portion of the catalog and its margin cache. They were limited by the box search in the region R.A {ra} and DEC {dec}.'\n",
    "    catalog_with_margin = lsdb.read_hats(\n",
    "        hats_input_catalog, search_filter=box, margin_cache=CATALOG_MARGIN_CACHE_DIR, columns=[hats_input_catalog_ra, hats_input_catalog_dec]\n",
    "    )\n",
    "\n",
    "if save_the_info == True:\n",
    "    with open(f'{logs_dir}/margin_cache_catalog_info_{current_date}.txt', 'a') as f:\n",
    "        f.write(loading_message)\n",
    "        f.write(f\"\\n Margin size: {catalog_with_margin.margin.hc_structure.catalog_info.margin_threshold} arcsec \\n\")\n",
    "\n",
    "if show_info_inline == True:\n",
    "    print(loading_message, '\\n')\n",
    "    print(f\"Margin size: {catalog_with_margin.margin.hc_structure.catalog_info.margin_threshold} arcsec \\n\")\n",
    "    print(catalog_with_margin.margin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bd71b7-67bc-42a5-adae-39fa8c6f3a81",
   "metadata": {},
   "source": [
    "## Getting the len of rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f0f24c-dfb8-4c75-8dd5-1f6c77d5262a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_margin_cache_from_disk_lsdb = lsdb.read_hats(CATALOG_MARGIN_CACHE_DIR)\n",
    "loaded_margin_cache_from_disk_hats = hats.read_hats(CATALOG_MARGIN_CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20371fb4-e729-4ea0-9d31-9002ec41fc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_the_info==True:\n",
    "    current_date = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "    plot_pixels(loaded_margin_cache_from_disk_hats)\n",
    "    plt.savefig(f\"{logs_dir}/margin_cache_pixels_plot_{current_date}.png\")\n",
    "else:\n",
    "    plot_pixels(loaded_margin_cache_from_disk_hats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a202ae-c562-4397-8bda-3c0f1ccc5326",
   "metadata": {},
   "outputs": [],
   "source": [
    "margin_cache_total_columns = loaded_margin_cache_from_disk_lsdb.columns.to_list()\n",
    "margin_cache_total_rows = loaded_margin_cache_from_disk_hats.catalog_info.total_rows\n",
    "\n",
    "if show_info_inline == True:\n",
    "    print(f\"Margin cache path: {CATALOG_MARGIN_CACHE_DIR} \\n\")\n",
    "    print(f\"Total number of rows: {margin_cache_total_rows}\\n\")\n",
    "    print(f\"Total number of columns: {len(margin_cache_total_columns)}\\n\\n\")\n",
    "\n",
    "if save_the_info == True:\n",
    "    with open(f'{logs_dir}/margin_cache_total_len_of_catalog_{current_date}.txt', 'a') as f:\n",
    "        f.write(f\"Margin cache path: {CATALOG_MARGIN_CACHE_DIR}\\n\")\n",
    "        f.write(f\"Total number of rows: {margin_cache_total_rows}\\n\")\n",
    "        f.write(f\"Total number of columns: {len(margin_cache_total_columns)}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b9b145-0042-4664-adf4-0723906f227f",
   "metadata": {},
   "source": [
    "## Plotting the margin cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f261f8-98b6-4d4a-8886-2bb165918989",
   "metadata": {},
   "source": [
    "Defining a function to plot the points in a pixel and the pixel boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a63357-edb3-4029-a3c9-94dd13ce8d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_points(\n",
    "    pixel_dfs, order, pixel, colors, ra_columns, dec_columns, xlim=None, ylim=None, markers=None, alphas=None, save_path=None\n",
    "):\n",
    "    ax = plt.subplot()\n",
    "\n",
    "    # Plot hp pixel bounds\n",
    "    nsides = hp.order2nside(order)\n",
    "    pix0_bounds = hp.vec2dir(hp.boundaries(nsides, pixel, step=100, nest=True), lonlat=True)\n",
    "    lon = pix0_bounds[0]\n",
    "    lat = pix0_bounds[1]\n",
    "    vertices = np.vstack([lon.ravel(), lat.ravel()]).transpose()\n",
    "    p = Polygon(vertices, closed=True, edgecolor=\"#3b81db\", facecolor=\"none\")\n",
    "    ax.add_patch(p)\n",
    "\n",
    "    if markers is None:\n",
    "        markers = [\"+\"] * len(pixel_dfs)\n",
    "\n",
    "    if alphas is None:\n",
    "        alphas = [1] * len(pixel_dfs)  # Default to alpha=1 for all dataframes\n",
    "\n",
    "    # Plot the points\n",
    "    for pixel_df, color, ra_column, dec_column, marker, alpha in zip(\n",
    "        pixel_dfs, colors, ra_columns, dec_columns, markers, alphas\n",
    "    ):\n",
    "        ax.scatter(\n",
    "            pixel_df[ra_column].to_numpy(),\n",
    "            pixel_df[dec_column].to_numpy(),\n",
    "            c=color,\n",
    "            marker=marker,\n",
    "            linewidths=1,\n",
    "            alpha=alpha,\n",
    "        )\n",
    "\n",
    "    # Plotting configuration\n",
    "    VIEW_MARGIN = 2\n",
    "    xlim_low = np.min(lon) - VIEW_MARGIN if xlim is None else xlim[0]\n",
    "    xlim_high = np.max(lon) + VIEW_MARGIN if xlim is None else xlim[1]\n",
    "    ylim_low = np.min(lat) - VIEW_MARGIN if ylim is None else ylim[0]\n",
    "    ylim_high = np.max(lat) + VIEW_MARGIN if ylim is None else ylim[1]\n",
    "\n",
    "    plt.xlim(xlim_low, xlim_high)\n",
    "    plt.ylim(ylim_low, ylim_high)\n",
    "    plt.xlabel(\"ra\")\n",
    "    plt.ylabel(\"dec\")\n",
    "\n",
    "    # Save the plot if a save_path is provided\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ef7b46-ed48-4da3-9175-0629d2a125b3",
   "metadata": {},
   "source": [
    "Plotting the points for the first pixel in the info_frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d35ac4-7aca-4bc9-8a84-089c7b7c4ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_with_margin_cache_info_frame = catalog_with_margin.hc_structure.partition_info.as_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39582562-85ca-4fa2-8ecc-e45191eacb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_partition(catalog_with_margin, orders, pixels):\n",
    "    \"\"\"\n",
    "    Função para encontrar o primeiro par válido de order e pixel no catálogo.\n",
    "    \"\"\"\n",
    "    for order, pixel in zip(orders, pixels):\n",
    "        try:\n",
    "            # Tenta acessar a partição\n",
    "            catalog_with_margin.get_partition(order, pixel).compute()\n",
    "            catalog_with_margin.margin.get_partition(order, pixel).compute()\n",
    "            return order, pixel  # Retorna o primeiro par válido\n",
    "        except ValueError:\n",
    "            continue  # Continua para o próximo par\n",
    "    raise ValueError(\"Nenhum pixel válido encontrado no catálogo.\")\n",
    "\n",
    "\n",
    "orders = catalog_with_margin_cache_info_frame['Norder']\n",
    "pixels = catalog_with_margin_cache_info_frame['Npix']\n",
    "\n",
    "try:\n",
    "    order, pixel = get_valid_partition(catalog_with_margin, orders, pixels)\n",
    "    print(f'Plotting points for healpix of order {order} and pixel {pixel}. \\n')\n",
    "\n",
    "    if save_the_info:\n",
    "        current_date = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "        save_path = f\"{logs_dir}/margin_cache_order_{order}_pixel_{pixel}_plot_{current_date}.png\"\n",
    "    else:\n",
    "        save_path = None\n",
    "\n",
    "    plot_points(\n",
    "        [\n",
    "            catalog_with_margin.get_partition(order, pixel).compute(),\n",
    "            catalog_with_margin.margin.get_partition(order, pixel).compute(),\n",
    "        ],\n",
    "        order,\n",
    "        pixel,\n",
    "        [\"green\", \"red\"],\n",
    "        [hats_input_catalog_ra, hats_input_catalog_ra],\n",
    "        [hats_input_catalog_dec, hats_input_catalog_dec],\n",
    "        save_path=save_path,\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b5086e-84e7-44eb-a537-ae53fb37b921",
   "metadata": {},
   "source": [
    "Below, you can personalize the plot if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed991ebb-f85b-4c51-b09c-dd00b6e4b872",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_personalized_plot = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3ea786-2996-4da2-8e4d-2bb4ecb8f511",
   "metadata": {},
   "source": [
    "First, take a look in the info_frame dataframe for getting a pixel order and number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18462a6d-aeea-45e5-91d3-a7655972964e",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_with_margin_cache_info_frame.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6893ec64-0fd1-4262-98df-7003a4766eff",
   "metadata": {},
   "source": [
    "Then, select the pixel you want to plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b487dc2-fec5-4e6d-a9b1-d40d24511bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_personalized_plot==True:\n",
    "    order = 3\n",
    "    pixel = 535"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee1d269-e286-463e-8af8-53c507cd5687",
   "metadata": {},
   "source": [
    "Now, select a region in the sky to restrict the x and y axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3e044f-3528-4343-b716-4ef7db0f6cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "restrict_x_and_y_axis = True\n",
    "\n",
    "if plot_personalized_plot==True:\n",
    "    if restrict_x_and_y_axis==True:\n",
    "        xlim = [72.5,74.5]\n",
    "        ylim = [-37, -34]\n",
    "    else:\n",
    "        xlim = None\n",
    "        ylim = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85ee497-3f5e-47b5-a12d-dac04337a1dc",
   "metadata": {},
   "source": [
    "Select an alpha level for the catalog and its margin cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ba3a73-eefc-4cec-b24a-058770174b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_alpha_levels = True\n",
    "\n",
    "if plot_personalized_plot==True:\n",
    "    if select_alpha_levels==True:\n",
    "        alphas = [0.1,1.0]\n",
    "    else:\n",
    "        alphas = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea01d122-067f-4c3a-8162-ff7a2cb94866",
   "metadata": {},
   "source": [
    "If you want to save the graph, select the place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188805ae-02a1-4842-b89f-eadb4092a2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_the_info == True:\n",
    "    current_date = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "    save_path = f\"{logs_dir}/margin_cache_order_{order}_pixel_{pixel}_personalized_plot_{current_date}.png\"\n",
    "else:\n",
    "    save_path = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1180c283-f742-4e0c-9088-2f44150fab65",
   "metadata": {},
   "source": [
    "Plotting the points from the specified pixel in green, and from the pixel's margin cache in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2fc32a-475a-40c1-ae24-00d3efafc940",
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_personalized_plot == True:\n",
    "    print(f'Plotting points for healpix of order {order} and pixel {pixel}. \\n')\n",
    "        \n",
    "    plot_points(\n",
    "        [\n",
    "            catalog_with_margin.get_partition(order, pixel).compute(),\n",
    "            catalog_with_margin.margin.get_partition(order, pixel).compute(),\n",
    "        ],\n",
    "        order,\n",
    "        pixel,\n",
    "        [\"green\", \"red\"],\n",
    "        [hats_input_catalog_ra, hats_input_catalog_ra],\n",
    "        [hats_input_catalog_dec, hats_input_catalog_dec],\n",
    "        xlim=xlim,\n",
    "        ylim=ylim,\n",
    "        alphas=alphas,\n",
    "        save_path=save_path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a50fc32-6782-4ab4-8c04-a77e23528ba4",
   "metadata": {},
   "source": [
    "## Summarize pixels and sizes of margin cache\n",
    "\n",
    "Source: https://hats.readthedocs.io/en/stable/notebooks/catalog_size_inspection.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20559407-5e2f-40a9-93aa-cfeb6b6683cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_dir = CATALOG_MARGIN_CACHE_DIR\n",
    "\n",
    "catalog = hats.read_hats(catalog_dir)\n",
    "\n",
    "info_frame = catalog.partition_info.as_dataframe()\n",
    "\n",
    "for index, partition in info_frame.iterrows():\n",
    "    file_name = result = hats.io.paths.pixel_catalog_file(\n",
    "        catalog_dir, HealpixPixel(partition[\"Norder\"], partition[\"Npix\"])\n",
    "    )\n",
    "    info_frame.loc[index, \"size_on_disk\"] = os.path.getsize(file_name)\n",
    "\n",
    "info_frame = info_frame.astype(int)\n",
    "info_frame[\"gbs\"] = info_frame[\"size_on_disk\"] / (1024 * 1024 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4f3367-f15a-4e64-aae5-bb3d362efd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_the_info==True:\n",
    "    current_date = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "    with open(f\"{logs_dir}/margin_cache_summarize_pixels_{current_date}.txt\", \"w\") as file:\n",
    "        file.write(f'healpix orders: {info_frame[\"Norder\"].unique()}\\n')\n",
    "        file.write(f'num partitions: {len(info_frame[\"Npix\"])}\\n')\n",
    "        file.write(\"------\\n\")\n",
    "        file.write(f'min size_on_disk: {info_frame[\"gbs\"].min():.8f}\\n')\n",
    "        file.write(f'max size_on_disk: {info_frame[\"gbs\"].max():.8f}\\n')\n",
    "        file.write(f'size_on_disk ratio: {info_frame[\"gbs\"].max()/info_frame[\"gbs\"].min():.8f}\\n')\n",
    "        file.write(f'total size_on_disk: {info_frame[\"gbs\"].sum():.8f}\\n')\n",
    "if show_info_inline==True:\n",
    "    print(f'healpix orders: {info_frame[\"Norder\"].unique()}')\n",
    "    print(f'num partitions: {len(info_frame[\"Npix\"])}')\n",
    "    print(\"------\")\n",
    "    print(f'min size_on_disk: {info_frame[\"gbs\"].min():.7f}')\n",
    "    print(f'max size_on_disk: {info_frame[\"gbs\"].max():.7f}')\n",
    "    print(f'size_on_disk ratio: {info_frame[\"gbs\"].max()/info_frame[\"gbs\"].min():.7f}')\n",
    "    print(f'total size_on_disk: {info_frame[\"gbs\"].sum():.7f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b928465-b5de-4309-b0a1-b26dff092b17",
   "metadata": {},
   "source": [
    "## File size distribution of margin cache\n",
    "\n",
    "Source: https://hats.readthedocs.io/en/stable/notebooks/catalog_size_inspection.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629faa12-7e59-4dba-b8e5-32efb6499200",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_the_info==True:\n",
    "    current_date = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "    plt.hist(info_frame[\"gbs\"], edgecolor='black')\n",
    "    plt.xlabel(\"File size (GB)\")\n",
    "    plt.ylabel(\"Number of files\")\n",
    "    plt.savefig(f\"{logs_dir}/margin_cache_file_size_histogram_{current_date}.png\")  \n",
    "    plt.close()  \n",
    "\n",
    "    bins = [0, 0.5, 1, 2, 100]\n",
    "    labels = [\"small-ish\", \"sweet-spot\", \"big-ish\", \"too-big\"]\n",
    "    hist = np.histogram(info_frame[\"gbs\"], bins=bins)[0]\n",
    "    pcts = hist / len(info_frame)\n",
    "\n",
    "    with open(f\"{logs_dir}/margin_cache_file_size_distribution_{current_date}.txt\", \"w\") as file:\n",
    "        for i in range(len(labels)):\n",
    "            file.write(f\"{labels[i]} \\t: {hist[i]} \\t({pcts[i]*100:.1f} %)\\n\")\n",
    "            \n",
    "if show_info_inline==True:\n",
    "    plt.hist(info_frame[\"gbs\"], edgecolor='black')\n",
    "    plt.xlabel(\"File size (GB)\")\n",
    "    plt.ylabel(\"Number of files\")\n",
    "    plt.show()\n",
    "\n",
    "    bins = [0, 0.5, 1, 2, 100]\n",
    "    labels = [\"small-ish\", \"sweet-spot\", \"big-ish\", \"too-big\"]\n",
    "    hist = np.histogram(info_frame[\"gbs\"], bins=bins)[0]\n",
    "    pcts = hist / len(info_frame)\n",
    "    for i in range(0, len(labels)):\n",
    "        print(f\"{labels[i]} \\t: {hist[i]} \\t({pcts[i]*100:.1f} %)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c92b76-c68d-45b0-b010-1028965434bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Closing the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a2869c-f4f4-4f1e-a3dc-cd58a1bc2ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if close_the_cluster==True:\n",
    "    client.close()\n",
    "    cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hats_env_081124",
   "language": "python",
   "name": "hats_env_081124"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
