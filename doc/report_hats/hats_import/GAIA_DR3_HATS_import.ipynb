{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cea31ded-6bc2-4a0b-8dad-3a93eeab11e9",
   "metadata": {},
   "source": [
    "<img align='left' src = '../../images/linea.png' width=150 style='padding: 20px'> \n",
    "\n",
    "# Report HATS\n",
    "## HATS import on GAIA DR3 catalog\n",
    "\n",
    "Performance report on LINCC libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1c3331-81a3-4a97-9640-d0b8f3a77f1f",
   "metadata": {},
   "source": [
    "Contacts: Luigi Silva ([luigi.silva@linea.org.br](mailto:luigi.silva@linea.org.br)); Julia Gschwend ([julia@linea.org.br](mailto:julia@linea.org.br)).\n",
    "\n",
    "Last check: 08/11/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8168f1f-b9a4-41de-ba6b-c52b27044dea",
   "metadata": {},
   "source": [
    "#### Acknowledgments\n",
    "\n",
    "'_This notebook used computational resources from the Associação Laboratório Interinstitucional de e-Astronomia (LIneA) with financial support from the INCT of e-Universe (Process No. 465376/2014-2)._'\n",
    "\n",
    "'_This notebook uses libraries from the LSST Interdisciplinary Network for Collaboration and Computing (LINCC) Frameworks project, such as the hats, hats_import, and lsdb libraries. The LINCC Frameworks project is supported by Schmidt Sciences. It is also based on work supported by the National Science Foundation under Grant No. AST-2003196. Additionally, it receives support from the DIRAC Institute at the Department of Astronomy of the University of Washington. The DIRAC Institute is supported by gifts from the Charles and Lisa Simonyi Fund for Arts and Sciences and the Washington Research Foundation._'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a5886a-379d-4abf-8aa1-769558f07fe3",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dd09ef-d61b-4c88-8c76-144578ad3f05",
   "metadata": {},
   "source": [
    "Let us import the packages that we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87f1f35-9c72-4489-9f8b-e71209067a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### GENERAL ##########################\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "import time\n",
    "import math\n",
    "import gzip\n",
    "import getpass\n",
    "import warnings\n",
    "import tables_io\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "############################ DASK ############################\n",
    "import dask\n",
    "from dask import dataframe as dd\n",
    "from dask import delayed\n",
    "from dask.distributed import Client, performance_report, wait\n",
    "import dask_jobqueue\n",
    "from dask_jobqueue import SLURMCluster\n",
    "########################## HATS ###########################\n",
    "import hats\n",
    "from hats.inspection.visualize_catalog import plot_pixels\n",
    "from hats.pixel_math import HealpixPixel\n",
    "########################## HATS IMPORT ###########################\n",
    "import hats_import\n",
    "from hats_import.catalog.file_readers import ParquetReader, FitsReader, CsvReader\n",
    "from hats_import.margin_cache.margin_cache_arguments import MarginCacheArguments\n",
    "from hats_import.pipeline import ImportArguments, pipeline_with_client\n",
    "############################ LSDB ############################\n",
    "import lsdb\n",
    "######################## VISUALIZATION #######################\n",
    "### BOKEH\n",
    "import bokeh\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.models import ColorBar, LinearColorMapper\n",
    "from bokeh.palettes import Viridis256\n",
    "\n",
    "### HOLOVIEWS\n",
    "import holoviews as hv\n",
    "from holoviews import opts\n",
    "from holoviews.operation.datashader import rasterize, dynspread\n",
    "\n",
    "### GEOVIEWS\n",
    "import geoviews as gv\n",
    "import geoviews.feature as gf\n",
    "from cartopy import crs\n",
    "\n",
    "### DATASHADER\n",
    "import datashader as ds\n",
    "\n",
    "### MATPLOTLIB\n",
    "import matplotlib.pyplot as plt\n",
    "########################## ASTRONOMY #########################\n",
    "from astropy.io import fits\n",
    "from astropy.io import ascii\n",
    "from astropy import units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.units.quantity import Quantity\n",
    "from astropy.table import Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d29eb2-5029-41fe-be9f-0b7913388cd2",
   "metadata": {},
   "source": [
    "Defining the plots to be inline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bca724-d9ac-4422-b5c3-b08737409153",
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.extension('bokeh')\n",
    "gv.extension('bokeh')\n",
    "output_notebook()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461fc517-36d0-4a3d-964d-02c211d7b5f9",
   "metadata": {},
   "source": [
    "Printing the versions of the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5410f242-4f62-49fb-916e-5738415260be",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Getting hats version manually, because it has no __version__ attribute.\n",
    "result = subprocess.run(\n",
    "    [\"conda\", \"run\", \"-p\", \"/lustre/t0/scratch/users/luigi.silva/hats_env_081124\", \"conda\", \"list\", \"hats\"],\n",
    "    stdout=subprocess.PIPE, text=True\n",
    ")\n",
    "for line in result.stdout.splitlines():\n",
    "    if line.startswith(\"hats \"):\n",
    "        hats_version = line.split()[1]\n",
    "        break\n",
    "\n",
    "### Printing the versions.\n",
    "print(f'python version: {sys.version}')\n",
    "print(f'numpy version: {np.__version__}')\n",
    "print(f'dask version: {dask.__version__}')\n",
    "print(f'dask_jobqueue version: {dask_jobqueue.__version__}')\n",
    "print(f'hats version: {hats_version}')\n",
    "print(f'hats_import version: {hats_import.__version__}')\n",
    "print(f'lsdb version: {lsdb.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f50c08-a2de-46f8-9e47-36092ec6826a",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758e7cbb-6c18-4dc8-8a5d-93aec4dc28ee",
   "metadata": {},
   "source": [
    "## Running configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f4dc37-137e-4c96-8b2a-ff2e9b54f5e3",
   "metadata": {},
   "source": [
    "Set the configurations for this running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f7b543-8e53-404d-bd04-d6015b8b0dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO YOU WANT TO RUN THE HATS IMPORT PIPELINE? \n",
    "run_the_pipeline = True\n",
    "\n",
    "# DO YOU WANT TO LOAD THE INPUT CATALOG AND ITS INFORMATIONS? (IF YOU CHOOSE \"TRUE\" ABOVE, YOU MUST CHOOSE \"TRUE\" HERE)\n",
    "load_the_input_catalog = True\n",
    "\n",
    "# DO YOU WANT TO SAVE ALL THE DASK JOBS OUTPUTS AND ERRORS OF DASK SLURMCluster?\n",
    "save_the_dask_jobs_info = True\n",
    "\n",
    "# DO YOU WANT TO SAVE ALL THE GENERAL INFORMATIONS OF THIS RUNNING (MAIN LIB VERSIONS, INPUT FILES SIZES, JOBS SCONTROL INFO, OUTPUT FILES SIZES)?\n",
    "save_the_info = True\n",
    "\n",
    "# DO YOU WANT TO SHOW THE INFO INLINE?\n",
    "show_info_inline = True\n",
    "\n",
    "# ONE OF THE FINAL ANALYSIS STEPS IS TO COMPUTE THE SIZES OF THE CATALOGS DIRECTLY FROM DISK. THIS MAY TAKE A WHILE. DO YOU WANT TO DO THIS COMPUTATION?\n",
    "compute_sizes_from_disk = True\n",
    "\n",
    "# DO YOU WANT TO CLOSE THE CLIENT AND THE CLUSTER AT THE END?\n",
    "close_the_cluster = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4e1f0c-5e2c-4e07-b864-2065d3e81692",
   "metadata": {},
   "source": [
    "If you choose not to run the pipeline, give the path to a existing HATS catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0f7604-7bca-43cf-a5f5-aa7d1251d098",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CATALOG_HATS_DIR = Path('/lustre/t1/cl/lsst/pz_project/test_data/gaia_dr3_hats')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab03e40-4ac8-4b33-b623-8b5187534b55",
   "metadata": {},
   "source": [
    "## Catalogs paths configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01e45c6-fbb3-40b0-80c1-71e33edd4ff8",
   "metadata": {},
   "source": [
    "Defining the INPUT catalog path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00153c4a-3868-476b-8869-c2a2ccf6dc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_the_input_catalog==True:\n",
    "    original_catalog_path = '/lustre/t1/public/gaia/dr3'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1f21a0-df8e-402b-916f-8a60af64d3cf",
   "metadata": {},
   "source": [
    "Defining the INPUT catalog files to be used. This can be a list or contain a wildcard, ex: ```files_*.parquet```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1e943c-9c76-40da-9b91-c3b7a0b7c1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_the_input_catalog==True:\n",
    "    original_catalog_files = '*.csv.gz'\n",
    "    #original_catalog_files = 'GaiaSource_31*.csv.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2ce3ad-2a69-49b7-8396-7012d53de748",
   "metadata": {},
   "source": [
    "Defining the OUTPUT catalog path and name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e695e1d-589a-4c0b-9cff-787390b800cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_the_pipeline==True:\n",
    "    current_date = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "    \n",
    "    hats_catalog_path = '/lustre/t1/cl/lsst/pz_project/test_data'\n",
    "    hats_catalog_name = f'gaia_dr3_hats'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385ebb81-5efe-493b-849d-dd07855836dc",
   "metadata": {},
   "source": [
    "Defining the USER base path, for saving logs, graphs and other informations about the running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c634f332-4bae-4ecd-b6d6-cbfcb30eee77",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_the_dask_jobs_info or save_the_info:\n",
    "    user = getpass.getuser()\n",
    "    user_base_path = f'/lustre/t0/scratch/users/{user}/report_hats/GAIA-DR3'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247be1ee-52cc-434a-9552-f3b096da2dca",
   "metadata": {},
   "source": [
    "### Creating directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21e4616-6171-43e7-a4eb-0d286c26ebe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_the_dask_jobs_info or save_the_info:\n",
    "    os.makedirs(user_base_path, exist_ok=True)\n",
    "\n",
    "    current_date = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "\n",
    "    run_path = os.path.join(user_base_path, f'run_hats_{current_date}')\n",
    "    os.makedirs(run_path, exist_ok=True)\n",
    "\n",
    "    logs_dir = os.path.join(run_path, f'logs')\n",
    "    os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "    dask_logs_dir = os.path.join(logs_dir, f'dask_logs')\n",
    "    os.makedirs(dask_logs_dir , exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce962447-e1f8-403a-a2e2-6270763bdd26",
   "metadata": {},
   "source": [
    "## Cluster configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c22c80b-972c-4c7f-842c-304bc0047f7d",
   "metadata": {},
   "source": [
    "Do you want to customize extra dask parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb07646-367c-4ae7-905f-e334ab1bfe16",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_dask_configs=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a2bc13-c23d-4d40-97ec-b4956d02f2ac",
   "metadata": {},
   "source": [
    "If you choose ```True```, see the explanation of the parameters and customize them below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bbae9f-7c5a-4d2f-8212-555c321e33d8",
   "metadata": {},
   "source": [
    "**Explanation of Parameters**\n",
    "\n",
    "* ```distributed.worker.memory.target```: sets the memory limit before Dask attempts to release memory from completed tasks. At the specified percentage, Dask will start memory collection earlier, reducing the risk of excessive accumulation.\n",
    "\n",
    "* ```distributed.worker.memory.spill```: defines the point at which Dask starts spilling data to disk (swap) instead of keeping it in RAM. This helps free up memory for new tasks.\n",
    "\n",
    "* ```distributed.worker.memory.pause```: when memory usage reaches the specified percentage, Dask will temporarily pause the worker to prevent excessive resource use.\n",
    "\n",
    "* ```distributed.worker.memory.terminate```: if memory usage reaches the specified percentage, the worker will be restarted, which prevents crashes and helps keep usage under control.\n",
    "\n",
    "* ```distributed.worker.memory.recent-to-old```: determines the fraction of recently accessed data Dask considers as “old” and, therefore, eligible for spilling to disk. A lower percentage (e.g., 0.2 for 20%) means only the most recent data is retained in RAM, while older data is more likely to be released, helping to manage cache memory efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3887274e-3f89-4027-a79f-1a716ca27fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if extra_dask_configs==True:\n",
    "    # Additional Dask configurations\n",
    "    dask_config = {\n",
    "        \"distributed.worker.memory.target\": 0.75,         # 75% before starting memory collection\n",
    "        \"distributed.worker.memory.spill\": 0.85,          # 85% before starting to use disk\n",
    "        \"distributed.worker.memory.pause\": 0.92,          # Pause the worker at 92%\n",
    "        \"distributed.worker.memory.terminate\": 0.98,      # Restart the worker at 98%\n",
    "        \"distributed.worker.memory.recent-to-old\": 0.2    # Keep 20% of recent data in memory\n",
    "    }\n",
    "\n",
    "    # Applying the Dask configurations\n",
    "    dask.config.set(dask_config)\n",
    "else:\n",
    "    print(\"Running DASK with the standard memory configuration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f35abe-7545-4c8e-aed2-d82e433b39f5",
   "metadata": {},
   "source": [
    "Defining the configurations for the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a4e509-ad16-45ff-b5ba-a45dde5e40f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "interface = \"ib0\"\n",
    "queue = \"cpu_small\"\n",
    "cores = 48\n",
    "processes = 2\n",
    "memory = \"114GB\"\n",
    "walltime = \"08:00:00\"\n",
    "\n",
    "account = \"hpc-bpglsst\"\n",
    "\n",
    "if save_the_dask_jobs_info:\n",
    "    job_extra_directives = [\n",
    "        '--propagate',\n",
    "        f'--output={dask_logs_dir}/dask_job_%j_{current_date}.out',  \n",
    "        f'--error={dask_logs_dir}/dask_job_%j_{current_date}.err',\n",
    "        f'--account={account}' \n",
    "    ]\n",
    "else:\n",
    "    job_extra_directives = [\n",
    "        '--propagate',\n",
    "        f'--output=/dev/null',  \n",
    "        f'--error=/dev/null',\n",
    "        f'--account={account}'  \n",
    "    ]\n",
    "\n",
    "number_of_nodes = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3358b7-f860-4e89-b083-46f12179d27c",
   "metadata": {},
   "source": [
    "Starting the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5d69ae-f2b1-4368-8d9f-7652c93e8f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_date = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "\n",
    "# Configuring the SLURMCluster.\n",
    "cluster = SLURMCluster(\n",
    "    interface=interface,         # Lustre interface\n",
    "    queue=queue,                 # Name of the queue\n",
    "    cores=cores,                 # Number of logical cores per node\n",
    "    processes=processes,         # Number of dask processes per node\n",
    "    memory=memory,               # Memory per node\n",
    "    walltime=walltime,           # Maximum execution time\n",
    "    job_extra_directives=job_extra_directives,\n",
    ")\n",
    "\n",
    "# Scaling the cluster to use X nodes\n",
    "cluster.scale(jobs=number_of_nodes)\n",
    "\n",
    "# Defining the dask client\n",
    "client = Client(cluster)\n",
    "\n",
    "# Wait for the workers to initialize\n",
    "cluster.wait_for_workers(n_workers=number_of_nodes*processes)\n",
    "client.run(lambda: gc.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ff08ee-ddbe-4cab-a1d4-dcc8a153a47f",
   "metadata": {},
   "source": [
    "Showing informations about the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1e7b37-22b5-4e30-9446-d761d7110a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_info = client.cluster\n",
    "cluster_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e84da8-62e5-4da0-aaed-8dba94498da4",
   "metadata": {},
   "source": [
    "Saving the requested resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38764081-349e-4930-8f29-dc504a7f977c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_the_info == True:  \n",
    "\n",
    "    # Specific settings that you want to separate for the memory section\n",
    "    memory_params = {\n",
    "        \"distributed.worker.memory.target\": None,\n",
    "        \"distributed.worker.memory.spill\": None,\n",
    "        \"distributed.worker.memory.pause\": None,\n",
    "        \"distributed.worker.memory.terminate\": None,\n",
    "        \"distributed.worker.memory.recent-to-old\": \"None\",\n",
    "        \"distributed.worker.memory.recent-to-old-time\": \"None\"\n",
    "    }\n",
    "\n",
    "    # Example of requested resource settings\n",
    "    requested_resources = {\n",
    "        \"interface\": f\"{interface}\",\n",
    "        \"queue\": f\"{queue}\",\n",
    "        \"cores\": cores,\n",
    "        \"processes\": processes,\n",
    "        \"memory\": f\"{memory}\",\n",
    "        \"walltime\": f\"{walltime}\",\n",
    "        \"job_extra_directives\": job_extra_directives,\n",
    "        \"number_of_nodes\": number_of_nodes\n",
    "    }\n",
    "\n",
    "    # Getting Dask configurations\n",
    "    dask_config = dask.config.config\n",
    "\n",
    "    # Overwrite the memory parameters if they are set in the Dask configuration\n",
    "    for param in memory_params.keys():\n",
    "        sections = param.split('.')\n",
    "        config = dask_config\n",
    "        for section in sections:\n",
    "            config = config.get(section, None)\n",
    "            if config is None:\n",
    "                break\n",
    "        if config is not None:\n",
    "            memory_params[param] = config\n",
    "\n",
    "    # Preparing sections\n",
    "    output = []\n",
    "\n",
    "    # Requested resources section\n",
    "    output.append(\"# Requested resources\")\n",
    "    for key, value in requested_resources.items():\n",
    "        output.append(f\"{key}={value}\")\n",
    "\n",
    "    # Memory configuration section\n",
    "    output.append(\"\\n# Dask memory configuration:\")\n",
    "    for key, value in memory_params.items():\n",
    "        output.append(f'\"{key}\": {value}')\n",
    "\n",
    "    # Section with all Dask configurations\n",
    "    output.append(\"\\n# Dask all configurations:\")\n",
    "    for section, config in dask_config.items():\n",
    "        if isinstance(config, dict):\n",
    "            output.append(f\"[{section}]\")\n",
    "            for key, value in config.items():\n",
    "                output.append(f\"{key}: {value}\")\n",
    "        else:\n",
    "            output.append(f\"{section}: {config}\")\n",
    "\n",
    "    # Saving to a file or displaying the result\n",
    "    with open(f'{logs_dir}/requested_resources_info.txt', 'w') as f:\n",
    "        f.write(\"\\n\".join(output))\n",
    "\n",
    "    print(\"Informations saved in requested_resources_info.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c15f63-17f6-4b0c-9ea5-4b9dfb88a1f7",
   "metadata": {},
   "source": [
    "## Catalogs columns configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b6ac1d-7e92-4c66-b63c-17213017f148",
   "metadata": {},
   "source": [
    "### Reading the catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eec0832-19f5-42c5-a8b6-a78689dab7b6",
   "metadata": {},
   "source": [
    "Getting the name of the catalog files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b133e696-07e4-478b-a9a4-d4da8cc0ab58",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_the_input_catalog==True:\n",
    "    total_files = [f for f in glob.glob(os.path.join(original_catalog_path, original_catalog_files))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7463a35d-aefa-4577-bd17-7422975c7263",
   "metadata": {},
   "source": [
    "Getting the type of files in the catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9726ea6e-3065-4033-bed3-795b8fa608c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_ecsv(file_path):\n",
    "    \"\"\"\n",
    "    Verify if the file is a ECSV.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        open_func = gzip.open if file_path.endswith('.gz') else open\n",
    "        with open_func(file_path, 'rt') as f:\n",
    "            first_line = f.readline().strip()\n",
    "            return first_line == '# %ECSV 1.0'\n",
    "    except Exception as e:\n",
    "        print(f\"Error verifying file {file_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "composed_extensions = ['.csv.gz', '.csv.zip', '.csv.bz2', '.csv.xz']\n",
    "\n",
    "if load_the_input_catalog == True:\n",
    "    if total_files:\n",
    "        file_name = os.path.basename(total_files[0])\n",
    "        file_extension = next(\n",
    "            (ext for ext in composed_extensions if file_name.endswith(ext)), None\n",
    "        )\n",
    "\n",
    "        if not file_extension:\n",
    "            _, file_extension = os.path.splitext(file_name)\n",
    "        \n",
    "        print(\"Files extension:\", file_extension)\n",
    "        \n",
    "        if is_ecsv(total_files[0]):\n",
    "            print(f\"File {total_files[0]} is a ECSV.\")\n",
    "            file_sub_extension = 'ECSV'\n",
    "            print(f\"Files sub extension:\", file_sub_extension)\n",
    "    else:\n",
    "        print(\"The list of files is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2255a190-7593-4329-9584-9aa1563cad4c",
   "metadata": {},
   "source": [
    "Reading the catalog with dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f038ccc7-d772-4154-b0fc-e2df0da511ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fits_to_df(filename):\n",
    "    with fits.open(filename) as hdul:\n",
    "        data = hdul[1].data\n",
    "        df = pd.DataFrame(data.byteswap().newbyteorder())\n",
    "    return df\n",
    "\n",
    "def read_ecsv_to_df(filename):\n",
    "    table = Table.read(filename, format='ascii.ecsv')\n",
    "    return table.to_pandas()\n",
    "\n",
    "if load_the_input_catalog==True:\n",
    "    if file_extension in ['.parquet', '.parq', '.pq']:\n",
    "        ddf_original = dd.read_parquet(total_files)\n",
    "    elif file_extension in ['.fits', '.fit', '.fts']:\n",
    "        delayed_dfs = [delayed(read_fits_to_df)(file) for file in total_files]\n",
    "        ddf_original = dd.from_delayed(delayed_dfs)\n",
    "    elif file_extension in ['.csv', '.csv.gz', '.csv.zip', '.csv.bz2', '.csv.xz', '.csv.zst']:\n",
    "        if file_sub_extension in ['ECSV']:\n",
    "            ddf_original = dd.read_csv(total_files, comment='#', blocksize=None, assume_missing=True)\n",
    "        else:\n",
    "            ddf_original = dd.read_csv(total_files, comment='#')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916e5337-4536-441c-88c4-e23b10263ae4",
   "metadata": {},
   "source": [
    "### Getting the columns of the catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02e4cc9-ec08-4151-8d18-dbbf1d52db51",
   "metadata": {},
   "source": [
    "Getting the name of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925657dc-df7e-499a-bdad-5869e206c0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Is your id column in the index of the catalog? If so, you must choose True here.\n",
    "id_column_is_the_index = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c338737-f17b-4cb6-ac7f-2909bca2b977",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_the_input_catalog==True:\n",
    "    original_catalog_columns = ddf_original.columns\n",
    "    original_catalog_columns_list = original_catalog_columns.to_list()\n",
    "    if id_column_is_the_index==True:\n",
    "        index_head = ddf_original.index.head(10)\n",
    "        index_name = index_head.name\n",
    "        original_catalog_columns_list.append(index_name)\n",
    "    if show_info_inline==True:\n",
    "        print(original_catalog_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b326eab0-4509-457f-879c-f77ced346256",
   "metadata": {},
   "source": [
    "Select the columns you want to use for the HATS conversion. By default, we have all the catalog columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42862cc9-d067-4bf3-a6b8-4dc5ed40a0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_the_input_catalog==True:\n",
    "    selected_catalog_columns = original_catalog_columns_list\n",
    "    #selected_catalog_columns = ['solution_id', 'designation', 'source_id', 'random_index', 'ref_epoch',\n",
    "    #                            'ra', 'ra_error', 'dec', 'dec_error', 'parallax']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc82cac4-c7b5-461d-8c37-30cc41a5e3ea",
   "metadata": {},
   "source": [
    "Getting all the columns containing the strings 'id', 'ra' or 'dec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39be119-3123-49c5-aa91-26508ee4a170",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_the_input_catalog==True:\n",
    "    original_id_column_list = [coluna for coluna in selected_catalog_columns if re.search(r\"id\", coluna, re.IGNORECASE)]\n",
    "    original_ra_column_list = [coluna for coluna in selected_catalog_columns if re.search(r\"ra\", coluna, re.IGNORECASE)]\n",
    "    original_dec_column_list = [coluna for coluna in selected_catalog_columns if re.search(r\"dec\", coluna, re.IGNORECASE)]\n",
    "    \n",
    "    if load_the_input_catalog==True:\n",
    "        print(original_id_column_list, '\\n')\n",
    "        print(original_ra_column_list, '\\n')\n",
    "        print(original_dec_column_list, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c54cc2-9588-4c24-8a8e-1f61cd5509b2",
   "metadata": {},
   "source": [
    "Defining the names of the ID, RA and DEC columns of the input catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cb561b-f5b1-4125-9523-f8efee39ab8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_the_input_catalog==True:\n",
    "    original_id_column_name = 'source_id'\n",
    "    original_ra_column_name = 'ra'\n",
    "    original_dec_column_name = 'dec'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e93bcd-49d1-4046-8e49-5e380cc024a6",
   "metadata": {},
   "source": [
    "# Saving libraries and jobs informations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f2a3df-6449-4ff8-99fa-a737a5484923",
   "metadata": {},
   "source": [
    "Saving the libraries versions information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5acac0c-622f-433d-b75a-7fdfada2eb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_the_info==True:\n",
    "    current_date = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "    with open(f'{logs_dir}/main_lib_versions_{current_date}.txt', 'w') as f:\n",
    "        f.write(f'python version: {sys.version} \\n')\n",
    "        f.write(f'numpy version: {np.__version__} \\n')\n",
    "        f.write(f'dask version: {dask.__version__} \\n')\n",
    "        f.write(f'dask_jobqueue version: {dask_jobqueue.__version__} \\n')\n",
    "        f.write(f'hats version: {hats_version} \\n')\n",
    "        f.write(f'hats_import version: {hats_import.__version__} \\n')\n",
    "        f.write(f'lsdb version: {lsdb.__version__} \\n')\n",
    "    print(f'File saved as: {logs_dir}/main_lib_versions_{current_date}.txt \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6645aff1-b25d-4cd2-a1a5-c3f0915f10d8",
   "metadata": {},
   "source": [
    "Defining functions to get informations about the jobs running in the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1849336-3caf-4a5a-96b6-f8488646a6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to collect information about a job using the scontrol show job command\n",
    "def get_scontrol_job_info(job_id):\n",
    "    # Remove any interval or `%` from job_id\n",
    "    clean_job_id = re.sub(r'\\[.*?\\]', '', job_id)\n",
    "    \n",
    "    # Execute scontrol show job\n",
    "    result = subprocess.run(['scontrol', 'show', 'job', clean_job_id], stdout=subprocess.PIPE)\n",
    "    job_info = result.stdout.decode('utf-8')\n",
    "    \n",
    "    job_dict = {}\n",
    "    \n",
    "    # Process the info line by line\n",
    "    for line in job_info.splitlines():\n",
    "        items = line.split()\n",
    "        for item in items:\n",
    "            if \"=\" in item:\n",
    "                key, value = item.split(\"=\", 1)\n",
    "                job_dict[key] = value\n",
    "    \n",
    "    return job_dict\n",
    "\n",
    "# Function to collect information about all jobs of the user\n",
    "def get_all_jobs_info_MINE():\n",
    "    # Gets the username using os.getenv('USER')\n",
    "    user = os.getenv('USER')\n",
    "    \n",
    "    # Captures the list of running jobs for the user\n",
    "    result = subprocess.run(['squeue', '-u', user, '-h', '-o', '%i'], stdout=subprocess.PIPE)\n",
    "    job_ids = result.stdout.decode('utf-8').splitlines()\n",
    "\n",
    "    # Collects information for each job\n",
    "    jobs_info = []\n",
    "    for job_id in job_ids:\n",
    "        # Removes intervals or % from job_id before passing it to scontrol\n",
    "        clean_job_id = re.sub(r'\\[.*?\\]', '', job_id)\n",
    "        try:\n",
    "            job_info = get_scontrol_job_info(clean_job_id)\n",
    "            jobs_info.append(job_info)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing job {job_id}: {e}\")\n",
    "    \n",
    "    # Converts the list of dictionaries into a Pandas DataFrame\n",
    "    df = pd.DataFrame(jobs_info)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Function to collect information about all jobs that do not belong to the current user\n",
    "def get_all_jobs_info_NOT_MINE():\n",
    "    current_user = os.getenv('USER')\n",
    "    \n",
    "    # Captures the list of running jobs\n",
    "    result = subprocess.run(['squeue', '-h', '-o', '%i %u'], stdout=subprocess.PIPE)\n",
    "    job_lines = result.stdout.decode('utf-8').splitlines()\n",
    "    \n",
    "    # Filters jobs from other users\n",
    "    jobs_info = []\n",
    "    for line in job_lines:\n",
    "        job_id, user = line.split()\n",
    "        \n",
    "        # Ignores jobs belonging to the current user\n",
    "        if user != current_user:\n",
    "            # Removes intervals or % from job_id before passing it to scontrol\n",
    "            clean_job_id = re.sub(r'\\[.*?\\]', '', job_id)\n",
    "            try:\n",
    "                job_info = get_scontrol_job_info(clean_job_id)\n",
    "                jobs_info.append(job_info)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing job {job_id}: {e}\")\n",
    "    \n",
    "    # Converts to DataFrame\n",
    "    df = pd.DataFrame(jobs_info)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9c84db-9700-400a-aa0b-c010ea373573",
   "metadata": {},
   "source": [
    "Getting my jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593606a4-e1bf-42fe-9896-0ad3ebdd0772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collects information of all jobs and saves it in the DataFrame\n",
    "df_jobs_MINE = get_all_jobs_info_MINE()\n",
    "\n",
    "if show_info_inline==True:\n",
    "    print(df_jobs_MINE[['JobId','NodeList','NumNodes','NumCPUs','NumTasks','CPUs/Task','TRES']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e85729-f42f-48d7-bce6-0ce997101117",
   "metadata": {},
   "source": [
    "Getting other people jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34159ad-c991-4086-b087-ca3f69606c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collects information of all jobs and saves it in the DataFrame\n",
    "df_jobs_NOT_MINE = get_all_jobs_info_NOT_MINE()\n",
    "\n",
    "if len(df_jobs_NOT_MINE)!=0:\n",
    "    if show_info_inline==True:\n",
    "        print(df_jobs_NOT_MINE[['JobId','NodeList','NumNodes','NumCPUs','NumTasks','CPUs/Task','TRES']])\n",
    "else:\n",
    "    df_jobs_NOT_MINE_EMPTY_MSG = pd.DataFrame({\"EMPTY\": [\"There are no other jobs running in the cluster.\"]})\n",
    "    print(\"There are no other jobs running in the cluster.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e383e2e-b6a1-46e8-9b00-f48e12ebcc14",
   "metadata": {},
   "source": [
    "Saving the data of the jobs in a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2968af24-9c87-4678-b274-37abad45ab2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_the_info==True:\n",
    "    current_date = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "    \n",
    "    file_name_MINE = f'{logs_dir}/jobs_info_MINE_{current_date}.csv'\n",
    "    file_name_NOT_MINE = f'{logs_dir}/jobs_info_NOT_MINE_{current_date}.csv'\n",
    "    \n",
    "    df_jobs_MINE.to_csv(file_name_MINE, index=False)\n",
    "    if len(df_jobs_NOT_MINE)!=0:\n",
    "        df_jobs_NOT_MINE.to_csv(file_name_NOT_MINE, index=False)\n",
    "    else:\n",
    "        df_jobs_NOT_MINE_EMPTY_MSG.to_csv(file_name_NOT_MINE, index=False)\n",
    "        \n",
    "    print(f'Files saved as: \\n')\n",
    "    print(f'{file_name_MINE} \\n')\n",
    "    print(f'{file_name_NOT_MINE} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4f23f5-d0e6-4363-ba9f-e92fa2b6663f",
   "metadata": {},
   "source": [
    "# Preview of the input files and catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232a6e00-e234-4eef-8fa2-67b379bbd453",
   "metadata": {},
   "source": [
    "Preview of some filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd7d7ec-b4fa-4a96-b9b2-fab5fcc843b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_the_input_catalog==True:\n",
    "    if show_info_inline==True:\n",
    "        for item in total_files[0:5]:\n",
    "            print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a89128c-9f6f-4f9f-81f0-7803edea75dd",
   "metadata": {},
   "source": [
    "First lines of the catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb995f9a-278c-4947-b89a-4c8e71cf4815",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_the_input_catalog==True:\n",
    "    if show_info_inline==True:\n",
    "        print(ddf_original.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c77b07f-db5c-4140-98dc-260591f3c46e",
   "metadata": {},
   "source": [
    "## Summarize pixels and sizes for input catalog files\n",
    "\n",
    "Size on disk data - check the balance of your data. \n",
    "\n",
    "* min size_on_disk: smallest file (in GB)\n",
    "\n",
    "* max size_on_disk: largest file size (in GB)\n",
    "\n",
    "* size_on_disk ratio: max/min\n",
    "\n",
    "* total size_on_disk: sum of all catalog files sizes\n",
    "\n",
    "Source: https://hats.readthedocs.io/en/stable/notebooks/catalog_size_inspection.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3171ab-a7b3-48aa-8779-22c54df38a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_the_input_catalog == True:\n",
    "    file_info_list = []\n",
    "\n",
    "    for file_path in total_files:\n",
    "        try:\n",
    "            file_size = os.stat(file_path).st_size  # Size in bytes\n",
    "            file_size_gb = file_size / (1024 ** 3)  # Converting bytes to GB\n",
    "            file_info_list.append({\"file\": file_path, \"size_on_disk\": file_size, \"gbs\": file_size_gb})\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "\n",
    "    input_info_frame = pd.DataFrame(file_info_list)\n",
    "\n",
    "    num_partitions = len(input_info_frame)\n",
    "    min_size_on_disk = input_info_frame[\"gbs\"].min() if not input_info_frame.empty else 0\n",
    "    max_size_on_disk = input_info_frame[\"gbs\"].max() if not input_info_frame.empty else 0\n",
    "    total_size_on_disk = input_info_frame[\"gbs\"].sum()\n",
    "    size_on_disk_ratio = max_size_on_disk / min_size_on_disk if min_size_on_disk > 0 else float('inf')\n",
    "\n",
    "    if show_info_inline == True:\n",
    "        print(f\"num partitions: {num_partitions}\")\n",
    "        print(f\"min size_on_disk: {min_size_on_disk:.6f} GB\")\n",
    "        print(f\"max size_on_disk: {max_size_on_disk:.6f} GB\")\n",
    "        print(f\"total size_on_disk: {total_size_on_disk:.6f} GB\")\n",
    "        print(f\"size_on_disk_ratio: {size_on_disk_ratio:.6f}\")\n",
    "\n",
    "    if save_the_info == True:\n",
    "        current_date = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "        with open(f\"{logs_dir}/input_summarize_pixels_{current_date}.txt\", \"w\") as summary_file:\n",
    "            summary_file.write(f\"num partitions: {num_partitions}\\n\")\n",
    "            summary_file.write(f\"min size_on_disk: {min_size_on_disk:.6f} GB\\n\")\n",
    "            summary_file.write(f\"max size_on_disk: {max_size_on_disk:.6f} GB\\n\")\n",
    "            summary_file.write(f\"total size_on_disk: {total_size_on_disk:.6f} GB\\n\")\n",
    "            summary_file.write(f\"size_on_disk_ratio: {size_on_disk_ratio:.6f}\\n\")\n",
    "\n",
    "        with open(f\"{logs_dir}/input_files_paths_{current_date}.txt\", \"w\") as paths_file:\n",
    "            for file_path in total_files:\n",
    "                paths_file.write(f\"{file_path}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bce411f-7ad6-47fc-8db9-59b014ceaa56",
   "metadata": {},
   "source": [
    "## File size distribution for input catalog files\n",
    "Below we look at histograms of file sizes.\n",
    "\n",
    "In our initial testing, we find that there’s a “sweet spot” file size of 100MB-1GB. Files that are smaller create more overhead for individual reads. Files that are much larger may create slow-downs when cross-matching between catalogs. Files that are much larger can create out-of-memory issues for dask when loading from disk.\n",
    "\n",
    "The majority of your files should be in the “sweet spot”, and no files in the “too-big” category.\"\n",
    "\n",
    "Source: https://hats.readthedocs.io/en/stable/notebooks/catalog_size_inspection.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b710f2b0-637f-4d82-843e-44cf6e06562b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file_size_info(info_frame, type_of_files, bins, labels, logs_dir, save=False, show=False):\n",
    "    \n",
    "    plt.hist(info_frame[\"gbs\"], edgecolor='black')\n",
    "    plt.xlabel(\"File size (GB)\")\n",
    "    plt.ylabel(\"Number of files\")\n",
    "\n",
    "    if save:\n",
    "        current_date = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "        plt.savefig(f\"{logs_dir}/{type_of_files}_file_size_histogram_{current_date}.png\")\n",
    "\n",
    "        hist = np.histogram(info_frame[\"gbs\"], bins=bins)[0]\n",
    "        pcts = hist / len(info_frame)\n",
    "\n",
    "        with open(f\"{logs_dir}/{type_of_files}_file_size_distribution_{current_date}.txt\", \"w\") as file:\n",
    "            for i, label in enumerate(labels):\n",
    "                file.write(f\"{label} \\t: {hist[i]} \\t({pcts[i]*100:.1f} %)\\n\")\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "        hist = np.histogram(info_frame[\"gbs\"], bins=bins)[0]\n",
    "        pcts = hist / len(info_frame)\n",
    "\n",
    "        for i, label in enumerate(labels):\n",
    "            print(f\"{label} \\t: {hist[i]} \\t({pcts[i]*100:.1f} %)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4508c7fd-bc35-47fb-b4cd-63bcb6c77fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_of_files = 'input'\n",
    "bins = [0, 0.5, 1, 2, 100]\n",
    "labels = [\"small-ish\", \"sweet-spot\", \"big-ish\", \"too-big\"]\n",
    "\n",
    "if save_the_info:\n",
    "    logs_dir = logs_dir\n",
    "else:\n",
    "    logs_dir=None\n",
    "\n",
    "process_file_size_info(\n",
    "    info_frame=input_info_frame,\n",
    "    type_of_files=type_of_files,\n",
    "    bins=bins,\n",
    "    labels=labels,\n",
    "    logs_dir=logs_dir,\n",
    "    save=save_the_info,\n",
    "    show=show_info_inline\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08688d78-9ee4-4780-b2cc-a0596d046ab4",
   "metadata": {},
   "source": [
    "# Converting the catalog to HATS format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a0b509-0797-410e-959e-c692396995e6",
   "metadata": {},
   "source": [
    "Generating the HATS catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e194643-0f49-4272-83a2-9bd4903a4a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_the_pipeline==True:\n",
    "    current_date = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "\n",
    "    ################################## INPUT CONFIGS #################################\n",
    "    ### Directory and name of the input files. The name can be a list or contain a wildcard, ex: files_*.parquet.\n",
    "    CATALOG_DIR = Path(original_catalog_path)\n",
    "    CATALOG_FILES = original_catalog_files\n",
    "    ### Columns to be selected in the input files. The id, ra e dec columns are essential.\n",
    "    CATALOG_SELECTED_COLUMNS = selected_catalog_columns\n",
    "    CATALOG_SORT_COLUMN = original_id_column_name\n",
    "    CATALOG_RA_COLUMN = original_ra_column_name\n",
    "    CATALOG_DEC_COLUMN = original_dec_column_name\n",
    "    ### Type of the files we will read.\n",
    "    FILE_TYPE = file_extension\n",
    "    if file_sub_extension:\n",
    "        FILE_SUB_TYPE = file_sub_extension\n",
    "    else:\n",
    "        FILE_SUB_TYPE = None\n",
    "    ###########################################################################################\n",
    "\n",
    "    ################################# OUTPUT CONFIGS #################################\n",
    "    ### Name of the HATS catalog to be saved.\n",
    "    CATALOG_HATS_NAME = hats_catalog_name\n",
    "\n",
    "    ### Output directory for the catalog and logs.\n",
    "    HATS_DIR = Path(hats_catalog_path)\n",
    "    LOGS_DIR = Path(logs_dir)\n",
    "\n",
    "    CATALOG_HATS_DIR = HATS_DIR / CATALOG_HATS_NAME\n",
    "\n",
    "    ### Path to dask performance report.\n",
    "    PERFORMANCE_REPORT_NAME = f'dask_performance_report_{current_date}.html'\n",
    "    PERFORMANCE_DIR = LOGS_DIR / PERFORMANCE_REPORT_NAME\n",
    "    ###########################################################################################\n",
    "\n",
    "    ############################### EXECUTING THE PIPELINE ######################################\n",
    "    with performance_report(filename=PERFORMANCE_DIR):\n",
    "        if isinstance(CATALOG_FILES, list)==True:\n",
    "            CATALOG_PATHS = [CATALOG_DIR / file for file in CATALOG_FILES]\n",
    "        elif isinstance(CATALOG_FILES, str)==True:\n",
    "            CATALOG_PATHS = list(CATALOG_DIR.glob(CATALOG_FILES))\n",
    "        else:\n",
    "            raise Exception('The type of names of catalogs files (CATALOG_FILES) is not supported. Supported types are list and str.')\n",
    "    \n",
    "        if FILE_TYPE in ['.parquet', '.parq', '.pq']:\n",
    "            ##########################################################\n",
    "            catalog_args = ImportArguments(\n",
    "                sort_columns=CATALOG_SORT_COLUMN,\n",
    "                ra_column=CATALOG_RA_COLUMN,\n",
    "                dec_column=CATALOG_DEC_COLUMN,\n",
    "                input_file_list=CATALOG_PATHS,\n",
    "                file_reader=ParquetReader(column_names=CATALOG_SELECTED_COLUMNS),\n",
    "                output_artifact_name=CATALOG_HATS_NAME,\n",
    "                output_path=HATS_DIR,\n",
    "            )\n",
    "            ##########################################################\n",
    "            pipeline_with_client(catalog_args, client)\n",
    "            ##########################################################\n",
    "        elif FILE_TYPE in ['.fits', '.fit', '.fts']:\n",
    "            ##########################################################\n",
    "            catalog_args = ImportArguments(\n",
    "                sort_columns=CATALOG_SORT_COLUMN,\n",
    "                ra_column=CATALOG_RA_COLUMN,\n",
    "                dec_column=CATALOG_DEC_COLUMN,\n",
    "                input_file_list=CATALOG_PATHS,\n",
    "                file_reader=FitsReader(column_names=CATALOG_SELECTED_COLUMNS),\n",
    "                output_artifact_name=CATALOG_HATS_NAME,\n",
    "                output_path=HATS_DIR,\n",
    "            )\n",
    "            ##########################################################\n",
    "            pipeline_with_client(catalog_args, client)\n",
    "            ##########################################################\n",
    "        elif FILE_TYPE in ['.csv', '.csv.gz', '.csv.zip', '.csv.bz2', '.csv.xz', '.csv.zst']:\n",
    "            if (FILE_TYPE in ['.csv.gz']) and (FILE_SUB_TYPE in ['ECSV']):\n",
    "                ##########################################################\n",
    "                catalog_file = CATALOG_PATHS[0]\n",
    "                empty_astropy_table = ascii.read(catalog_file, format=\"ecsv\", data_end=1)\n",
    "                CATALOG_SCHEMA_FILE = LOGS_DIR / \"schema.parquet\"\n",
    "                empty_astropy_table.write(\n",
    "                    CATALOG_SCHEMA_FILE,\n",
    "                    # Uncomment to overwrite existing schema file\n",
    "                    overwrite=True,\n",
    "                )\n",
    "                \n",
    "                catalog_args = ImportArguments(\n",
    "                    sort_columns=CATALOG_SORT_COLUMN,\n",
    "                    ra_column=CATALOG_RA_COLUMN,\n",
    "                    dec_column=CATALOG_DEC_COLUMN,\n",
    "                    input_file_list=CATALOG_PATHS,\n",
    "                    file_reader=CsvReader(\n",
    "                        comment=\"#\",\n",
    "                        schema_file=CATALOG_SCHEMA_FILE,\n",
    "                        compression=\"gzip\",\n",
    "                        parquet_kwargs={\"dtype_backend\": \"pyarrow\", \"engine\": \"pyarrow\"},\n",
    "                    ),\n",
    "                    use_schema_file=CATALOG_SCHEMA_FILE,\n",
    "                    output_artifact_name=CATALOG_HATS_NAME,\n",
    "                    output_path=HATS_DIR,\n",
    "                )\n",
    "                \n",
    "                print('For ECSV files, the pipeline only works considering all columns in the table. \\n')\n",
    "                print('Ignoring the selected columns and running the pipeline with all columns. \\n')\n",
    "                \n",
    "                ##########################################################\n",
    "                pipeline_with_client(catalog_args, client)\n",
    "                ##########################################################\n",
    "            else:\n",
    "                ##########################################################\n",
    "                catalog_args = ImportArguments(\n",
    "                    sort_columns=CATALOG_SORT_COLUMN,\n",
    "                    ra_column=CATALOG_RA_COLUMN,\n",
    "                    dec_column=CATALOG_DEC_COLUMN,\n",
    "                    input_file_list=CATALOG_PATHS,\n",
    "                    file_reader=CsvReader(column_names=CATALOG_SELECTED_COLUMNS),\n",
    "                    output_artifact_name=CATALOG_HATS_NAME,\n",
    "                    output_path=HATS_DIR,\n",
    "                )\n",
    "                ##########################################################\n",
    "                pipeline_with_client(catalog_args, client)\n",
    "                ##########################################################\n",
    "        else:\n",
    "            raise Exception('Input catalog type not supported yet.')\n",
    "###########################################################################################\n",
    "else:\n",
    "    print('You selected not to run the pipeline.') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19547d2-0269-41ea-9a76-217087f8c736",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Analysing the outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dd2880-23bc-487d-9969-875db9961bd3",
   "metadata": {},
   "source": [
    "## Pixels plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2417ee-1056-4b57-a829-7e2dd9060d61",
   "metadata": {},
   "source": [
    "Plotting the pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141de3c5-4569-480b-ae3a-88b241babbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_hats_catalog = hats.read_hats(CATALOG_HATS_DIR)\n",
    "if save_the_info==True:\n",
    "    current_date = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "    plot_pixels(loaded_hats_catalog)\n",
    "    plt.savefig(f\"{logs_dir}/pixels_plot_{current_date}.png\")\n",
    "else:\n",
    "    plot_pixels(loaded_hats_catalog)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de9d7b8-3bd3-489c-85c7-8d569fbfc1f3",
   "metadata": {},
   "source": [
    "## Comparing sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b942c95-9c2a-41a3-be62-996164053b36",
   "metadata": {},
   "source": [
    "Computing the number of row in the original catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88b536e-bc36-4fb0-bd59-7e06da972b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_date = datetime.now().strftime('%Y-%m-%d_%H-%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0367f361-8934-48c5-875c-184f15ceb131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_file_rows(file_path):\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(file_path, comment='#', compression='gzip')\n",
    "    return len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e9df81-59c7-4f6e-b47e-347ba680979e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if compute_sizes_from_disk==True:\n",
    "    if load_the_input_catalog==True:\n",
    "        file_lengths = [delayed(compute_file_rows)(file) for file in total_files]\n",
    "        original_total_rows_computed = sum(dask.compute(*file_lengths))\n",
    "    \n",
    "        if show_info_inline==True:\n",
    "            print(f\"Input catalog path: {original_catalog_path}\\n\")\n",
    "            print(f\"Total number of rows: {original_total_rows_computed}\\n\")\n",
    "            print(f\"Total number of columns: {len(original_catalog_columns_list)}\\n\\n\")\n",
    "    \n",
    "        if save_the_info==True:\n",
    "            with open(f'{logs_dir}/total_len_of_files_{current_date}.txt', 'a') as f:            \n",
    "                f.write(f\"Input catalog path: {original_catalog_path}\\n\")\n",
    "                f.write(f\"Total number of rows: {original_total_rows_computed}\\n\")\n",
    "                f.write(f\"Total number of columns: {len(original_catalog_columns_list)}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777c14dd-bbd5-4a7a-a7a2-eba20d2f26e0",
   "metadata": {},
   "source": [
    "Computing the number of rows in the HATS catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9404410e-e679-460e-b4ad-7209cae3b6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if compute_sizes_from_disk == True:\n",
    "    loaded_hats_catalog_from_disk_lsdb = lsdb.read_hats(CATALOG_HATS_DIR)\n",
    "    loaded_hats_catalog_from_disk_hats = hats.read_hats(CATALOG_HATS_DIR)\n",
    "    \n",
    "    hats_catalog_total_columns = loaded_hats_catalog_from_disk_lsdb.columns.to_list()\n",
    "    hats_catalog_total_rows = loaded_hats_catalog_from_disk_hats.catalog_info.total_rows\n",
    "\n",
    "    if show_info_inline == True:\n",
    "        print(f\"HATS catalog path: {CATALOG_HATS_DIR} \\n\")\n",
    "        print(f\"Total number of rows: {hats_catalog_total_rows}\\n\")\n",
    "        print(f\"Total number of columns: {len(hats_catalog_total_columns)}\\n\\n\")\n",
    "\n",
    "    if save_the_info == True:\n",
    "        with open(f'{logs_dir}/total_len_of_files_{current_date}.txt', 'a') as f:\n",
    "            f.write(f\"HATS catalog path: {CATALOG_HATS_DIR}\\n\")\n",
    "            f.write(f\"Total number of rows: {hats_catalog_total_rows}\\n\")\n",
    "            f.write(f\"Total number of columns: {len(hats_catalog_total_columns)}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da31746-5d4c-4735-8aea-2183d5afd43a",
   "metadata": {},
   "source": [
    "Comparing the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfc6815-20dc-4018-9dca-31c8464c5785",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if compute_sizes_from_disk==True:\n",
    "    original_columns_set = set(original_catalog_columns_list)\n",
    "    loaded_columns_set = set(hats_catalog_total_columns)\n",
    "\n",
    "    missing_columns = list(original_columns_set - loaded_columns_set)\n",
    "    extra_columns = list(loaded_columns_set - original_columns_set)\n",
    "\n",
    "    if show_info_inline==True:\n",
    "        print(f\"missing_columns = {missing_columns} \\n\")\n",
    "        print(f\"extra_columns = {extra_columns}\\n\\n\")\n",
    "    if save_the_info==True:\n",
    "        with open(f'{logs_dir}/total_len_of_files_{current_date}.txt', 'a') as f:\n",
    "            f.write(f\"missing_columns = {missing_columns} \\n\")\n",
    "            f.write(f\"extra_columns = {extra_columns}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6a5e08-8759-497c-a9d3-38f74b1446f0",
   "metadata": {},
   "source": [
    "## Summarize pixels and sizes\n",
    "* \"healpix orders: distinct healpix orders represented in the partitions\n",
    "\n",
    "* num partitions: total number of partition files\n",
    "\n",
    "Size on disk data - using the file sizes fetched above, check the balance of your data. If your rows are fixed-width (e.g. no nested arrays, and few NaNs), the ratio here should be similar to the ratio above. If they’re very different, and you experience problems when parallelizing operations on your data, you may consider re-structuring the data representation.\n",
    "\n",
    "* min size_on_disk: smallest file (in GB)\n",
    "\n",
    "* max size_on_disk: largest file size (in GB)\n",
    "\n",
    "* size_on_disk ratio: max/min\n",
    "\n",
    "total size_on_disk: sum of all parquet catalog files (actual catalog size may vary due to other metadata files)\"\n",
    "\n",
    "Source: https://hats.readthedocs.io/en/stable/notebooks/catalog_size_inspection.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc00328f-70a7-46f7-9716-94a175ba8c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Change this path!!!\n",
    "catalog_dir = CATALOG_HATS_DIR\n",
    "\n",
    "### ----------------\n",
    "### You probably won't have to change anything from here.\n",
    "\n",
    "catalog = hats.read_hats(catalog_dir)\n",
    "\n",
    "output_info_frame = catalog.partition_info.as_dataframe()\n",
    "\n",
    "for index, partition in output_info_frame.iterrows():\n",
    "    file_name = result = hats.io.paths.pixel_catalog_file(\n",
    "        catalog_dir, HealpixPixel(partition[\"Norder\"], partition[\"Npix\"])\n",
    "    )\n",
    "    output_info_frame.loc[index, \"size_on_disk\"] = os.path.getsize(file_name)\n",
    "\n",
    "output_info_frame = output_info_frame.astype(int)\n",
    "output_info_frame[\"gbs\"] = output_info_frame[\"size_on_disk\"] / (1024 * 1024 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0b9686-476b-40fd-a69e-e800c2ac404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_the_info==True:\n",
    "    current_date = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "    with open(f\"{logs_dir}/output_summarize_pixels_{current_date}.txt\", \"w\") as file:\n",
    "        file.write(f'healpix orders: {output_info_frame[\"Norder\"].unique()}\\n')\n",
    "        file.write(f'num partitions: {len(output_info_frame[\"Npix\"])}\\n')\n",
    "        file.write(\"------\\n\")\n",
    "        file.write(f'min size_on_disk: {output_info_frame[\"gbs\"].min():.8f}\\n')\n",
    "        file.write(f'max size_on_disk: {output_info_frame[\"gbs\"].max():.8f}\\n')\n",
    "        file.write(f'size_on_disk ratio: {output_info_frame[\"gbs\"].max()/output_info_frame[\"gbs\"].min():.8f}\\n')\n",
    "        file.write(f'total size_on_disk: {output_info_frame[\"gbs\"].sum():.8f}\\n')\n",
    "if show_info_inline==True:\n",
    "    print(f'healpix orders: {output_info_frame[\"Norder\"].unique()}')\n",
    "    print(f'num partitions: {len(output_info_frame[\"Npix\"])}')\n",
    "    print(\"------\")\n",
    "    print(f'min size_on_disk: {output_info_frame[\"gbs\"].min():.7f}')\n",
    "    print(f'max size_on_disk: {output_info_frame[\"gbs\"].max():.7f}')\n",
    "    print(f'size_on_disk ratio: {output_info_frame[\"gbs\"].max()/output_info_frame[\"gbs\"].min():.7f}')\n",
    "    print(f'total size_on_disk: {output_info_frame[\"gbs\"].sum():.7f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c98faec-5a36-4139-a44a-1dce3c736d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_info_inline==True:\n",
    "    print(output_info_frame[output_info_frame['gbs'] == output_info_frame['gbs'].min()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2a89a3-df4e-48dd-a6ea-8a24ec8f7c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_info_inline==True:\n",
    "    print(output_info_frame[output_info_frame['gbs'] == output_info_frame['gbs'].max()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb60ec4-ab87-469c-9e4f-1798826d8bc9",
   "metadata": {},
   "source": [
    "## File size distribution\n",
    "\"Below we look at histograms of file sizes.\n",
    "\n",
    "In our initial testing, we find that there’s a “sweet spot” file size of 100MB-1GB. Files that are smaller create more overhead for individual reads. Files that are much larger may create slow-downs when cross-matching between catalogs. Files that are much larger can create out-of-memory issues for dask when loading from disk.\n",
    "\n",
    "The majority of your files should be in the “sweet spot”, and no files in the “too-big” category.\"\n",
    "\n",
    "Source: https://hats.readthedocs.io/en/stable/notebooks/catalog_size_inspection.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1a1fd9-145e-4de8-8ffa-4976eb99b5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file_size_info(info_frame, type_of_files, bins, labels, logs_dir, save=False, show=False):\n",
    "    \n",
    "    plt.hist(info_frame[\"gbs\"], edgecolor='black')\n",
    "    plt.xlabel(\"File size (GB)\")\n",
    "    plt.ylabel(\"Number of files\")\n",
    "\n",
    "    if save:\n",
    "        current_date = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "        plt.savefig(f\"{logs_dir}/{type_of_files}_file_size_histogram_{current_date}.png\")\n",
    "\n",
    "        hist = np.histogram(info_frame[\"gbs\"], bins=bins)[0]\n",
    "        pcts = hist / len(info_frame)\n",
    "\n",
    "        with open(f\"{logs_dir}/{type_of_files}_file_size_distribution_{current_date}.txt\", \"w\") as file:\n",
    "            for i, label in enumerate(labels):\n",
    "                file.write(f\"{label} \\t: {hist[i]} \\t({pcts[i]*100:.1f} %)\\n\")\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "        hist = np.histogram(info_frame[\"gbs\"], bins=bins)[0]\n",
    "        pcts = hist / len(info_frame)\n",
    "\n",
    "        for i, label in enumerate(labels):\n",
    "            print(f\"{label} \\t: {hist[i]} \\t({pcts[i]*100:.1f} %)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fb7559-0837-4113-9d4b-04c849563e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_of_files = 'output'\n",
    "bins = [0, 0.5, 1, 2, 100]\n",
    "labels = [\"small-ish\", \"sweet-spot\", \"big-ish\", \"too-big\"]\n",
    "\n",
    "if save_the_info:\n",
    "    logs_dir = logs_dir\n",
    "else:\n",
    "    logs_dir=None\n",
    "\n",
    "process_file_size_info(\n",
    "    info_frame=output_info_frame,\n",
    "    type_of_files=type_of_files,\n",
    "    bins=bins,\n",
    "    labels=labels,\n",
    "    logs_dir=logs_dir,\n",
    "    save=save_the_info,\n",
    "    show=show_info_inline\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c92b76-c68d-45b0-b010-1028965434bf",
   "metadata": {},
   "source": [
    "# Closing the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a2869c-f4f4-4f1e-a3dc-cd58a1bc2ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if close_the_cluster==True:\n",
    "    client.close()\n",
    "    cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hats_env_081124",
   "language": "python",
   "name": "hats_env_081124"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
