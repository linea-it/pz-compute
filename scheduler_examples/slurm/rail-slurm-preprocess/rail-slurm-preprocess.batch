#!/usr/bin/env python3
#SBATCH --spread-job
#SBATCH --time=2:00:00
#SBATCH -p sequana_cpu
#SBATCH -J rail-slurm-preprocess
#SBATCH --mem-per-cpu=2G
#SBATCH -N 4-50
#SBATCH --ntasks-per-node=24

# Usage:
#
#     $ sbatch [<sbatch_options>...] rail-slurm-preprocess.batch \
#             <input_dir> <output_dir> <files>...
#
# - input_dir: directory with the input files
# - output_dir: directory for the generated files
# - files: patterns with the input file list
#
# Note: the input directory appears twice, on parameters input_dir and
# files.
#
# Example usage:
#
#     # Schedule in 10 to 20 machines, process specified files in the
#     # input subdirectories.
#     $ sbatch -N 10-20 rail-slurm-preprocess.batch input_dir \
#             output_dir input_dir/*/*.parq
#
#     # Process the whole input directory hierarchy.
#     $ sbatch rail-slurm-preprocess.batch input1 output1 "input1/**"
#
# File generation in the output directory will mirror the directory
# structure from the input directory.
#
# This script is somewhat "Unix intensive" because it tries to exploit support
# for dynamic scheduling on slurm. It calls "srun" in parallel and keeps calling
# it while previous "srun" executions finish, keeping the cluster's task slots
# fully allocated until all files are processed.

from contextlib import contextmanager
from datetime import datetime
from glob import glob
from os import P_NOWAIT, WNOHANG, environ, kill, makedirs, spawnv, waitpid
from os.path import dirname, getsize, isfile, join, relpath
from shutil import which
from signal import SIGHUP
from sys import argv, stderr
from time import sleep

SRUN = 'srun'
SRUN_ARGS = [SRUN, '-n1', '-N1', '--exact']
LOG = 'log'
PROG = 'rail-slurm-preprocess'
PREPROCESS = 'rail-preprocess-parquet'
PREPROCESS_ARGS = ['--rows=150000']

# Spawns a single task to a single cluster slot
def srun(cmd, stdout, stderr):
    args = SRUN_ARGS + ['--output', stdout, '--error', stderr] + list(cmd)

    return spawnv(P_NOWAIT, SRUN_PATH, args)

def stdout_name(task_id):
    return join(LOG, '%s-%d.out' % (PROG, task_id))

def stderr_name(task_id):
    return join(LOG, '%s-%d.err' % (PROG, task_id))

# Best effort cleanup on abnormal program termination
def try_kill_children(children):
    for pid in children:
        kill(pid, SIGHUP)

    sleep(1)

    for _ in children:
        waitpid(-1, WNOHANG)

def info(msg):
    print(msg, file=stderr, flush=True)

@contextmanager
def now(msg):
    info('%s: Starting: %s' % (datetime.now(), msg))
    yield
    info('%s: Finished: %s' % (datetime.now(), msg))

def parallel(files, slots, input_dir, output_base_dir):
    children = {}
    task_id = 0

    while len(children) > 0 or len(files) > 0:
        # Keep all cluster slots full by spawning each task
        while len(children) < slots and len(files) > 0:
            input_file = files.pop()
            relative_path = relpath(input_file, input_dir)
            output_file = join(output_base_dir, relative_path)
            output_dir = dirname(output_file)
            stdout_file = stdout_name(task_id)
            stderr_file = stderr_name(task_id)
            args = [PREPROCESS_PATH] + PREPROCESS_ARGS + \
                    [input_file, output_dir]

            makedirs(output_dir, exist_ok=True)

            info('%s: Starting: %s %s id=%d' % (datetime.now(), PREPROCESS,
                relative_path, task_id))

            pid = srun(args, stdout_file, stderr_file)
            children[pid] = (relative_path, task_id)
            task_id += 1

        # Wait for tasks to finish
        if len(children) > 0:
            pid, ret = waitpid(-1, 0)
            prev_relative_path, prev_task_id = children.pop(pid)

            if ret != 0:
                try_kill_children(children)
                raise RuntimeError('Error: child process returned failure (%d).'
                        % ret)

            info('%s: Finished: %s %s id=%d' % (datetime.now(), PREPROCESS,
                prev_relative_path, prev_task_id))

def get_input_files(patterns):
    files = []

    for pattern in patterns:
        files += [f for f in glob(pattern, recursive=True) if isfile(f)]

   return files

def schedule_by_size(files):
    sizes = [(getsize(file), file) for file in files]
    sizes.sort()

    return [file for (size, file) in sizes]

def parse_cmdline():
    input_dir = argv[1]
    output_base_dir = argv[2]
    patterns = argv[3:]

    return input_dir, output_base_dir, patterns

def find_program_paths():
    with now('finding program paths'):
        srun = which(SRUN)
        if not srun:
            raise RuntimeError('Unable to find %s program.' % SRUN)
        info(srun)
        preprocess = which(PREPROCESS)
        if not preprocess:
            raise RuntimeError('Unable to find %s program.' % PREPROCESS)
        info(preprocess)

        global SRUN_PATH, PREPROCESS_PATH

        SRUN_PATH = srun
        PREPROCESS_PATH = preprocess

def main():
    with now(PROG):
        slots = int(environ['SLURM_NTASKS'])
        input_dir, output_base_dir, patterns = parse_cmdline()
        files = get_input_files(patterns)
        files = schedule_by_size(files)
        find_program_paths()
        parallel(files, slots, input_dir, output_base_dir)

if __name__ == '__main__': main()
