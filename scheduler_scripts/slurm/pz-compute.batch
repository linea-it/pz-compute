#!/usr/bin/env python3
#SBATCH --time=16:00:00
#SBATCH -J rail-slurm
#SBATCH --mem-per-cpu=2140M
#SBATCH -p cpu
#SBATCH --propagate=NPROC

# This script calls rail-estimate on the cluster.
#
# Usage:
#
#     $ sbatch [<sbatch_options>...] rail-slurm.batch \
#             [<input_dir> [<output_dir> [<args>...]]]
#
# - input_dir: Input directory with pre-processed input data
# - output_dir: Target output directory for the estimations
# - args: Extra parameters passed to rail-estimate
#
# The rail-estimate program also requires by default an
# estimator_<algorithm>.pkl file available in the current directory or in a
# standard data directory.  In case of BPZ algorithm, the BPZ cache files (AB
# directory) must have been previously generated.
#
# Example usage:
#
#     # Defaults: input_dir=input, output_dir=output
#     $ sbatch rail_slurm.batch
#
#     # Change the slurm queue, change estimation algorithm to BPZ
#     $ sbatch -p cpu_long -t 8:00:00 -N20 --ntasks-per-node=40 \
#             input1 output1 -a bpz
#
# This script is somewhat "Unix intensive" because it tries to exploit support
# for dynamic scheduling on slurm. It calls "srun" in parallel and keeps calling
# it while previous "srun" executions finish, keeping the cluster's task slots
# fully allocated until all files are processed.

import shlex
from collections import deque, namedtuple
from contextlib import contextmanager
from datetime import datetime
from glob import glob
from os import P_NOWAIT, WNOHANG, chdir, environ, getcwd, kill, makedirs, \
        spawnv, unlink, waitpid, sched_getaffinity
from os.path import dirname, getsize, isfile, join, relpath
from shutil import which
from signal import SIGHUP
from subprocess import run
from sys import argv, stderr
from time import sleep

from ceci.stage import IN_PROGRESS_PREFIX
from yaml import YAMLError, safe_dump, safe_load

SRUN = 'srun'
SRUN_ARGS = [SRUN, '-n1', '-N1', '--cpu-bind=none']
LOG = 'log'
PROG = 'pz-compute'
ESTIMATE = 'rail-estimate'
OUTPUT_DIR = 'output'
INPUT_DIR = 'input'
HELPER = 'pz-compute.run'
MAX_ATTEMPTS = 3

Mask = namedtuple('Mask', ('host', 'slot'))

# Spawns a single task to a single cluster slot
def srun(cmd, stdout, stderr, mask):
    args = SRUN_ARGS + ['--output', stdout, '--error', stderr, '-w', mask.host,
            HELPER, str(mask.slot)] + list(cmd)

    return spawnv(P_NOWAIT, SRUN_PATH, args)

def stdout_name(task_id):
    subdir = '%05d' % (task_id // 10000 * 10000)
    return join(LOG, subdir, '%s-%d.out' % (PROG, task_id))

def stderr_name(task_id):
    subdir = '%05d' % (task_id // 10000 * 10000)
    return join(LOG, subdir, '%s-%d.err' % (PROG, task_id))

def cmdline_name():
    return join(LOG, '%s.cmdline.yaml' % PROG)

# Best effort cleanup on abnormal program termination
def try_kill_children(children):
    for pid in children:
        kill(pid, SIGHUP)

    sleep(1)

    for _ in children:
        waitpid(-1, WNOHANG)

def info(msg):
    print(msg, file=stderr, flush=True)

@contextmanager
def now(msg):
    info('%s: Starting: %s' % (datetime.now(), msg))
    yield
    info('%s: Finished: %s' % (datetime.now(), msg))

def parallel(files, slots, input_dir, output_base_dir, extra_args, masks):
    children = {}
    task_id = 0
    attempt = { file: 0 for file in files }
    files = deque(files)
    masks = deque(masks)

    while len(children) > 0 or len(files) > 0:
        # Keep all cluster slots full by spawning each task
        while len(children) < slots and len(files) > 0:
            relative_path = files.pop()
            input_file = join(input_dir, relative_path)
            output_file = join(output_base_dir, relative_path)
            output_dir = dirname(output_file)
            stdout_file = stdout_name(task_id)
            stderr_file = stderr_name(task_id)
            args = [ESTIMATE, input_file, output_file] + extra_args

            makedirs(dirname(stdout_file), exist_ok=True)
            makedirs(dirname(stderr_file), exist_ok=True)
            makedirs(output_dir, exist_ok=True)

            info('%s: Starting: %s %s id=%d' % (datetime.now(), ESTIMATE,
                relative_path, task_id))

            mask = masks.pop()

            pid = srun(args, stdout_file, stderr_file, mask)
            children[pid] = (relative_path, task_id, mask)
            task_id += 1

        # Wait for tasks to finish
        if len(children) > 0:
            pid, ret = waitpid(-1, 0)
            prev_relative_path, prev_task_id, mask = children.pop(pid)
            masks.appendleft(mask)

            attempt[prev_relative_path] += 1

            if ret != 0:
                info('Error: child process returned failure: ret=%d, '
                     'signal=%d, id=%d, host=%s, slot=%d.' % (ret//256, ret%256,
                     prev_task_id, mask.host, mask.slot))
                if attempt[prev_relative_path] > MAX_ATTEMPTS:
                    info('Error: too many failed attempts.')
                    try_kill_children(children)
                    raise RuntimeError('Error: too many failed attempts.')
                else:
                    info('Will retry again later.')
                    files.appendleft(prev_relative_path)
                    sleep(0.2)
            else:
                info('%s: Finished: %s %s id=%d' % (datetime.now(), ESTIMATE,
                    prev_relative_path, prev_task_id))

                # Best effort delay to avoid srun's "node is busy" messages,
                # give a little breathing room to the nodes.
                sleep(0.005)

def get_input_files(input_dir):
    with now('searching input files'):
        cwd = getcwd()
        chdir(input_dir)
        try:
            return [path for path in glob('**/*', recursive=True) if
                    isfile(path)]
        finally:
            chdir(cwd)

def parse_cmdline():
    if len(argv) > 1:
        input_dir = argv[1]
    else:
        input_dir = INPUT_DIR

    if len(argv) > 2:
        output_dir = argv[2]
    else:
        output_dir = OUTPUT_DIR

    extra_args = argv[3:]
    return input_dir, output_dir, extra_args

def find_program_paths():
    with now('finding program paths'):
        srun = which(SRUN)
        if not srun:
            raise RuntimeError('Unable to find %s program.' % SRUN)
        info(srun)
        estimate = which(ESTIMATE)
        if not estimate:
            raise RuntimeError('Unable to find %s program.' % ESTIMATE)
        info(estimate)
        helper = which(HELPER)
        if not helper:
            helper = which(join('.', HELPER))
        if not helper:
            raise RuntimeError('Unable to find %s program.' % HELPER)
        info(helper)

        global SRUN_PATH

        SRUN_PATH = srun

def parse_slurm_tasks_per_node(tasks_per_node):
    task_list = []
    for t in tasks_per_node.split(','):
        l = t.split('(x')
        v = int(l[0])
        if len(l) > 1:
            m = int(l[1].strip(')'))
        else:
            m = 1
        for i in range(m):
            task_list.append(v)

    return task_list

def parse_slurm_variables():
    slots = int(environ['SLURM_NTASKS'])
    tasks_per_node = environ['SLURM_TASKS_PER_NODE']
    node_list = run('scontrol show hostnames', check=True, capture_output=True,
                    shell=True).stdout.decode('utf-8').split()
    task_list = parse_slurm_tasks_per_node(tasks_per_node)

    return slots, node_list, task_list

def build_cpu_mask_set(node_list, task_list):
    masks = set()

    for n, t in zip(node_list, task_list):
        for i in range(t):
            masks.add(Mask(host=n, slot=i))

    return masks

def configure_threads(slots, node_list, task_list):
    # Work around libraries starting a large amount of auxiliary threads.
    environ['OMP_NUM_THREADS'] = '2'
    environ['ARROW_IO_THREADS'] = '2'
    environ['ARROW_DEFAULT_MEMORY_POOL'] = 'system'
    masks = build_cpu_mask_set(node_list, task_list)
    assert(len(masks) == slots)
    return masks

def skip_existing_output(files, output_dir):
    new_files = []
    already = 0

    with now('inspecting the output directory'):
        for file in files:
            if isfile(join(output_dir, file)):
                already += 1
            else:
                new_files.append(file)

            in_progress_basename = IN_PROGRESS_PREFIX + file
            in_progress_path = join(output_dir, in_progress_basename)

            if isfile(in_progress_path):
                info('Deleting incomplete file: %s' % in_progress_basename)

                try:
                    unlink(in_progress_path)
                except OSError as e:
                    info('Warning: unable to delete file "%s": %s' %
                            (in_progress_path, e))

        if already > 0:
            info('Skipped %d files already processed.' % already)

        return new_files

def log_cmdline(args):
    fname = cmdline_name()
    dname = dirname(fname)

    try:
        makedirs(dname, exist_ok=True)
    except OSError as e:
        info('Warning: unable to create directory for loggin the command line: %s' % e)

    try:
        with open(fname, 'r') as f:
                old = safe_load(f)
    except (OSError, YAMLError) as e:
        old = None

    info('Current command line: %s' % shlex.join(args))

    if old is not None:
        info('Old command line from log file: %s' % shlex.join(old))

    if args != old:
        info('Assuming new execution from start.')

        try:
            with open(fname, 'w') as f:
                safe_dump(args, f)
        except (OSError, YAMLError) as e:
            info('Warning: unable to log the command line parameters to file: '
                    '%s' % e)

    return args == old

def main():
    with now(PROG):
        input_dir, output_dir, extra_args = parse_cmdline()
        slots, node_list, task_list = parse_slurm_variables()
        files = get_input_files(input_dir)
        maybe_reexecution = log_cmdline(argv[1:])

        if maybe_reexecution:
            files = skip_existing_output(files, output_dir)

        find_program_paths()
        masks = configure_threads(slots, node_list, task_list)
        parallel(files, slots, input_dir, output_dir, extra_args, masks)

if __name__ == '__main__': main()
