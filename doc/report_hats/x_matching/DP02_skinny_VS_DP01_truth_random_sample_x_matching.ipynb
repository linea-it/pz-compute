{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cea31ded-6bc2-4a0b-8dad-3a93eeab11e9",
   "metadata": {},
   "source": [
    "<img align='left' src = '../../images/linea.png' width=150 style='padding: 20px'> \n",
    "\n",
    "# Report HATS\n",
    "## Doing the crossmatching for DP0.2 (Object) Skinny VS DP0.1 Truth Random Sample catalogs\n",
    "\n",
    "Performance report on LINCC libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1c3331-81a3-4a97-9640-d0b8f3a77f1f",
   "metadata": {},
   "source": [
    "Contacts: Luigi Silva ([luigi.silva@linea.org.br](mailto:luigi.silva@linea.org.br)); Julia Gschwend ([julia@linea.org.br](mailto:julia@linea.org.br)).\n",
    "\n",
    "Last check: 08/11/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8168f1f-b9a4-41de-ba6b-c52b27044dea",
   "metadata": {},
   "source": [
    "#### Acknowledgments\n",
    "\n",
    "'_This notebook used computational resources from the Associação Laboratório Interinstitucional de e-Astronomia (LIneA) with financial support from the INCT of e-Universe (Process No. 465376/2014-2)._'\n",
    "\n",
    "'_This notebook uses libraries from the LSST Interdisciplinary Network for Collaboration and Computing (LINCC) Frameworks project, such as the hats, hats_import, and lsdb libraries. The LINCC Frameworks project is supported by Schmidt Sciences. It is also based on work supported by the National Science Foundation under Grant No. AST-2003196. Additionally, it receives support from the DIRAC Institute at the Department of Astronomy of the University of Washington. The DIRAC Institute is supported by gifts from the Charles and Lisa Simonyi Fund for Arts and Sciences and the Washington Research Foundation._'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a5886a-379d-4abf-8aa1-769558f07fe3",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dd09ef-d61b-4c88-8c76-144578ad3f05",
   "metadata": {},
   "source": [
    "Let us import the packages that we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87f1f35-9c72-4489-9f8b-e71209067a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### GENERAL ##########################\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "import time\n",
    "import math\n",
    "import getpass\n",
    "import warnings\n",
    "import tables_io\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import healpy as hp\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "############################ DASK ############################\n",
    "import dask\n",
    "from dask import dataframe as dd\n",
    "from dask import delayed\n",
    "from dask.distributed import Client, performance_report, wait\n",
    "import dask_jobqueue\n",
    "from dask_jobqueue import SLURMCluster\n",
    "########################## HATS ###########################\n",
    "import hats\n",
    "from hats.inspection.visualize_catalog import plot_pixels\n",
    "from hats.pixel_math import HealpixPixel\n",
    "########################## HATS IMPORT ###########################\n",
    "import hats_import\n",
    "from hats_import.catalog.file_readers import ParquetReader, FitsReader\n",
    "from hats_import.margin_cache.margin_cache_arguments import MarginCacheArguments\n",
    "from hats_import.pipeline import ImportArguments, pipeline_with_client\n",
    "############################ LSDB ############################\n",
    "import lsdb\n",
    "from lsdb.core.search import BoxSearch\n",
    "######################## VISUALIZATION #######################\n",
    "### BOKEH\n",
    "import bokeh\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.models import ColorBar, LinearColorMapper, PrintfTickFormatter\n",
    "from bokeh.palettes import Viridis256\n",
    "\n",
    "### HOLOVIEWS\n",
    "import holoviews as hv\n",
    "from holoviews import opts\n",
    "from holoviews.operation.datashader import datashade, rasterize, dynspread\n",
    "\n",
    "### GEOVIEWS\n",
    "import geoviews as gv\n",
    "import geoviews.feature as gf\n",
    "from cartopy import crs\n",
    "\n",
    "### DATASHADER\n",
    "import datashader as ds\n",
    "from datashader.colors import viridis\n",
    "\n",
    "### MATPLOTLIB\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Polygon\n",
    "########################## ASTRONOMY #########################\n",
    "from astropy.io import fits\n",
    "from astropy import units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.units.quantity import Quantity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d29eb2-5029-41fe-be9f-0b7913388cd2",
   "metadata": {},
   "source": [
    "Defining the plots to be inline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bca724-d9ac-4422-b5c3-b08737409153",
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.extension('bokeh')\n",
    "gv.extension('bokeh')\n",
    "output_notebook()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461fc517-36d0-4a3d-964d-02c211d7b5f9",
   "metadata": {},
   "source": [
    "Printing the versions of the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5410f242-4f62-49fb-916e-5738415260be",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Getting hats version manually, because it has no __version__ attribute.\n",
    "result = subprocess.run(\n",
    "    [\"conda\", \"run\", \"-p\", \"/lustre/t0/scratch/users/luigi.silva/hats_env_081124\", \"conda\", \"list\", \"hats\"],\n",
    "    stdout=subprocess.PIPE, text=True\n",
    ")\n",
    "for line in result.stdout.splitlines():\n",
    "    if line.startswith(\"hats \"):\n",
    "        hats_version = line.split()[1]\n",
    "        break\n",
    "\n",
    "### Printing the versions.\n",
    "print(f'python version: {sys.version}')\n",
    "print(f'numpy version: {np.__version__}')\n",
    "print(f'dask version: {dask.__version__}')\n",
    "print(f'dask_jobqueue version: {dask_jobqueue.__version__}')\n",
    "print(f'hats version: {hats_version}')\n",
    "print(f'hats_import version: {hats_import.__version__}')\n",
    "print(f'lsdb version: {lsdb.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f50c08-a2de-46f8-9e47-36092ec6826a",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758e7cbb-6c18-4dc8-8a5d-93aec4dc28ee",
   "metadata": {},
   "source": [
    "## Running configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f4dc37-137e-4c96-8b2a-ff2e9b54f5e3",
   "metadata": {},
   "source": [
    "Set the configurations for this running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f7b543-8e53-404d-bd04-d6015b8b0dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO YOU WANT TO RUN THE X MATCHING PIPELINE? \n",
    "run_the_pipeline = False\n",
    "\n",
    "# DO YOU WANT TO SAVE ALL THE DASK JOBS OUTPUTS AND ERRORS OF DASK SLURMCluster?\n",
    "save_the_dask_jobs_info = True\n",
    "\n",
    "# DO YOU WANT TO SAVE ALL THE GENERAL INFORMATIONS OF THIS RUNNING (MAIN LIB VERSIONS, INPUT FILES SIZES, JOBS SCONTROL INFO, OUTPUT FILES SIZES)?\n",
    "save_the_info = True\n",
    "\n",
    "# DO YOU WANT TO SHOW THE INFO INLINE?\n",
    "show_info_inline = True\n",
    "\n",
    "# DO YOU WANT TO CLOSE THE CLIENT AND THE CLUSTER AT THE END?\n",
    "close_the_cluster = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f946e82c-53e5-4182-83e7-b2cdb95f1b6d",
   "metadata": {},
   "source": [
    "If you choose not to run the pipeline, give the path to an existing x-matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc369e5-e091-4c25-8d7c-d751cbdd037b",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_HATS_DIR='/lustre/t1/cl/lsst/pz_project/test_data/dp01_truth_random_sample_hats_x_skinny_hats'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab03e40-4ac8-4b33-b623-8b5187534b55",
   "metadata": {},
   "source": [
    "## Catalogs paths configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b131598-602e-4035-a890-f7705d8550c7",
   "metadata": {},
   "source": [
    "First of all, what catalog do you want to use as 'left'? Set ```True``` for using the first catalog as 'left', or ```False``` for using the second catalog as 'left'. The left catalog doesn't need a margin cache, however you need to define the margin_cache variable below anyway, as ```None``` for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a337f9-7d1f-483b-8adc-5e5ce31449c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_first_catalog_as_left = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01e45c6-fbb3-40b0-80c1-71e33edd4ff8",
   "metadata": {},
   "source": [
    "Defining the path to the first HATS catalog and its margin cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00153c4a-3868-476b-8869-c2a2ccf6dc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "hats_first_catalog_path = '/lustre/t1/cl/lsst/dp02/secondary/catalogs'\n",
    "hats_first_catalog_name = 'skinny_hats'\n",
    "hats_first_margin_cache = '/lustre/t1/cl/lsst/pz_project/test_data/dp02_skinny_margin_cache'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d8cf12-56c8-40cb-8202-5ead302c2e21",
   "metadata": {},
   "source": [
    "Defining the path to the second HATS catalog and its margin cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c77c093-e270-4d50-a211-5be00eec8c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "hats_second_catalog_path = '/lustre/t1/cl/lsst/pz_project/test_data'\n",
    "hats_second_catalog_name = 'dp01_truth_random_sample_hats'\n",
    "hats_second_margin_cache = '/lustre/t1/cl/lsst/pz_project/test_data/dp01_truth_random_sample_margin_cache'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a650b6e-a81a-45c7-934d-2b9073eeff60",
   "metadata": {},
   "source": [
    "Defining the names of key columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1caf90b-d419-401a-90f4-6f06ef5c2087",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_first_name = 'objectId'\n",
    "ra_first_name = 'coord_ra'\n",
    "dec_first_name = 'coord_dec'\n",
    "\n",
    "id_second_name = 'id'\n",
    "ra_second_name = 'ra'\n",
    "dec_second_name = 'dec'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2ce3ad-2a69-49b7-8396-7012d53de748",
   "metadata": {},
   "source": [
    "Defining the OUTPUT catalog path and name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e695e1d-589a-4c0b-9cff-787390b800cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_the_pipeline==True:\n",
    "    x_matching_path = '/lustre/t1/cl/lsst/pz_project/test_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385ebb81-5efe-493b-849d-dd07855836dc",
   "metadata": {},
   "source": [
    "Defining the USER base path, for saving logs, graphs and other informations about the running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c634f332-4bae-4ecd-b6d6-cbfcb30eee77",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_the_dask_jobs_info or save_the_info:\n",
    "    user = getpass.getuser()\n",
    "    user_base_path = f'/lustre/t0/scratch/users/{user}/report_hats/DP02-skinny-VS-DP01-truth-random-sample'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247be1ee-52cc-434a-9552-f3b096da2dca",
   "metadata": {},
   "source": [
    "### Creating directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21e4616-6171-43e7-a4eb-0d286c26ebe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_the_dask_jobs_info or save_the_info:\n",
    "    os.makedirs(user_base_path, exist_ok=True)\n",
    "\n",
    "    current_date = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "\n",
    "    run_path = os.path.join(user_base_path, f'run_hats_{current_date}')\n",
    "    os.makedirs(run_path, exist_ok=True)\n",
    "\n",
    "    logs_dir = os.path.join(run_path, f'logs')\n",
    "    os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "    dask_logs_dir = os.path.join(logs_dir, f'dask_logs')\n",
    "    os.makedirs(dask_logs_dir , exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce962447-e1f8-403a-a2e2-6270763bdd26",
   "metadata": {},
   "source": [
    "## Cluster configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c22c80b-972c-4c7f-842c-304bc0047f7d",
   "metadata": {},
   "source": [
    "Do you want to customize extra dask parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb07646-367c-4ae7-905f-e334ab1bfe16",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_dask_configs=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a2bc13-c23d-4d40-97ec-b4956d02f2ac",
   "metadata": {},
   "source": [
    "If you choose ```True```, see the explanation of the parameters and customize them below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bbae9f-7c5a-4d2f-8212-555c321e33d8",
   "metadata": {},
   "source": [
    "**Explanation of Parameters**\n",
    "\n",
    "* ```distributed.worker.memory.target```: sets the memory limit before Dask attempts to release memory from completed tasks. At the specified percentage, Dask will start memory collection earlier, reducing the risk of excessive accumulation.\n",
    "\n",
    "* ```distributed.worker.memory.spill```: defines the point at which Dask starts spilling data to disk (swap) instead of keeping it in RAM. This helps free up memory for new tasks.\n",
    "\n",
    "* ```distributed.worker.memory.pause```: when memory usage reaches the specified percentage, Dask will temporarily pause the worker to prevent excessive resource use.\n",
    "\n",
    "* ```distributed.worker.memory.terminate```: if memory usage reaches the specified percentage, the worker will be restarted, which prevents crashes and helps keep usage under control.\n",
    "\n",
    "* ```distributed.worker.memory.recent-to-old```: determines the fraction of recently accessed data Dask considers as “old” and, therefore, eligible for spilling to disk. A lower percentage (e.g., 0.2 for 20%) means only the most recent data is retained in RAM, while older data is more likely to be released, helping to manage cache memory efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3887274e-3f89-4027-a79f-1a716ca27fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if extra_dask_configs==True:\n",
    "    # Additional Dask configurations\n",
    "    dask_config = {\n",
    "        \"distributed.worker.memory.target\": 0.75,         # 75% before starting memory collection\n",
    "        \"distributed.worker.memory.spill\": 0.85,          # 85% before starting to use disk\n",
    "        \"distributed.worker.memory.pause\": 0.92,          # Pause the worker at 92%\n",
    "        \"distributed.worker.memory.terminate\": 0.98,      # Restart the worker at 98%\n",
    "        \"distributed.worker.memory.recent-to-old\": 0.2    # Keep 20% of recent data in memory\n",
    "    }\n",
    "\n",
    "    # Applying the Dask configurations\n",
    "    dask.config.set(dask_config)\n",
    "else:\n",
    "    print(\"Running DASK with the standard memory configuration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f35abe-7545-4c8e-aed2-d82e433b39f5",
   "metadata": {},
   "source": [
    "Defining the configurations for the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a4e509-ad16-45ff-b5ba-a45dde5e40f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "interface=\"ib0\"\n",
    "queue='cpu_small'\n",
    "cores=48         \n",
    "processes=2       \n",
    "memory='114GB'   \n",
    "walltime='04:00:00'\n",
    "\n",
    "if save_the_dask_jobs_info:\n",
    "    job_extra_directives=[\n",
    "        '--propagate',\n",
    "        f'--output={dask_logs_dir}/dask_job_%j_{current_date}.out',  \n",
    "        f'--error={dask_logs_dir}/dask_job_%j_{current_date}.err'\n",
    "    ]\n",
    "else:\n",
    "    job_extra_directives=[\n",
    "        '--propagate',\n",
    "        f'--output=/dev/null',  \n",
    "        f'--error=/dev/null'\n",
    "    ]\n",
    "\n",
    "number_of_nodes=20 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3358b7-f860-4e89-b083-46f12179d27c",
   "metadata": {},
   "source": [
    "Starting the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5d69ae-f2b1-4368-8d9f-7652c93e8f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_date = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "\n",
    "# Configuring the SLURMCluster.\n",
    "cluster = SLURMCluster(\n",
    "    interface=interface,         # Lustre interface\n",
    "    queue=queue,                 # Name of the queue\n",
    "    cores=cores,                 # Number of logical cores per node\n",
    "    processes=processes,         # Number of dask processes per node\n",
    "    memory=memory,               # Memory per node\n",
    "    walltime=walltime,           # Maximum execution time\n",
    "    job_extra_directives=job_extra_directives,\n",
    ")\n",
    "\n",
    "# Scaling the cluster to use X nodes\n",
    "cluster.scale(jobs=number_of_nodes)\n",
    "\n",
    "# Defining the dask client\n",
    "client = Client(cluster)\n",
    "\n",
    "# Wait for the workers to initialize\n",
    "cluster.wait_for_workers(n_workers=number_of_nodes*processes)\n",
    "client.run(lambda: gc.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ff08ee-ddbe-4cab-a1d4-dcc8a153a47f",
   "metadata": {},
   "source": [
    "Showing informations about the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1e7b37-22b5-4e30-9446-d761d7110a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_info = client.cluster\n",
    "cluster_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e84da8-62e5-4da0-aaed-8dba94498da4",
   "metadata": {},
   "source": [
    "Saving the requested resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38764081-349e-4930-8f29-dc504a7f977c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_the_info == True:  \n",
    "\n",
    "    # Specific settings that you want to separate for the memory section\n",
    "    memory_params = {\n",
    "        \"distributed.worker.memory.target\": None,\n",
    "        \"distributed.worker.memory.spill\": None,\n",
    "        \"distributed.worker.memory.pause\": None,\n",
    "        \"distributed.worker.memory.terminate\": None,\n",
    "        \"distributed.worker.memory.recent-to-old\": \"None\",\n",
    "        \"distributed.worker.memory.recent-to-old-time\": \"None\"\n",
    "    }\n",
    "\n",
    "    # Example of requested resource settings\n",
    "    requested_resources = {\n",
    "        \"interface\": f\"{interface}\",\n",
    "        \"queue\": f\"{queue}\",\n",
    "        \"cores\": cores,\n",
    "        \"processes\": processes,\n",
    "        \"memory\": f\"{memory}\",\n",
    "        \"walltime\": f\"{walltime}\",\n",
    "        \"job_extra_directives\": job_extra_directives,\n",
    "        \"number_of_nodes\": number_of_nodes\n",
    "    }\n",
    "\n",
    "    # Getting Dask configurations\n",
    "    dask_config = dask.config.config\n",
    "\n",
    "    # Overwrite the memory parameters if they are set in the Dask configuration\n",
    "    for param in memory_params.keys():\n",
    "        sections = param.split('.')\n",
    "        config = dask_config\n",
    "        for section in sections:\n",
    "            config = config.get(section, None)\n",
    "            if config is None:\n",
    "                break\n",
    "        if config is not None:\n",
    "            memory_params[param] = config\n",
    "\n",
    "    # Preparing sections\n",
    "    output = []\n",
    "\n",
    "    # Requested resources section\n",
    "    output.append(\"# Requested resources\")\n",
    "    for key, value in requested_resources.items():\n",
    "        output.append(f\"{key}={value}\")\n",
    "\n",
    "    # Memory configuration section\n",
    "    output.append(\"\\n# Dask memory configuration:\")\n",
    "    for key, value in memory_params.items():\n",
    "        output.append(f'\"{key}\": {value}')\n",
    "\n",
    "    # Section with all Dask configurations\n",
    "    output.append(\"\\n# Dask all configurations:\")\n",
    "    for section, config in dask_config.items():\n",
    "        if isinstance(config, dict):\n",
    "            output.append(f\"[{section}]\")\n",
    "            for key, value in config.items():\n",
    "                output.append(f\"{key}: {value}\")\n",
    "        else:\n",
    "            output.append(f\"{section}: {config}\")\n",
    "\n",
    "    # Saving to a file or displaying the result\n",
    "    with open(f'{logs_dir}/requested_resources_info.txt', 'w') as f:\n",
    "        f.write(\"\\n\".join(output))\n",
    "\n",
    "    print(\"Informations saved in requested_resources_info.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b6ac1d-7e92-4c66-b63c-17213017f148",
   "metadata": {},
   "source": [
    "# Preview of the input catalogs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45cc937-e3dd-4ad3-8c89-66ae5ebe0a48",
   "metadata": {},
   "source": [
    "## Loading the catalogs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589e7315-7346-46be-b018-4cdb7139cbc2",
   "metadata": {},
   "source": [
    "Loading the input catalogs with hats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d0b5dd-b0ae-4cff-bb1d-89c7c72828d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_first_catalog_as_left == True:\n",
    "    hats_left_catalog_name = hats_first_catalog_name\n",
    "    left_catalog_complete_dir = Path(hats_first_catalog_path)/Path(hats_first_catalog_name)\n",
    "    left_catalog_from_disk_lsdb = lsdb.read_hats(left_catalog_complete_dir)\n",
    "    left_catalog_from_disk_hats = hats.read_hats(left_catalog_complete_dir)\n",
    "    hats_left_margin_cache = hats_first_margin_cache\n",
    "    \n",
    "    id_left_name = id_first_name\n",
    "    ra_left_name = ra_first_name\n",
    "    dec_left_name = dec_first_name\n",
    "    \n",
    "    hats_right_catalog_name = hats_second_catalog_name\n",
    "    right_catalog_complete_dir = Path(hats_second_catalog_path)/Path(hats_second_catalog_name)\n",
    "    right_catalog_from_disk_lsdb = lsdb.read_hats(right_catalog_complete_dir)\n",
    "    right_catalog_from_disk_hats = hats.read_hats(right_catalog_complete_dir)\n",
    "    hats_right_margin_cache = hats_second_margin_cache\n",
    "    \n",
    "    id_right_name = id_second_name\n",
    "    ra_right_name = ra_second_name\n",
    "    dec_right_name = dec_second_name\n",
    "else:\n",
    "    hats_left_catalog_name = hats_second_catalog_name\n",
    "    left_catalog_complete_dir = Path(hats_second_catalog_path)/Path(hats_second_catalog_name)\n",
    "    left_catalog_from_disk_lsdb = lsdb.read_hats(left_catalog_complete_dir)\n",
    "    left_catalog_from_disk_hats = hats.read_hats(left_catalog_complete_dir)\n",
    "    hats_left_margin_cache = hats_second_margin_cache\n",
    "    \n",
    "    id_left_name = id_second_name\n",
    "    ra_left_name = ra_second_name\n",
    "    dec_left_name = dec_second_name\n",
    "    \n",
    "    hats_right_catalog_name = hats_first_catalog_name\n",
    "    right_catalog_complete_dir = Path(hats_first_catalog_path)/Path(hats_first_catalog_name)\n",
    "    right_catalog_from_disk_lsdb = lsdb.read_hats(right_catalog_complete_dir)\n",
    "    right_catalog_from_disk_hats = hats.read_hats(right_catalog_complete_dir)\n",
    "    hats_right_margin_cache = hats_first_margin_cache\n",
    "    \n",
    "    id_right_name = id_first_name\n",
    "    ra_right_name = ra_first_name\n",
    "    dec_right_name = dec_first_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0dd9ac-260d-44a1-85d5-83c3e682b9e1",
   "metadata": {},
   "source": [
    "## Making the pixels plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c8300f-f3c2-4172-8ae6-bf9bc0484e19",
   "metadata": {},
   "source": [
    "Making the pixels plot for the left catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55df398-e8be-4766-b471-7d9f59dd2300",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_the_info==True:\n",
    "    current_date = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "    \n",
    "    plot_pixels(left_catalog_from_disk_hats)\n",
    "    plt.savefig(f\"{logs_dir}/input_left_catalog_pixels_plot_{current_date}.png\")\n",
    "else:\n",
    "    plot_pixels(left_catalog_from_disk_hats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecf97fa-6178-4d83-9f79-714724d3c4e7",
   "metadata": {},
   "source": [
    "Making the pixels plot for the right catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9f6674-040e-4ec1-be60-8909e8e98ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_the_info==True:\n",
    "    current_date = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "    \n",
    "    plot_pixels(right_catalog_from_disk_hats)\n",
    "    plt.savefig(f\"{logs_dir}/input_right_catalog_pixels_plot_{current_date}.png\")\n",
    "else:\n",
    "    plot_pixels(right_catalog_from_disk_hats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c87fdae-c8a7-41d6-ba57-d62bca43bf4d",
   "metadata": {},
   "source": [
    "## Summarize pixels and sizes\n",
    "* \"healpix orders: distinct healpix orders represented in the partitions\n",
    "\n",
    "* num partitions: total number of partition files\n",
    "\n",
    "Size on disk data - using the file sizes fetched above, check the balance of your data. If your rows are fixed-width (e.g. no nested arrays, and few NaNs), the ratio here should be similar to the ratio above. If they’re very different, and you experience problems when parallelizing operations on your data, you may consider re-structuring the data representation.\n",
    "\n",
    "* min size_on_disk: smallest file (in GB)\n",
    "\n",
    "* max size_on_disk: largest file size (in GB)\n",
    "\n",
    "* size_on_disk ratio: max/min\n",
    "\n",
    "total size_on_disk: sum of all parquet catalog files (actual catalog size may vary due to other metadata files)\"\n",
    "\n",
    "Source: https://hats.readthedocs.io/en/stable/notebooks/catalog_size_inspection.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d21a638-1bb6-4151-ba40-ea49efa635f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "left_catalog = hats.read_hats(left_catalog_complete_dir)\n",
    "\n",
    "left_output_info_frame = left_catalog.partition_info.as_dataframe()\n",
    "\n",
    "for index, partition in left_output_info_frame.iterrows():\n",
    "    file_name = result = hats.io.paths.pixel_catalog_file(\n",
    "        left_catalog_complete_dir, HealpixPixel(partition[\"Norder\"], partition[\"Npix\"])\n",
    "    )\n",
    "    left_output_info_frame.loc[index, \"size_on_disk\"] = os.path.getsize(file_name)\n",
    "\n",
    "left_output_info_frame = left_output_info_frame.astype(int)\n",
    "left_output_info_frame[\"gbs\"] = left_output_info_frame[\"size_on_disk\"] / (1024 * 1024 * 1024)\n",
    "\n",
    "############################################################################################\n",
    "right_catalog = hats.read_hats(right_catalog_complete_dir)\n",
    "\n",
    "right_output_info_frame = right_catalog.partition_info.as_dataframe()\n",
    "\n",
    "for index, partition in right_output_info_frame.iterrows():\n",
    "    file_name = result = hats.io.paths.pixel_catalog_file(\n",
    "        right_catalog_complete_dir, HealpixPixel(partition[\"Norder\"], partition[\"Npix\"])\n",
    "    )\n",
    "    right_output_info_frame.loc[index, \"size_on_disk\"] = os.path.getsize(file_name)\n",
    "\n",
    "right_output_info_frame = right_output_info_frame.astype(int)\n",
    "right_output_info_frame[\"gbs\"] = right_output_info_frame[\"size_on_disk\"] / (1024 * 1024 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5ee540-38be-4bfd-8c58-6215f85d3ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "if save_the_info == True:\n",
    "    current_date = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "    with open(f\"{logs_dir}/input_summarize_pixels_{current_date}.txt\", \"w\") as file:\n",
    "        # Informações do catálogo à esquerda\n",
    "        file.write(f'Left catalog: {hats_left_catalog_name}\\n')\n",
    "        file.write(f'healpix orders: {left_output_info_frame[\"Norder\"].unique()}\\n')\n",
    "        file.write(f'num partitions: {len(left_output_info_frame[\"Npix\"])}\\n')\n",
    "        file.write(\"------\\n\")\n",
    "        file.write(f'min size_on_disk: {left_output_info_frame[\"gbs\"].min():.6f}\\n')\n",
    "        file.write(f'max size_on_disk: {left_output_info_frame[\"gbs\"].max():.6f}\\n')\n",
    "        file.write(f'size_on_disk ratio: {left_output_info_frame[\"gbs\"].max()/left_output_info_frame[\"gbs\"].min():.6f}\\n')\n",
    "        file.write(f'total size_on_disk: {left_output_info_frame[\"gbs\"].sum():.6f}\\n\\n')\n",
    "\n",
    "        # Informações do catálogo à direita\n",
    "        file.write(f'Right catalog: {hats_right_catalog_name}\\n')\n",
    "        file.write(f'healpix orders: {right_output_info_frame[\"Norder\"].unique()}\\n')\n",
    "        file.write(f'num partitions: {len(right_output_info_frame[\"Npix\"])}\\n')\n",
    "        file.write(\"------\\n\")\n",
    "        file.write(f'min size_on_disk: {right_output_info_frame[\"gbs\"].min():.6f}\\n')\n",
    "        file.write(f'max size_on_disk: {right_output_info_frame[\"gbs\"].max():.6f}\\n')\n",
    "        file.write(f'size_on_disk ratio: {right_output_info_frame[\"gbs\"].max()/right_output_info_frame[\"gbs\"].min():.6f}\\n')\n",
    "        file.write(f'total size_on_disk: {right_output_info_frame[\"gbs\"].sum():.6f}\\n\\n')\n",
    "\n",
    "if show_info_inline == True:\n",
    "    # Informações do catálogo à esquerda\n",
    "    print(f'Left catalog: {hats_left_catalog_name}')\n",
    "    print(f'healpix orders: {left_output_info_frame[\"Norder\"].unique()}')\n",
    "    print(f'num partitions: {len(left_output_info_frame[\"Npix\"])}')\n",
    "    print(\"------\")\n",
    "    print(f'min size_on_disk: {left_output_info_frame[\"gbs\"].min():.6f}')\n",
    "    print(f'max size_on_disk: {left_output_info_frame[\"gbs\"].max():.6f}')\n",
    "    print(f'size_on_disk ratio: {left_output_info_frame[\"gbs\"].max()/left_output_info_frame[\"gbs\"].min():.6f}')\n",
    "    print(f'total size_on_disk: {left_output_info_frame[\"gbs\"].sum():.6f}\\n')\n",
    "\n",
    "    # Informações do catálogo à direita\n",
    "    print(f'Right catalog: {hats_right_catalog_name}')\n",
    "    print(f'healpix orders: {right_output_info_frame[\"Norder\"].unique()}')\n",
    "    print(f'num partitions: {len(right_output_info_frame[\"Npix\"])}')\n",
    "    print(\"------\")\n",
    "    print(f'min size_on_disk: {right_output_info_frame[\"gbs\"].min():.6f}')\n",
    "    print(f'max size_on_disk: {right_output_info_frame[\"gbs\"].max():.6f}')\n",
    "    print(f'size_on_disk ratio: {right_output_info_frame[\"gbs\"].max()/right_output_info_frame[\"gbs\"].min():.6f}')\n",
    "    print(f'total size_on_disk: {right_output_info_frame[\"gbs\"].sum():.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d153f682-5435-442f-b3ca-bfaec8472e00",
   "metadata": {},
   "source": [
    "## File size distribution\n",
    "\"Below we look at histograms of file sizes.\n",
    "\n",
    "In our initial testing, we find that there’s a “sweet spot” file size of 100MB-1GB. Files that are smaller create more overhead for individual reads. Files that are much larger may create slow-downs when cross-matching between catalogs. Files that are much larger can create out-of-memory issues for dask when loading from disk.\n",
    "\n",
    "The majority of your files should be in the “sweet spot”, and no files in the “too-big” category.\"\n",
    "\n",
    "Source: https://hats.readthedocs.io/en/stable/notebooks/catalog_size_inspection.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61edccf6-da5b-42a4-a56a-8cffb3db0bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file_size_info(info_frame, type_of_files, bins, labels, logs_dir, save=False, show=False):\n",
    "    \n",
    "    plt.hist(info_frame[\"gbs\"], edgecolor='black')\n",
    "    plt.xlabel(\"File size (GB)\")\n",
    "    plt.ylabel(\"Number of files\")\n",
    "\n",
    "    if save:\n",
    "        current_date = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "        plt.savefig(f\"{logs_dir}/{type_of_files}_file_size_histogram_{current_date}.png\")\n",
    "\n",
    "        hist = np.histogram(info_frame[\"gbs\"], bins=bins)[0]\n",
    "        pcts = hist / len(info_frame)\n",
    "\n",
    "        with open(f\"{logs_dir}/{type_of_files}_file_size_distribution_{current_date}.txt\", \"w\") as file:\n",
    "            for i, label in enumerate(labels):\n",
    "                file.write(f\"{label} \\t: {hist[i]} \\t({pcts[i]*100:.1f} %)\\n\")\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "        hist = np.histogram(info_frame[\"gbs\"], bins=bins)[0]\n",
    "        pcts = hist / len(info_frame)\n",
    "\n",
    "        for i, label in enumerate(labels):\n",
    "            print(f\"{label} \\t: {hist[i]} \\t({pcts[i]*100:.1f} %)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189ec038-780b-4266-8e42-2736e9082f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_type_of_files = 'input_left'\n",
    "right_type_of_files = 'input_right'\n",
    "\n",
    "bins = [0, 0.5, 1, 2, 100]\n",
    "labels = [\"small-ish\", \"sweet-spot\", \"big-ish\", \"too-big\"]\n",
    "\n",
    "if save_the_info:\n",
    "    logs_dir = logs_dir\n",
    "else:\n",
    "    logs_dir=None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d47521-56f1-4629-975e-bdb98f84ad02",
   "metadata": {},
   "source": [
    "Left catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499a9709-8e34-45b8-b95b-1e210f6b7165",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_file_size_info(\n",
    "    info_frame=left_output_info_frame,\n",
    "    type_of_files=left_type_of_files,\n",
    "    bins=bins,\n",
    "    labels=labels,\n",
    "    logs_dir=logs_dir,\n",
    "    save=save_the_info,\n",
    "    show=show_info_inline\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88d50f8-8619-4783-adcc-42b022154754",
   "metadata": {},
   "source": [
    "Right catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c693b5-ca6d-4ba6-9dc2-fdd5dfbbfac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_file_size_info(\n",
    "    info_frame=right_output_info_frame,\n",
    "    type_of_files=right_type_of_files,\n",
    "    bins=bins,\n",
    "    labels=labels,\n",
    "    logs_dir=logs_dir,\n",
    "    save=save_the_info,\n",
    "    show=show_info_inline\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a221f1-9897-4515-a829-f877aafd8221",
   "metadata": {},
   "source": [
    "## Computing the total number of rows and columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d749736-cb53-4ab8-a6fd-6fd4c8cbc972",
   "metadata": {},
   "source": [
    "Computing the number of rows in the HATS catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d53a420-ced2-44d7-956e-75834b749a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_catalog_total_columns = left_catalog_from_disk_lsdb.columns.to_list()\n",
    "left_catalog_total_rows = left_catalog_from_disk_hats.catalog_info.total_rows\n",
    "\n",
    "right_catalog_total_columns = right_catalog_from_disk_lsdb.columns.to_list()\n",
    "right_catalog_total_rows = right_catalog_from_disk_hats.catalog_info.total_rows\n",
    "\n",
    "if show_info_inline == True:\n",
    "    print(f\"Left HATS catalog path: {left_catalog_complete_dir} \\n\")\n",
    "    print(f\"Total number of rows: {left_catalog_total_rows}\\n\")\n",
    "    print(f\"Total number of columns: {len(left_catalog_total_columns)}\\n\\n\")\n",
    "    \n",
    "    print(f\"Right HATS catalog path: {right_catalog_complete_dir} \\n\")\n",
    "    print(f\"Total number of rows: {right_catalog_total_rows}\\n\")\n",
    "    print(f\"Total number of columns: {len(right_catalog_total_columns)}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c877f863-d454-42dd-b46e-7e500158c70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_the_info == True:\n",
    "    with open(f'{logs_dir}/input_total_len_of_input_catalogs_{current_date}.txt', 'a') as f:\n",
    "        f.write(f\"Left HATS catalog path: {left_catalog_complete_dir}\\n\")\n",
    "        f.write(f\"Total number of rows: {left_catalog_total_rows}\\n\")\n",
    "        f.write(f\"Total number of columns: {len(left_catalog_total_columns)}\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Right HATS catalog path: {right_catalog_complete_dir}\\n\")\n",
    "        f.write(f\"Total number of rows: {right_catalog_total_rows}\\n\")\n",
    "        f.write(f\"Total number of columns: {len(right_catalog_total_columns)}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e93bcd-49d1-4046-8e49-5e380cc024a6",
   "metadata": {},
   "source": [
    "# Saving libraries and jobs informations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f2a3df-6449-4ff8-99fa-a737a5484923",
   "metadata": {},
   "source": [
    "Saving the libraries versions information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5acac0c-622f-433d-b75a-7fdfada2eb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_the_info==True:\n",
    "    current_date = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "    with open(f'{logs_dir}/main_lib_versions_{current_date}.txt', 'w') as f:\n",
    "        f.write(f'python version: {sys.version} \\n')\n",
    "        f.write(f'numpy version: {np.__version__} \\n')\n",
    "        f.write(f'dask version: {dask.__version__} \\n')\n",
    "        f.write(f'dask_jobqueue version: {dask_jobqueue.__version__} \\n')\n",
    "        f.write(f'hats version: {hats_version} \\n')\n",
    "        f.write(f'hats_import version: {hats_import.__version__} \\n')\n",
    "        f.write(f'lsdb version: {lsdb.__version__} \\n')\n",
    "    print(f'File saved as: {logs_dir}/main_lib_versions_{current_date}.txt \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6645aff1-b25d-4cd2-a1a5-c3f0915f10d8",
   "metadata": {},
   "source": [
    "Defining functions to get informations about the jobs running in the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1849336-3caf-4a5a-96b6-f8488646a6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to collect information about a job using the scontrol show job command\n",
    "def get_scontrol_job_info(job_id):\n",
    "    # Remove any interval or `%` from job_id\n",
    "    clean_job_id = re.sub(r'\\[.*?\\]', '', job_id)\n",
    "    \n",
    "    # Execute scontrol show job\n",
    "    result = subprocess.run(['scontrol', 'show', 'job', clean_job_id], stdout=subprocess.PIPE)\n",
    "    job_info = result.stdout.decode('utf-8')\n",
    "    \n",
    "    job_dict = {}\n",
    "    \n",
    "    # Process the info line by line\n",
    "    for line in job_info.splitlines():\n",
    "        items = line.split()\n",
    "        for item in items:\n",
    "            if \"=\" in item:\n",
    "                key, value = item.split(\"=\", 1)\n",
    "                job_dict[key] = value\n",
    "    \n",
    "    return job_dict\n",
    "\n",
    "# Function to collect information about all jobs of the user\n",
    "def get_all_jobs_info_MINE():\n",
    "    # Gets the username using os.getenv('USER')\n",
    "    user = os.getenv('USER')\n",
    "    \n",
    "    # Captures the list of running jobs for the user\n",
    "    result = subprocess.run(['squeue', '-u', user, '-h', '-o', '%i'], stdout=subprocess.PIPE)\n",
    "    job_ids = result.stdout.decode('utf-8').splitlines()\n",
    "\n",
    "    # Collects information for each job\n",
    "    jobs_info = []\n",
    "    for job_id in job_ids:\n",
    "        # Removes intervals or % from job_id before passing it to scontrol\n",
    "        clean_job_id = re.sub(r'\\[.*?\\]', '', job_id)\n",
    "        try:\n",
    "            job_info = get_scontrol_job_info(clean_job_id)\n",
    "            jobs_info.append(job_info)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing job {job_id}: {e}\")\n",
    "    \n",
    "    # Converts the list of dictionaries into a Pandas DataFrame\n",
    "    df = pd.DataFrame(jobs_info)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Function to collect information about all jobs that do not belong to the current user\n",
    "def get_all_jobs_info_NOT_MINE():\n",
    "    current_user = os.getenv('USER')\n",
    "    \n",
    "    # Captures the list of running jobs\n",
    "    result = subprocess.run(['squeue', '-h', '-o', '%i %u'], stdout=subprocess.PIPE)\n",
    "    job_lines = result.stdout.decode('utf-8').splitlines()\n",
    "    \n",
    "    # Filters jobs from other users\n",
    "    jobs_info = []\n",
    "    for line in job_lines:\n",
    "        job_id, user = line.split()\n",
    "        \n",
    "        # Ignores jobs belonging to the current user\n",
    "        if user != current_user:\n",
    "            # Removes intervals or % from job_id before passing it to scontrol\n",
    "            clean_job_id = re.sub(r'\\[.*?\\]', '', job_id)\n",
    "            try:\n",
    "                job_info = get_scontrol_job_info(clean_job_id)\n",
    "                jobs_info.append(job_info)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing job {job_id}: {e}\")\n",
    "    \n",
    "    # Converts to DataFrame\n",
    "    df = pd.DataFrame(jobs_info)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9c84db-9700-400a-aa0b-c010ea373573",
   "metadata": {},
   "source": [
    "Getting my jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593606a4-e1bf-42fe-9896-0ad3ebdd0772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collects information of all jobs and saves it in the DataFrame\n",
    "df_jobs_MINE = get_all_jobs_info_MINE()\n",
    "\n",
    "if show_info_inline==True:\n",
    "    print(df_jobs_MINE[['JobId','NodeList','NumNodes','NumCPUs','NumTasks','CPUs/Task','TRES']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e85729-f42f-48d7-bce6-0ce997101117",
   "metadata": {},
   "source": [
    "Getting other people jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34159ad-c991-4086-b087-ca3f69606c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collects information of all jobs and saves it in the DataFrame\n",
    "df_jobs_NOT_MINE = get_all_jobs_info_NOT_MINE()\n",
    "\n",
    "if len(df_jobs_NOT_MINE)!=0:\n",
    "    if show_info_inline==True:\n",
    "        print(df_jobs_NOT_MINE[['JobId','NodeList','NumNodes','NumCPUs','NumTasks','CPUs/Task','TRES']])\n",
    "else:\n",
    "    df_jobs_NOT_MINE_EMPTY_MSG = pd.DataFrame({\"EMPTY\": [\"There are no other jobs running in the cluster.\"]})\n",
    "    print(\"There are no other jobs running in the cluster.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e383e2e-b6a1-46e8-9b00-f48e12ebcc14",
   "metadata": {},
   "source": [
    "Saving the data of the jobs in a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2968af24-9c87-4678-b274-37abad45ab2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_the_info==True:\n",
    "    current_date = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "    \n",
    "    file_name_MINE = f'{logs_dir}/jobs_info_MINE_{current_date}.csv'\n",
    "    file_name_NOT_MINE = f'{logs_dir}/jobs_info_NOT_MINE_{current_date}.csv'\n",
    "    \n",
    "    df_jobs_MINE.to_csv(file_name_MINE, index=False)\n",
    "    if len(df_jobs_NOT_MINE)!=0:\n",
    "        df_jobs_NOT_MINE.to_csv(file_name_NOT_MINE, index=False)\n",
    "    else:\n",
    "        df_jobs_NOT_MINE_EMPTY_MSG.to_csv(file_name_NOT_MINE, index=False)\n",
    "        \n",
    "    print(f'Files saved as: \\n')\n",
    "    print(f'{file_name_MINE} \\n')\n",
    "    print(f'{file_name_NOT_MINE} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08688d78-9ee4-4780-b2cc-a0596d046ab4",
   "metadata": {},
   "source": [
    "# Doing the cross-matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a0b509-0797-410e-959e-c692396995e6",
   "metadata": {},
   "source": [
    "Doing the cross-matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bd6ec4-c183-42c2-b15e-a086d2e62f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_the_pipeline==True:\n",
    "    ################################## INPUT CONFIGS #################################\n",
    "\n",
    "    LEFT_HATS_DIR = left_catalog_complete_dir\n",
    "    LEFT_CATALOG_HATS_NAME = hats_left_catalog_name\n",
    "    RIGHT_HATS_DIR = right_catalog_complete_dir\n",
    "    RIGHT_CATALOG_HATS_NAME = hats_right_catalog_name\n",
    "    RIGHT_MARGIN_CACHE_DIR = hats_right_margin_cache\n",
    "\n",
    "    CROSS_MATCHING_RADIUS = 1.0 # Up to 1 arcsec distance, it is the default\n",
    "    NEIGHBORS_NUMBER = 1 # Single closest object, it is the default\n",
    "    ###########################################################################################\n",
    "\n",
    "    ################################# CONFIGURAÇÕES DE OUTPUT #################################\n",
    "    ### Output directory for the x-matching and logs.\n",
    "    HATS_DIR = Path(x_matching_path)\n",
    "    LOGS_DIR = Path(logs_dir)\n",
    "\n",
    "    XMATCH_NAME = LEFT_CATALOG_HATS_NAME+'_x_'+RIGHT_CATALOG_HATS_NAME\n",
    "    OUTPUT_HATS_DIR = HATS_DIR / XMATCH_NAME\n",
    "\n",
    "    ### Path to dask performance report.\n",
    "    PERFORMANCE_REPORT_NAME = f'dask_performance_report_{current_date}.html'\n",
    "    PERFORMANCE_DIR = LOGS_DIR / PERFORMANCE_REPORT_NAME\n",
    "    ###########################################################################################\n",
    "\n",
    "    ############################### EXECUTANDO O PIPELINE ######################################\n",
    "    with performance_report(filename=PERFORMANCE_DIR):\n",
    "        left_catalog = lsdb.read_hats(LEFT_HATS_DIR)\n",
    "        right_catalog = lsdb.read_hats(RIGHT_HATS_DIR, margin_cache=RIGHT_MARGIN_CACHE_DIR)\n",
    "    \n",
    "        xmatched = left_catalog.crossmatch(\n",
    "            right_catalog,\n",
    "            radius_arcsec=CROSS_MATCHING_RADIUS,\n",
    "            n_neighbors=NEIGHBORS_NUMBER,\n",
    "            suffixes=(LEFT_CATALOG_HATS_NAME, RIGHT_CATALOG_HATS_NAME),\n",
    "        )\n",
    "        xmatched.to_hats(OUTPUT_HATS_DIR, overwrite=True)\n",
    "###########################################################################################\n",
    "else:\n",
    "    print('You selected not to run the pipeline.') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19547d2-0269-41ea-9a76-217087f8c736",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Analysing the outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f3182d-a2dc-482b-9a96-c9d555cfc52c",
   "metadata": {},
   "source": [
    "## Loading the catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcf6e73-95cd-4046-b515-ffdde35c342c",
   "metadata": {},
   "source": [
    "Loading the crossmatching output catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714e9f04-d750-429b-8261-8a23dbb7a61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_catalog_complete_dir = OUTPUT_HATS_DIR\n",
    "output_catalog_from_disk_lsdb = lsdb.read_hats(output_catalog_complete_dir)\n",
    "output_catalog_from_disk_hats = hats.read_hats(output_catalog_complete_dir)\n",
    "\n",
    "print(OUTPUT_HATS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d58a03-b8ec-4853-9d28-c27e60137acf",
   "metadata": {},
   "source": [
    "Loading the catalog as a dask dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50ebe50-d6a9-46db-9b9b-627e6ee4707b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_catalog_from_disk_delayed = output_catalog_from_disk_lsdb.to_delayed()\n",
    "output_catalog_from_disk_ddf = dd.from_delayed(output_catalog_from_disk_delayed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ffa7ad-b30f-4db3-9b00-5559b542cdfa",
   "metadata": {},
   "source": [
    "## Making the pixels plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4586ee4-156e-43cc-99b4-8116e3b14f87",
   "metadata": {},
   "source": [
    "Making the pixels plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0086b557-11da-4b92-8172-bad16d84a0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_the_info==True:\n",
    "    current_date = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "    \n",
    "    plot_pixels(output_catalog_from_disk_hats)\n",
    "    plt.savefig(f\"{logs_dir}/output_catalog_pixels_plot_{current_date}.png\")\n",
    "else:\n",
    "    plot_pixels(output_catalog_from_disk_hats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a096f8b-7d17-4de7-8533-d7edee4917b7",
   "metadata": {},
   "source": [
    "## Summarize pixels and sizes\n",
    "* \"healpix orders: distinct healpix orders represented in the partitions\n",
    "\n",
    "* num partitions: total number of partition files\n",
    "\n",
    "Size on disk data - using the file sizes fetched above, check the balance of your data. If your rows are fixed-width (e.g. no nested arrays, and few NaNs), the ratio here should be similar to the ratio above. If they’re very different, and you experience problems when parallelizing operations on your data, you may consider re-structuring the data representation.\n",
    "\n",
    "* min size_on_disk: smallest file (in GB)\n",
    "\n",
    "* max size_on_disk: largest file size (in GB)\n",
    "\n",
    "* size_on_disk ratio: max/min\n",
    "\n",
    "total size_on_disk: sum of all parquet catalog files (actual catalog size may vary due to other metadata files)\"\n",
    "\n",
    "Source: https://hats.readthedocs.io/en/stable/notebooks/catalog_size_inspection.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5b6057-8e5f-4352-98c9-42d56bfafd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = hats.read_hats(OUTPUT_HATS_DIR)\n",
    "\n",
    "output_info_frame = catalog.partition_info.as_dataframe()\n",
    "\n",
    "for index, partition in output_info_frame.iterrows():\n",
    "    file_name = result = hats.io.paths.pixel_catalog_file(\n",
    "        OUTPUT_HATS_DIR, HealpixPixel(partition[\"Norder\"], partition[\"Npix\"])\n",
    "    )\n",
    "    output_info_frame.loc[index, \"size_on_disk\"] = os.path.getsize(file_name)\n",
    "\n",
    "output_info_frame = output_info_frame.astype(int)\n",
    "output_info_frame[\"gbs\"] = output_info_frame[\"size_on_disk\"] / (1024 * 1024 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d006188c-c57f-48d2-97f4-03cfd0e45898",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_the_info==True:\n",
    "    current_date = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "    with open(f\"{logs_dir}/output_summarize_pixels_{current_date}.txt\", \"w\") as file:\n",
    "        file.write(f'healpix orders: {output_info_frame[\"Norder\"].unique()}\\n')\n",
    "        file.write(f'num partitions: {len(output_info_frame[\"Npix\"])}\\n')\n",
    "        file.write(\"------\\n\")\n",
    "        file.write(f'min size_on_disk: {output_info_frame[\"gbs\"].min():.6f}\\n')\n",
    "        file.write(f'max size_on_disk: {output_info_frame[\"gbs\"].max():.6f}\\n')\n",
    "        file.write(f'size_on_disk ratio: {output_info_frame[\"gbs\"].max()/output_info_frame[\"gbs\"].min():.6f}\\n')\n",
    "        file.write(f'total size_on_disk: {output_info_frame[\"gbs\"].sum():.6f}\\n')\n",
    "if show_info_inline==True:\n",
    "    print(f'healpix orders: {output_info_frame[\"Norder\"].unique()}')\n",
    "    print(f'num partitions: {len(output_info_frame[\"Npix\"])}')\n",
    "    print(\"------\")\n",
    "    print(f'min size_on_disk: {output_info_frame[\"gbs\"].min():.6f}')\n",
    "    print(f'max size_on_disk: {output_info_frame[\"gbs\"].max():.6f}')\n",
    "    print(f'size_on_disk ratio: {output_info_frame[\"gbs\"].max()/output_info_frame[\"gbs\"].min():.6f}')\n",
    "    print(f'total size_on_disk: {output_info_frame[\"gbs\"].sum():.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4db0713-e5e8-4f27-af23-0eb081e8288d",
   "metadata": {},
   "source": [
    "## File size distribution\n",
    "\"Below we look at histograms of file sizes.\n",
    "\n",
    "In our initial testing, we find that there’s a “sweet spot” file size of 100MB-1GB. Files that are smaller create more overhead for individual reads. Files that are much larger may create slow-downs when cross-matching between catalogs. Files that are much larger can create out-of-memory issues for dask when loading from disk.\n",
    "\n",
    "The majority of your files should be in the “sweet spot”, and no files in the “too-big” category.\"\n",
    "\n",
    "Source: https://hats.readthedocs.io/en/stable/notebooks/catalog_size_inspection.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7563128-df1d-4693-a758-7cd54801e683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file_size_info(info_frame, type_of_files, bins, labels, logs_dir, save=False, show=False):\n",
    "    \n",
    "    plt.hist(info_frame[\"gbs\"], edgecolor='black')\n",
    "    plt.xlabel(\"File size (GB)\")\n",
    "    plt.ylabel(\"Number of files\")\n",
    "\n",
    "    if save:\n",
    "        current_date = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "        plt.savefig(f\"{logs_dir}/{type_of_files}_file_size_histogram_{current_date}.png\")\n",
    "\n",
    "        hist = np.histogram(info_frame[\"gbs\"], bins=bins)[0]\n",
    "        pcts = hist / len(info_frame)\n",
    "\n",
    "        with open(f\"{logs_dir}/{type_of_files}_file_size_distribution_{current_date}.txt\", \"w\") as file:\n",
    "            for i, label in enumerate(labels):\n",
    "                file.write(f\"{label} \\t: {hist[i]} \\t({pcts[i]*100:.1f} %)\\n\")\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "        hist = np.histogram(info_frame[\"gbs\"], bins=bins)[0]\n",
    "        pcts = hist / len(info_frame)\n",
    "\n",
    "        for i, label in enumerate(labels):\n",
    "            print(f\"{label} \\t: {hist[i]} \\t({pcts[i]*100:.1f} %)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b5f2c0-096e-4fae-b170-2ab65a3c97e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_of_files = 'output'\n",
    "bins = [0, 0.5, 1, 2, 100]\n",
    "labels = [\"small-ish\", \"sweet-spot\", \"big-ish\", \"too-big\"]\n",
    "\n",
    "if save_the_info:\n",
    "    logs_dir = logs_dir\n",
    "else:\n",
    "    logs_dir=None\n",
    "\n",
    "process_file_size_info(\n",
    "    info_frame=output_info_frame,\n",
    "    type_of_files=type_of_files,\n",
    "    bins=bins,\n",
    "    labels=labels,\n",
    "    logs_dir=logs_dir,\n",
    "    save=save_the_info,\n",
    "    show=show_info_inline\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069b6645-f703-4bf7-80bc-165b76daf859",
   "metadata": {},
   "source": [
    "## Computing the total number of rows and columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946b2f4b-dc86-4c66-9d1c-831717bc8d43",
   "metadata": {},
   "source": [
    "Computing the total number of rows and columns for the output catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5f427b-9ff8-4546-a9f3-2500afa8b739",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_catalog_total_columns = output_catalog_from_disk_lsdb.columns.to_list()\n",
    "output_catalog_total_rows = output_catalog_from_disk_hats.catalog_info.total_rows\n",
    "\n",
    "if show_info_inline == True:\n",
    "    print(f\"HATS catalog path: {output_catalog_complete_dir} \\n\")\n",
    "    print(f\"Total number of rows: {output_catalog_total_rows}\\n\")\n",
    "    print(f\"Total number of columns: {len(output_catalog_total_columns)}\\n\\n\")\n",
    "    \n",
    "if save_the_info == True:\n",
    "    with open(f'{logs_dir}/output_total_len_of_files_{current_date}.txt', 'a') as f:\n",
    "        f.write(f\"HATS catalog path: {output_catalog_complete_dir}\\n\")\n",
    "        f.write(f\"Total number of rows: {output_catalog_total_rows}\\n\")\n",
    "        f.write(f\"Total number of columns: {len(output_catalog_total_columns)}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb41676c-9f1a-4185-b1a0-3a8676e420a1",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdbcb22-b76c-4035-aede-84e50d6e7db8",
   "metadata": {},
   "source": [
    "Columns to be used for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422e2a4f-b4ed-433e-88e8-e1120c701096",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_left_output_name = id_left_name+hats_left_catalog_name\n",
    "ra_left_output_name = ra_left_name+hats_left_catalog_name\n",
    "dec_left_output_name = dec_left_name+hats_left_catalog_name\n",
    "\n",
    "id_right_output_name = id_right_name+hats_right_catalog_name\n",
    "ra_right_output_name = ra_right_name+hats_right_catalog_name\n",
    "dec_right_output_name = dec_right_name+hats_right_catalog_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b19eaa-11bb-4455-a205-cbda0ce34e95",
   "metadata": {},
   "source": [
    "### Plotting a region in the sky"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e07b18e-ca4f-4e6b-a4a0-94e60e4a3fc1",
   "metadata": {},
   "source": [
    "First, select the coordinates of the region for the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f4da1d-9b9d-49bb-924c-51f61ef10be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ra_min = 70\n",
    "ra_max = 70.5\n",
    "dec_min = -30\n",
    "dec_max = -29.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21329845-ed32-42da-bb9a-c0a2954fdc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"R.A. min: {ra_min}\")\n",
    "print(f\"R.A. max: {ra_max}\")\n",
    "print(f\"DEC min: {dec_min}\")\n",
    "print(f\"DEC max: {dec_max}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d639636b-b323-4fdb-a7fc-eb38d461e772",
   "metadata": {},
   "source": [
    "We use the polygon_search method from LSDB lib to select this region of interest in the catalogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d9a76a-eeb5-4990-9cf8-06cb13b7c0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "polygon_coords = [[ra_min, dec_max], [ra_max, dec_max], [ra_max, dec_min], [ra_min, dec_min]]\n",
    "\n",
    "left_catalog_box = left_catalog_from_disk_lsdb.polygon_search(polygon_coords).compute()\n",
    "right_catalog_box = right_catalog_from_disk_lsdb.polygon_search(polygon_coords).compute()\n",
    "output_catalog_box = output_catalog_from_disk_lsdb.polygon_search(polygon_coords).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06278a7f-f2dd-44d0-86c4-d7c431911dd9",
   "metadata": {},
   "source": [
    "Converting the R.A. coordinates to the interval $(-180^{\\circ}, 180^{\\circ}]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c5b067-576f-49db-9b6d-7aa19fa0a994",
   "metadata": {},
   "outputs": [],
   "source": [
    "ra_left = np.where(left_catalog_box[ra_left_name] > 180, left_catalog_box[ra_left_name] - 360, left_catalog_box[ra_left_name])\n",
    "ra_right = np.where(right_catalog_box[ra_right_name] > 180, right_catalog_box[ra_right_name] - 360, right_catalog_box[ra_right_name])\n",
    "ra_output = np.where(output_catalog_box[ra_right_output_name] > 180, output_catalog_box[ra_right_output_name] - 360, output_catalog_box[ra_right_output_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cecd3d-28ec-463f-8502-3e0f4a059d33",
   "metadata": {},
   "source": [
    "Making the plot with matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62926df-1703-4d88-b3eb-8a89287cdcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(ra_left, left_catalog_box[dec_left_name], s=10, alpha=0.5, marker=\"+\", color=\"blue\", label=\"left\")\n",
    "plt.scatter(ra_right, right_catalog_box[dec_right_name], s=1, alpha=0.05, color=\"green\", label=\"right\")\n",
    "plt.scatter(ra_output, output_catalog_box[dec_right_output_name], s=5, alpha=0.8, color=\"red\", label=\"x-matched\")\n",
    "plt.xlabel(\"R.A. (deg)\")\n",
    "plt.ylabel(\"DEC (deg)\")\n",
    "plt.title(f'X-matched points - {hats_left_catalog_name} vs {hats_right_catalog_name}')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.gca().invert_xaxis()  # Inverter o eixo x\n",
    "\n",
    "if save_the_info == True:\n",
    "    plt.savefig(f\"{logs_dir}/output_small_region_{ra_min}_{ra_max}_{dec_min}_{dec_max}.png\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117420bc-1c82-4c24-b019-adeba5743abd",
   "metadata": {},
   "source": [
    "### Plotting all the points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642e6cf8-41e6-4fcd-9e98-c3603d288748",
   "metadata": {},
   "source": [
    "Filtering the dataframe to contain only id, ra, dec and distance info, and computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897f2ffd-4d71-4ce7-b462-09f3dfaab5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar um novo DataFrame contendo apenas as colunas relevantes e garantir que seja independente\n",
    "filtered_df = output_catalog_from_disk_ddf[[id_left_output_name, ra_left_output_name, dec_left_output_name,\n",
    "                                            id_right_output_name, ra_right_output_name, dec_right_output_name, '_dist_arcsec']].compute()\n",
    "\n",
    "# Converter o DataFrame para tipos nativos do Pandas/NumPy\n",
    "filtered_df = filtered_df.astype({\n",
    "    ra_left_output_name: 'float64',\n",
    "    dec_left_output_name: 'float64',\n",
    "    ra_right_output_name: 'float64',\n",
    "    dec_right_output_name: 'float64',\n",
    "    '_dist_arcsec': 'float64'\n",
    "})\n",
    "\n",
    "# Computar os valores mínimo e máximo de '_dist_arcsec'\n",
    "lowest_dist_value = filtered_df['_dist_arcsec'].min()\n",
    "highest_dist_value = filtered_df['_dist_arcsec'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c477e02f-99c6-476b-a50b-65d622aaccef",
   "metadata": {},
   "source": [
    "Plotting the spatial distribution of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2aa7b6-4c93-46fb-8ee2-b9bf8fd19c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar o gráfico 2D com Holoviews\n",
    "points_counts = hv.Points(\n",
    "    filtered_df,\n",
    "    kdims=[ra_left_output_name, dec_right_output_name]\n",
    ")\n",
    "\n",
    "# Aplicar rasterização com contagem de pontos\n",
    "rasterized_counts = rasterize(points_counts, aggregator='count')\n",
    "\n",
    "# Adicionar opções ao gráfico com formatação personalizada no ColorBar\n",
    "rasterized_counts = rasterized_counts.opts(\n",
    "    width=750, height=500,\n",
    "    cmap=viridis,\n",
    "    colorbar=True,\n",
    "    colorbar_opts={\n",
    "        'formatter': PrintfTickFormatter(format=\"%.0f\"),  # Formato inteiro para contagem\n",
    "        'title': 'Counts',  # Título da barra de cores\n",
    "    },\n",
    "    tools=['hover'], \n",
    "    cnorm='linear',  # Linear por padrão para contagem\n",
    "    xlabel='RA (deg)', ylabel='DEC (deg)',\n",
    "    fontsize={'xticks': 12, 'yticks': 12, 'xlabel': 14, 'ylabel': 14},\n",
    "    invert_xaxis=True  # Inverte apenas o eixo X\n",
    ")\n",
    "\n",
    "# Salvar o gráfico como PNG, se necessário\n",
    "if save_the_info:\n",
    "    output_image_path = os.path.join(logs_dir, 'output_spatial_distribution_counts.html')\n",
    "    hv.save(rasterized_counts, output_image_path, fmt='html')\n",
    "    print(f\"Plot saved in: {output_image_path}\")\n",
    "\n",
    "# Renderizar e exibir o gráfico\n",
    "hv.output(rasterized_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f4dcd9-6487-42ca-a761-17b9861516b4",
   "metadata": {},
   "source": [
    "Plotting the spatial distribution of points and coloring according to the separation distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dd0266-9e33-4942-aa04-d2fe98f1a408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar o gráfico 2D com Holoviews\n",
    "points = hv.Points(\n",
    "    filtered_df,\n",
    "    kdims=[ra_left_output_name, dec_right_output_name],\n",
    "    vdims=['_dist_arcsec']\n",
    ")\n",
    "\n",
    "# Aplicar rasterização com equalização de histograma\n",
    "rasterized = rasterize(points, aggregator='mean')\n",
    "\n",
    "# Adicionar opções ao gráfico com formatação personalizada no ColorBar\n",
    "rasterized = rasterized.opts(\n",
    "    width=750, height=500,\n",
    "    cmap=viridis,\n",
    "    colorbar=True,\n",
    "    colorbar_opts={\n",
    "        'formatter': PrintfTickFormatter(format=\"%.2f\"),  # Formato legível com 2 casas decimais\n",
    "        'title': 'Dist. (Arcsec)',  # Título da barra de cores\n",
    "    },\n",
    "    tools=['hover'], \n",
    "    cnorm='eq_hist',  # Equalização de histograma\n",
    "    xlabel='RA (deg)', ylabel='DEC (deg)',\n",
    "    fontsize={'xticks': 12, 'yticks': 12, 'xlabel': 14, 'ylabel': 14},\n",
    "    invert_xaxis=True  # Inverte apenas o eixo X\n",
    ")\n",
    "\n",
    "if save_the_info:\n",
    "    output_image_path = os.path.join(logs_dir, 'output_spatial_distribution_distances.html') \n",
    "    hv.save(rasterized, output_image_path, fmt='html')\n",
    "    print(f\"Plot saved in: {output_image_path}\")\n",
    "\n",
    "# Renderizar e exibir o gráfico\n",
    "hv.output(rasterized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652d1008-b416-4730-b8d9-fe92e1830fe2",
   "metadata": {},
   "source": [
    "## Plotting the histogram of distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1093e99-4a13-470d-90f0-8204fa2b7cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar um Dataset a partir do DataFrame\n",
    "dataset = hv.Dataset(filtered_df, kdims='_dist_arcsec')\n",
    "\n",
    "# Aplicar a operação de histograma\n",
    "hist = hv.operation.histogram(dataset, dimension='_dist_arcsec', normed=False, bins=20)\n",
    "\n",
    "# Personalizar o histograma\n",
    "hist = hist.opts(\n",
    "    xlabel='Distance (arcsec)',\n",
    "    ylabel='Frequency',\n",
    "    title='Histogram of separation distances',\n",
    "    color='blue',\n",
    "    tools=['hover'],\n",
    "    width=750,\n",
    "    height=500,\n",
    "    yticks=[(i, f\"{i}\") for i in range(0, int(hist.range(1)[1]) + 1, int(hist.range(1)[1]) // 10)]  # Formato legível no eixo Y\n",
    ")\n",
    "\n",
    "if save_the_info==True:\n",
    "    output_image_path = os.path.join(logs_dir, 'output_separation_distances_histogram.html') \n",
    "    hv.save(hist, output_image_path, fmt='html')\n",
    "    print(f\"Plot saved in: {output_image_path}\")\n",
    "\n",
    "# Mostrar o histograma\n",
    "hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7183d40a-8528-444d-ad16-bf0811012518",
   "metadata": {},
   "source": [
    "## Checking for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a257b6cf-28c7-4334-abf4-cc00bf562b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mantendo todos os valores duplicados com base na coluna 'id_right_output_name'\n",
    "df_duplicates_total = filtered_df[filtered_df.duplicated(subset=[id_right_output_name], keep=False)]\n",
    "\n",
    "# Organizando o DataFrame resultante em ordem crescente pela coluna 'id_right_output_name'\n",
    "df_duplicates_total = df_duplicates_total.sort_values(by=id_right_output_name)\n",
    "\n",
    "# Mantendo apenas uma ocorrência de cada valor duplicado na coluna 'id_right_output_name'\n",
    "df_duplicates_individual = df_duplicates_total.drop_duplicates(subset=[id_right_output_name], keep='first')\n",
    "\n",
    "# Salvando os DataFrames em arquivos CSV, se necessário\n",
    "if save_the_info:\n",
    "    # Salvando duplicados totais\n",
    "    output_csv_path_total = os.path.join(logs_dir, 'output_duplicates_total.csv')\n",
    "    df_duplicates_total.to_csv(output_csv_path_total, index=False)\n",
    "    print(f\"DataFrame with total duplicates saved in: {output_csv_path_total}\")\n",
    "    \n",
    "    # Salvando duplicados individuais\n",
    "    output_csv_path_individual = os.path.join(logs_dir, 'output_duplicates_individual.csv')\n",
    "    df_duplicates_individual.to_csv(output_csv_path_individual, index=False)\n",
    "    print(f\"DataFrame with individual duplicates saved in: {output_csv_path_individual}\")\n",
    "\n",
    "# Exibindo os DataFrames resultantes\n",
    "print(\"Total duplicates:\")\n",
    "print(len(df_duplicates_total))\n",
    "\n",
    "print(\"\\nIndividual duplicates: \")\n",
    "print(len(df_duplicates_individual))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c92b76-c68d-45b0-b010-1028965434bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Closing the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a2869c-f4f4-4f1e-a3dc-cd58a1bc2ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if close_the_cluster==True:\n",
    "    client.close()\n",
    "    cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hats_env_081124",
   "language": "python",
   "name": "hats_env_081124"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
