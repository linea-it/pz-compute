{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3502450f-0ad0-4916-88ac-6dbd5e269604",
   "metadata": {},
   "source": [
    "<img align=\"left\" src = \"images/linea.png\" width=120 style=\"padding: 20px\"> \n",
    "<img align=\"left\" src = \"images/rubin.png\" width=140 style=\"padding: 30px\"> \n",
    "\n",
    "# PZ Compute - E2E Notebook \n",
    "### Photo-zs for LSST Object catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062130f7-7643-48e7-8659-74ba733233ee",
   "metadata": {},
   "source": [
    "## Data Release: DP0.2 \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8fca8ea8-0374-4f01-b2c7-376c8c9a689a",
   "metadata": {},
   "source": [
    "TEXTO EM PT-BR = PLANEJAMENTO \n",
    "- atualizar o script de preparação do processo para fazer uma cópia do template  deste notebook de E2E em cada processo, junto com o process_info.yaml - Helo \n",
    "- mais texto em markdown para definir melhor todas as etapas - Julia "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b696e47a-6aca-4d18-9d37-a9b3953e619e",
   "metadata": {},
   "outputs": [],
   "source": [
    "release = 'lsst_dp02' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04de6ae3-37d5-466a-835b-666aac3dd5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! conda env list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55755ddc-2bac-4acc-bbef-eca403802d98",
   "metadata": {},
   "source": [
    "Notebook contributors: Julia Gschwend, Luigi Silva, Heloisa Mengisztki <br>\n",
    "Contact: [julia@linea.org.br](mailto:julia@linea.org.br) <br>\n",
    "Last verified run: **2024-Nov-08** <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17152e67-232f-4a54-8f6f-ee57bec5b0e1",
   "metadata": {},
   "source": [
    "## README - Disclaimer\n",
    "This notebook is an alternative front-end for the pipeline Photo-z Compute, originally developed for command line execution on LIneA's HPC environment. It is meant to be used by the \"photo-z experts\" in charge of the production tasks related to the Brazilian in-kind contribution to LSST. It should **not** be considered as a source of [documentation or user guide](https://github.com/linea-it/pz-compute/tree/main/doc/manpages). \n",
    "\n",
    "After each complete execution, this notebook must be exported and saved as HTML file to serve as an execution report for future provenance tracking. Additional process metadata and provenance info are available in the `provenance_info.yaml` file attached. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2bb938-26f6-497f-9950-c0b07047787e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Notebook contents \n",
    "\n",
    "1. Pre-processing: data preparation, photo-z training and validation \n",
    "2. Photo-z Compute \n",
    "3. Post-processing: analize results and performance  \n",
    "\n",
    "\n",
    "Each one of these steps was carefuly explored in separate notebooks. This notebook contains only the final decisions regarding sample selection and configuration choices.   \n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b60d108-1ddb-45cd-a744-3ba59f01400b",
   "metadata": {},
   "source": [
    "Setup:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5322c2e7-123d-4ac5-a666-8c00c94b8324",
   "metadata": {},
   "source": [
    "setar o ambiente, levantar kernel e fazer com que o ambiente esteja valendo quando rodar coisas do terminal na célula (\"!\") - Helo, Nubia e Singulani "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67c4c73-275e-4c7e-8326-b141ad974670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "#import ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3555d19-d171-4e1e-a1d7-859605854387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PZ Server\n",
    "from pzserver import PzServer\n",
    "with open('.token.txt', 'r') as file:\n",
    "    token = file.read()\n",
    "pz_server = PzServer(token=token, host=\"pz-dev\") # \"pz-dev\" is the temporary host for test phase  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff0acd8-0396-4a00-8d89-1abcbb65fa7c",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "# 1. Pre-processing \n",
    "\n",
    "## 1.1 Create Skinny tables \n",
    "\n",
    "Skinny tables are a subset of the [LSST Object catalog](https://sdm-schemas.lsst.io/dp02.html#Object) that includes only the columns of interest for photo-z algorithms, with ready-to-use data, i.e.: fluxes converted into deredded magnitudes.  \n",
    "\n",
    "### Input data\n",
    "\n",
    "The very first input data of this end-to-end sequence is the original LSST Object catalog for DP0.2, stored in Lustre system at: \n",
    "\n",
    "`/lustre/t1/cl/lsst/dp02/primary/catalogs/object/` \n",
    "\n",
    "Filename pattern: `objectTable_tract_xxxx_DC2_2_2i_runs_DP0_2_v23_0_1_PREOPS-905_step3_x_2022xxxxTxxxxxxZ.parq`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dde627e-8a4e-4332-97b5-7cd6a09edf7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import os\n",
    "import glob\n",
    "import getpass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import pyarrow.parquet as pq  \n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34117bb1-059d-46bf-8e08-2ac813cae57b",
   "metadata": {},
   "source": [
    "File size summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f3487d-a441-4742-bd76-abf12016b3f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Paths for the catalog files\n",
    "catalog_path = '/lustre/t1/cl/lsst/dp02/primary/catalogs/object'\n",
    "catalog_files = '*.parq'\n",
    "catalog_files_paths = [f for f in glob.glob(os.path.join(catalog_path, catalog_files))]\n",
    "\n",
    "# If the IDs are in the index column, reset the index and add a column corresponding to the IDs.\n",
    "ids_are_in_the_index = True\n",
    "\n",
    "# Defining if you want to save the file size distribution, file size histogram and summarize pixels.\n",
    "save_input_catalog_info = False\n",
    "\n",
    "# If you choose True above, select the path to save the info.\n",
    "if save_input_catalog_info==True:\n",
    "    user = getpass.getuser()\n",
    "    path_to_save_input_catalog_info = f'/lustre/t0/scratch/users/{user}/pz-compute/ondemand/data'\n",
    "    os.makedirs(path_to_save_input_catalog_info, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f31bc1a-d410-4252-8717-a05f7c8202c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reading the catalog and getting the columns\n",
    "catalog_ddf = dd.read_parquet(catalog_files_paths)\n",
    "if ids_are_in_the_index:\n",
    "    catalog_ddf = catalog_ddf.reset_index()\n",
    "catalog_columns = catalog_ddf.columns\n",
    "catalog_columns_list = catalog_columns.to_list()\n",
    "\n",
    "# Getting general information about the catalog files\n",
    "file_info_list = []\n",
    "for file_path in catalog_files_paths:\n",
    "    try:\n",
    "        # File size\n",
    "        file_size = os.stat(file_path).st_size  # Size in bytes\n",
    "        file_size_gb = file_size / (1024 ** 3)  # Converting to gigabytes\n",
    "        \n",
    "        # Counting the rows using parquet metadata\n",
    "        parquet_file = pq.ParquetFile(file_path)  # Loading parquet metadata\n",
    "        num_rows = parquet_file.metadata.num_rows  # Number of rows in the file\n",
    "        \n",
    "        # Adding information to the dictionary\n",
    "        file_info_list.append({\n",
    "            \"file\": file_path,\n",
    "            \"size_on_disk\": file_size,\n",
    "            \"gbs\": file_size_gb,\n",
    "            \"rows\": num_rows\n",
    "        })\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "# Creating a dataframe with the files info\n",
    "input_info_frame = pd.DataFrame(file_info_list)\n",
    "\n",
    "# Calculating statistics\n",
    "num_partitions = len(input_info_frame)\n",
    "min_size_on_disk = input_info_frame[\"gbs\"].min() if not input_info_frame.empty else 0\n",
    "max_size_on_disk = input_info_frame[\"gbs\"].max() if not input_info_frame.empty else 0\n",
    "total_size_on_disk = input_info_frame[\"gbs\"].sum()\n",
    "\n",
    "# Getting the rows count corresponding to the min and max size files\n",
    "min_size_file = input_info_frame[input_info_frame[\"gbs\"] == min_size_on_disk]\n",
    "max_size_file = input_info_frame[input_info_frame[\"gbs\"] == max_size_on_disk]\n",
    "min_rows = min_size_file[\"rows\"].iloc[0] if not min_size_file.empty else 0\n",
    "max_rows = max_size_file[\"rows\"].iloc[0] if not max_size_file.empty else 0\n",
    "\n",
    "total_rows = input_info_frame[\"rows\"].sum() if not input_info_frame.empty else 0\n",
    "avg_file_size = total_size_on_disk / num_partitions if num_partitions > 0 else 0\n",
    "avg_rows_per_file = total_rows / num_partitions if num_partitions > 0 else 0\n",
    "\n",
    "# Preparing the table in Markdown format\n",
    "markdown_table = f\"\"\"\n",
    "| Metric               | Value                       |\n",
    "|----------------------|-----------------------------|\n",
    "| Number of files      | {len(catalog_files_paths)}  |\n",
    "| Number of columns    | {len(catalog_columns_list)} |\n",
    "| Min file size        | {min_size_on_disk:.2f} GB; {min_rows} rows |\n",
    "| Max file size        | {max_size_on_disk:.2f} GB; {max_rows} rows |\n",
    "| Average file size    | {avg_file_size:.2f} GB; {avg_rows_per_file:.0f} rows |\n",
    "| Total size on disk   | {total_size_on_disk:.2f} GB; {total_rows} rows |\n",
    "\"\"\"\n",
    "\n",
    "# Display the Markdown table in a cell\n",
    "display(Markdown(markdown_table))\n",
    "\n",
    "# Saving the information if required\n",
    "if save_input_catalog_info:\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(path_to_save_input_catalog_info, exist_ok=True)\n",
    "    \n",
    "    # Save the DataFrame as CSV\n",
    "    input_info_frame_path = os.path.join(path_to_save_input_catalog_info, \"input_info_frame.csv\")\n",
    "    input_info_frame.to_csv(input_info_frame_path, index=False)\n",
    "    \n",
    "    # Save the Markdown table as a text file\n",
    "    markdown_table_path = os.path.join(path_to_save_input_catalog_info, \"input_info_summary.txt\")\n",
    "    with open(markdown_table_path, \"w\") as f:\n",
    "        f.write(markdown_table)\n",
    "    \n",
    "    # Save the provenance information\n",
    "    provenance_path = os.path.join(path_to_save_input_catalog_info, \"input_info_provenance.txt\")\n",
    "    with open(provenance_path, \"w\") as f:\n",
    "        f.write(f\"catalog_path: {catalog_path}\\n\")\n",
    "        f.write(f\"catalog_files: {catalog_files}\\n\")\n",
    "    \n",
    "    print(f\"Information saved to {path_to_save_input_catalog_info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17f5832-2f45-49bb-8451-dddeb083b1fc",
   "metadata": {},
   "source": [
    "File sizes distribution and file sizes histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbe69ea-1eb9-4bde-9837-15313fd6fa46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_and_save_histogram_info(info_frame, bins, labels, type_of_files, logs_dir, save=False, show=False):\n",
    "    \"\"\"\n",
    "    Computes, optionally saves, and optionally shows histogram information (counts and percentages) for given bins and labels.\n",
    "\n",
    "    Args:\n",
    "        info_frame (DataFrame): DataFrame containing the file size information.\n",
    "        bins (list): Bin edges for the histogram.\n",
    "        labels (list): Labels corresponding to the bins.\n",
    "        type_of_files (str): Identifier for the type of files (used in filenames).\n",
    "        logs_dir (str): Directory to save the output files.\n",
    "        save (bool): Whether to save the histogram information.\n",
    "        show (bool): Whether to display the histogram information as a Markdown table.\n",
    "    \"\"\"\n",
    "    # Compute histogram counts and percentages\n",
    "    hist = np.histogram(info_frame[\"gbs\"], bins=bins)[0]\n",
    "    pcts = hist / len(info_frame) if len(info_frame) > 0 else [0] * len(labels)\n",
    "\n",
    "    # Save histogram information\n",
    "    if save and logs_dir:\n",
    "        os.makedirs(logs_dir, exist_ok=True)\n",
    "        output_path = os.path.join(logs_dir, f\"{type_of_files}_file_size_distribution.txt\")\n",
    "        with open(output_path, \"w\") as file:\n",
    "            file.write(f\"Bins: {bins} GB\\n\")\n",
    "            file.write(f\"Labels: {labels} \\n \\n\")\n",
    "            for i, label in enumerate(labels):\n",
    "                file.write(f\"{label} \\t: {hist[i]} \\t({pcts[i]*100:.1f} %)\\n\")\n",
    "        print(f\"Histogram information saved to {output_path}\")\n",
    "        \n",
    "    # Show histogram information as a Markdown table\n",
    "    if show:\n",
    "        # Prepare the Markdown table\n",
    "        markdown_table = \"| Label        | Count | Percentage (%) |\\n\"\n",
    "        markdown_table += \"|--------------|-------|----------------|\\n\"\n",
    "        for i, label in enumerate(labels):\n",
    "            markdown_table += f\"| {label:<12} | {hist[i]:<5} | {pcts[i]*100:.1f}       |\\n\"\n",
    "\n",
    "        # Display bins and labels in text, then the Markdown table\n",
    "        bins_labels_md = f\"**Bins**: {bins}  GB \\n\\n**Labels**: {labels}  \\n\\n\"\n",
    "        display(Markdown(bins_labels_md + markdown_table))\n",
    "\n",
    "\n",
    "def plot_and_save_histogram(info_frame, bins, type_of_files, logs_dir, save=False, show=False):\n",
    "    \"\"\"\n",
    "    Plots and optionally saves the histogram of file sizes and displays the bins as Markdown.\n",
    "\n",
    "    Args:\n",
    "        info_frame (DataFrame): DataFrame containing the file size information.\n",
    "        bins (list or None): Bin edges for the histogram. If None, bins will be generated automatically.\n",
    "        type_of_files (str): Identifier for the type of files (used in filenames).\n",
    "        logs_dir (str): Directory to save the output files.\n",
    "        save (bool): Whether to save the histogram plot.\n",
    "        show (bool): Whether to display the plot and bins information.\n",
    "    \"\"\"\n",
    "    # Plot the histogram\n",
    "    n, bins_generated, _ = plt.hist(info_frame[\"gbs\"], bins=bins, edgecolor='black')\n",
    "    plt.xlabel(\"File size (GB)\")\n",
    "    plt.ylabel(\"Number of files\")\n",
    "    \n",
    "    bins_used = bins_generated if bins is None else bins\n",
    "    bins_used_rounded = [round(float(bin_edge), 3) for bin_edge in bins_used]\n",
    "    \n",
    "    # Save the plot\n",
    "    if save and logs_dir:\n",
    "        os.makedirs(logs_dir, exist_ok=True)\n",
    "        output_path_plot = os.path.join(logs_dir, f\"{type_of_files}_file_size_histogram.png\")\n",
    "        plt.savefig(output_path_plot)\n",
    "        print(f\"Histogram plot saved to {output_path_plot}\")\n",
    "        \n",
    "        output_path_bins = os.path.join(logs_dir, f\"{type_of_files}_file_size_histogram.txt\")\n",
    "        with open(output_path_bins, \"w\") as file:\n",
    "            file.write(f\"Bins: {bins_used} GB\\n\")\n",
    "\n",
    "    # Show the plot\n",
    "    if show:\n",
    "        # Display bins in Markdown format\n",
    "        bins_md = f\"**Bins**: {bins_used_rounded}  GB\\n\"\n",
    "        display(Markdown(bins_md))\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b24ad15-d88d-414c-8e96-59f7207439f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "type_of_files = 'input'\n",
    "\n",
    "bins_lincc_categories = [0, 0.5, 1, 2, 100]\n",
    "labels_lincc_categories = [\"small-ish\", \"sweet-spot\", \"big-ish\", \"too-big\"]\n",
    "\n",
    "bins_plotted_histogram = None\n",
    "\n",
    "logs_dir = path_to_save_input_catalog_info if save_input_catalog_info else None\n",
    "\n",
    "compute_and_save_histogram_info(\n",
    "    info_frame=input_info_frame, \n",
    "    bins=bins_lincc_categories, \n",
    "    labels=labels_lincc_categories, \n",
    "    type_of_files=type_of_files, \n",
    "    logs_dir=logs_dir, \n",
    "    save=save_input_catalog_info, \n",
    "    show=True\n",
    ")\n",
    "\n",
    "print(\"\\n \\n\")\n",
    "\n",
    "plot_and_save_histogram(\n",
    "    info_frame=input_info_frame, \n",
    "    bins=bins_plotted_histogram,  \n",
    "    type_of_files=type_of_files, \n",
    "    logs_dir=logs_dir, \n",
    "    save=save_input_catalog_info, \n",
    "    show=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7113bdff-6112-4cbc-bf39-127a9d70037f",
   "metadata": {},
   "source": [
    "### How to run commands in the conda env corresponding to the kernel directly in the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b87be55-e788-42b1-9971-2331a1c81ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "def run_command(command_text):\n",
    "    # Get the path to the current Python executable (used by the active kernel)\n",
    "    current_python = sys.executable\n",
    "\n",
    "    # Resolve the path to the Conda environment directory\n",
    "    conda_env_path = os.path.dirname(os.path.dirname(current_python))\n",
    "\n",
    "    # Display the active Conda environment path (optional, for confirmation)\n",
    "    print(f\"Active Conda environment path: {conda_env_path}\")\n",
    "\n",
    "    # Locate the base Conda installation\n",
    "    base_conda_path = os.path.dirname(os.path.dirname(conda_env_path))\n",
    "\n",
    "    # Use the Conda activation script to activate the environment and run the command\n",
    "    command = f\"source {base_conda_path}/etc/profile.d/conda.sh && conda activate {conda_env_path} && {command_text}\"\n",
    "    os.system(command)\n",
    "\n",
    "run_command('conda list')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65287f8f-8229-4757-b3b2-9c9802453d10",
   "metadata": {},
   "source": [
    "### Column selection  \n",
    "\n",
    "Columns included in the skinny tables: \n",
    "\n",
    "| column name | data type |  description |\n",
    "| ---         | ---       |  ---         |\n",
    "| objectId\t  | int  \t  | Unique identifier | \n",
    "| coord_ra\t  | float64\t  | Fiducial ICRS Right Ascension of centroid (degrees)|\n",
    "| coord_dec   |\tfloat64\t  | Fiducial ICRS Declination of centroid (degrees)| \n",
    "| detect_isPrimary\t| boolean\t| True if source has no children and is in the inner region of a coadd patch and is in the inner region of a coadd tract and is not a sky source | \n",
    "| mag_{u, g, r, i, z, y} | float64 | {u, g, r, i, z, y}-band magnitude converted from final cmodel fit flux measurements | \n",
    "| magerr_{u, g, r, i, z, y} | float64 | {u, g, r, i, z, y}-band magnitude errors converted from final cmodel fit flux error measurements | \n",
    "\n",
    "### Object selection  \n",
    "\n",
    "Data cleaning to reduce the number of rows in the catalog is recommended for tests or when using the pipeline to create value-added catalogs for science cases. Survey conditions maps can be used to remove intire regions at once, based on a given quality threshold.   \n",
    "\n",
    "\n",
    "**WARNING: Data cleaning must not be applied on the production runs to generate the official data products to be delivered as part of the in-kind contribution. Every detected object from the Object catalog must receive a photo-z estimate regardless of its nature or photometry quality.**      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513f39b1-c57b-400c-b088-cb94bf8b63df",
   "metadata": {},
   "source": [
    "### Configuration parameters \n",
    "\n",
    "Parameters defined inside `$SCRATCH/bin/rail-slurm-preprocess.batch`: \n",
    "\n",
    "```python \n",
    "SRUN = 'srun'\n",
    "SRUN_ARGS = [SRUN, '-n1', '-N1']\n",
    "LOG = 'log'\n",
    "PROG = 'rail-slurm-preprocess'\n",
    "PREPROCESS = 'rail-preprocess-parquet'\n",
    "PREPROCESS_ARGS = ['--rows=130000', '--apply-dered=sfd', '--apply-detect-flag=True', \n",
    "                   '--round-mags=4', '--output-template={fname}-part{idx}.parquet']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9755e60-2667-406e-afa5-ed142e7a260a",
   "metadata": {},
   "source": [
    "### Execute `rail-preprocess-parquet` on Apollo\n",
    "\n",
    "WARNING: Current working directory must contain a directory named as `log`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ea8b0f-1187-4f4c-a108-5dde5c0c385c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fazer funcionar a execução em linha de comando - Helo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4e8e22-0adf-49db-a071-be6b902aadac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! sbatch -N 2 -n 20 rail-slurm-preprocess.batch  input/ output/ input/objectTable_tract_*.parq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c37b74-4356-4385-8eda-7697e6f49da4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a077c3bc-b878-4ecb-84bc-f47abf3a0a8c",
   "metadata": {},
   "source": [
    "monitor slurm queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c67614c-9c87-443f-a71a-9deea57a3270",
   "metadata": {},
   "outputs": [],
   "source": [
    "! squeue"
   ]
  },
  {
   "cell_type": "raw",
   "id": "42a392ad-c59e-4838-a829-97178a32ddc2",
   "metadata": {},
   "source": [
    "TO DO: debug\n",
    "\n",
    "- mag_err com notação cient;ífica apenas em algumas bandas quando usa o round mag --> trocar para float 32 - Julia \n",
    "- verificar impacto do deredenning (em outro notebook)  - Julia \n",
    "- drop coluna detect_isPrimary quando for usada para o corte e manter quando não usar - Julia  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "38072a87-0fb0-41bc-b5b4-0782fdbd1b7e",
   "metadata": {},
   "source": [
    "travar aqui enquanto a skinny table fica pronta (vide \"job wait\" no tap service síncrono do LSST nos notebooks de tutorial) - Helo "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c517038f-1231-4335-b581-27ccf48ba7a9",
   "metadata": {},
   "source": [
    "### Output data \n",
    "\n",
    "#### Basic QA of skinny tables "
   ]
  },
  {
   "cell_type": "raw",
   "id": "f43378a9-1179-4f88-b924-ad0078620995",
   "metadata": {},
   "source": [
    "Histograma tamanhos arquivos tabela skinny (comparar lado a lado) - Luigi  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d979d4-633f-4f66-9d49-16a15156afd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "2a221adf-75b2-48c4-ba45-a8a5afcd3ed3",
   "metadata": {},
   "source": [
    "QA científico - selecionar plots mais importantes - Luigi\n",
    "começar por: \n",
    "- dist. espacial \n",
    "- N(mag), banda i \n",
    "- mag x err, só banda i "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bb4f20-9423-4c16-9bb8-3b4e87d73e72",
   "metadata": {},
   "source": [
    "## 1.2 Create Training and Test Sets \n",
    "\n",
    "\n",
    "### Representative spectroscopic sample \n",
    "\n",
    "A true-z sample randomly selected from the DC2 simulation to mimic a representative spectroscopic sample regarding the color-magnitude-redshift space. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "6fa06133-7945-4192-9072-c9b4a4523e34",
   "metadata": {},
   "source": [
    "executar random selecion - Luigi "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbec976b-d376-4dcf-834c-a2e47191be66",
   "metadata": {},
   "source": [
    "Split the sample into two random subsets, with 70% of the galaxies designated for training and 30% for tests by adding an extra column `test`: \n",
    "* `test=0`: galaxies included in the **training** procedure\n",
    "* `test=1`: galaxies included in the **test** procedure, mandatorily excluded from the training procedure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20de284e-fe9d-42b4-a2c8-fe04cdd4ead0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b051d8-44f7-44ca-a7a7-c00a6dcbaf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "Run Training Set Maker via pz server lib \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06c5a97-e234-434b-af9b-81775e3e077a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36100ef1-c8f9-4cfe-bba3-91eaeb7c1162",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4192dc12-27bf-4741-8e60-e570440179a4",
   "metadata": {},
   "source": [
    "#### Basic QA of the representative training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bc4e17-75bc-417c-b5b8-e401a0272449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c32d5ec3-fa75-4700-aadf-8263041ee52c",
   "metadata": {},
   "source": [
    "### Realistic spectroscopic sample (TBD)\n",
    "\n",
    "A true-z sample arbitrarily selected from the DC2 simulation to mimic realistic spectroscopic sample regarding the color-magnitude-redshift space, based on current spectroscopic data available from the literature . \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c61c05-8c3a-4974-9763-a013b1b20e47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf98631c-3d87-44c1-ba84-b90cab8bed44",
   "metadata": {},
   "source": [
    "#### Basic QA of the realistic training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b010715-b669-4fea-b41c-5386d5f29d62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f109f543-d0b4-4734-a21b-696d7757818b",
   "metadata": {},
   "source": [
    "## 1.3 Train the photo-z algorithm  \n",
    "\n",
    "Train the photo-z algorithm with RAIL (`rail_inform`). Available options: BPZ, FlexZBoost, GPz, LePHARE,and  TPZ.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944b0b23-5287-4902-82f4-53ba32aa8b64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336d7cee-c5e2-41bc-aad5-635e22a96564",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4af7418-29a2-47f3-94d9-6f37dcdd1f72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85663883-845a-4432-821b-c8662c0bd8c5",
   "metadata": {},
   "source": [
    "## 1.4 Photo-z Validation    \n",
    "\n",
    "### PZ estimates for the Test Set\n",
    "\n",
    "Run `rail_estimate` module to produce the photo-z estimates (PDFs) for the Test Set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d65e292-ab10-4a63-b9f6-a1487546afb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e720b20c-6a88-49d2-8e3c-4bfb2e5077a9",
   "metadata": {},
   "source": [
    "### PZ validation results\n",
    "\n",
    "#### Metrics and plots \n",
    "\n",
    "Run `rail_evaluate` module to compute PDF metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da29f0ae-19ef-406c-b7bd-606024a514d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "217b0e87-bc94-47fb-a7cb-43cb94c1a099",
   "metadata": {},
   "source": [
    "#### PZ Validation conclusions \n",
    "\n",
    "Quality assessment, comparison with science requirements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0089887-aa2d-4789-95a6-6ef0e45ab26c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "486d38a4-847a-4b37-ac4a-c68d5a0785a5",
   "metadata": {},
   "source": [
    "# 2. Photo-z Compute \n",
    "\n",
    "## Submit pipeline to Apollo cluster \n",
    "\n",
    "## Real-time monitoring  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d282952-935d-49e3-a307-3e73b60859c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97eae09-06b4-413d-8850-32a1ee087c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369fd28c-0899-48a6-ad21-28b19b697445",
   "metadata": {},
   "source": [
    "# 3. Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e31846-704a-4c6d-b8b0-bf67afdc222a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89263a61-5f42-4de8-8bf9-7bd78e4478d6",
   "metadata": {},
   "source": [
    "## Performance evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80294cc4-315d-4e16-bd81-1a55f968ee51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20448fcd-eb7f-4052-a790-7440c360282e",
   "metadata": {},
   "source": [
    "## PZ Estimates - QA of final results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa29fa0-ce48-4f9a-b832-9ca3520fd60f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a36238b-541d-4907-b89b-f453712f2c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b932a1c0-92a7-4a8a-9cdc-748ecb881298",
   "metadata": {},
   "source": [
    "# Export to HTML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede214aa-cc81-4704-ac08-5157047687ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pz_compute_dev",
   "language": "python",
   "name": "pz_compute_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
